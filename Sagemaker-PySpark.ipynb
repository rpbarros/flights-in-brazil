{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab50a081",
   "metadata": {},
   "source": [
    "# [Distributed Data Processing using Apache Spark and SageMaker Processing](https://sagemaker-examples.readthedocs.io/en/latest/sagemaker_processing/spark_distributed_data_processing/sagemaker-spark-processing.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2a2775",
   "metadata": {},
   "source": [
    "# Setup\n",
    "## Install the latest SageMaker Python SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07df02c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (2.70.0)\n",
      "Requirement already satisfied: protobuf>=3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker) (3.19.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker) (21.2)\n",
      "Requirement already satisfied: importlib-metadata>=1.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker) (4.8.2)\n",
      "Requirement already satisfied: protobuf3-to-dict>=0.1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker) (0.1.5)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker) (1.1.5)\n",
      "Requirement already satisfied: pathos in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker) (0.2.8)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker) (1.19.5)\n",
      "Requirement already satisfied: boto3>=1.20.18 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker) (1.20.20)\n",
      "Requirement already satisfied: attrs in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker) (21.2.0)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: google-pasta in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: botocore<1.24.0,>=1.23.20 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.20.18->sagemaker) (1.23.20)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.20.18->sagemaker) (0.5.0)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.20.18->sagemaker) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.24.0,>=1.23.20->boto3>=1.20.18->sagemaker) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.24.0,>=1.23.20->boto3>=1.20.18->sagemaker) (1.26.7)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata>=1.4.0->sagemaker) (3.10.0.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata>=1.4.0->sagemaker) (3.6.0)\n",
      "Requirement already satisfied: pyparsing<3,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from packaging>=20.0->sagemaker) (2.4.7)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from protobuf3-to-dict>=0.1.5->sagemaker) (1.16.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pandas->sagemaker) (2021.3)\n",
      "Requirement already satisfied: dill>=0.3.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pathos->sagemaker) (0.3.4)\n",
      "Requirement already satisfied: ppft>=1.6.6.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pathos->sagemaker) (1.6.6.4)\n",
      "Requirement already satisfied: pox>=0.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pathos->sagemaker) (0.3.0)\n",
      "Requirement already satisfied: multiprocess>=0.70.12 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pathos->sagemaker) (0.70.12.2)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4915d0",
   "metadata": {},
   "source": [
    "## Setup S3 bucket locations and roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "782cf455",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sagemaker\n",
    "from time import gmtime, strftime\n",
    "\n",
    "sagemaker_logger = logging.getLogger(\"sagemaker\")\n",
    "sagemaker_logger.setLevel(logging.INFO)\n",
    "sagemaker_logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf71b5aa",
   "metadata": {},
   "source": [
    "## Fetch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45accec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-12-05 17:51:48--  https://datahub.io/machine-learning/abalone/r/abalone.csv\n",
      "Resolving datahub.io (datahub.io)... 172.67.157.38, 104.21.40.221, 2606:4700:3030::ac43:9d26, ...\n",
      "Connecting to datahub.io (datahub.io)|172.67.157.38|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://pkgstore.datahub.io/machine-learning/abalone/abalone_csv/data/dc2ecfb999b9c9dc9fa4f20f6d30c9a9/abalone_csv.csv [following]\n",
      "--2021-12-05 17:51:49--  https://pkgstore.datahub.io/machine-learning/abalone/abalone_csv/data/dc2ecfb999b9c9dc9fa4f20f6d30c9a9/abalone_csv.csv\n",
      "Resolving pkgstore.datahub.io (pkgstore.datahub.io)... 104.21.40.221, 172.67.157.38, 2606:4700:3033::6815:28dd, ...\n",
      "Connecting to pkgstore.datahub.io (pkgstore.datahub.io)|104.21.40.221|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 196156 (192K) [text/csv]\n",
      "Saving to: ‘./data/abalone.csv’\n",
      "\n",
      "./data/abalone.csv  100%[===================>] 191.56K  --.-KB/s    in 0.003s  \n",
      "\n",
      "2021-12-05 17:51:49 (68.2 MB/s) - ‘./data/abalone.csv’ saved [196156/196156]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fetch the dataset from the SageMaker bucket\n",
    "!wget https://datahub.io/machine-learning/abalone/r/abalone.csv -O ./data/abalone.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249892e4",
   "metadata": {},
   "source": [
    "## Write the PySpark script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "088678b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./code/preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./code/preprocess.py\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import argparse\n",
    "import csv\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import (\n",
    "    OneHotEncoder,\n",
    "    StringIndexer,\n",
    "    VectorAssembler,\n",
    "    VectorIndexer,\n",
    "    Imputer,\n",
    ")\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import (\n",
    "    DoubleType,\n",
    "    StringType,\n",
    "    StructField,\n",
    "    StructType,\n",
    ")\n",
    "\n",
    "\n",
    "def csv_line(data):\n",
    "    r = \",\".join(str(d) for d in data[1])\n",
    "    return str(data[0]) + \",\" + r\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"app inputs and outputs\")\n",
    "    parser.add_argument(\"--s3_input_bucket\", type=str, help=\"s3 input bucket\")\n",
    "    parser.add_argument(\"--s3_input_key_prefix\", type=str, help=\"s3 input key prefix\")\n",
    "    parser.add_argument(\"--s3_output_bucket\", type=str, help=\"s3 output bucket\")\n",
    "    parser.add_argument(\"--s3_output_key_prefix\", type=str, help=\"s3 output key prefix\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    spark = SparkSession.builder.appName(\"PySparkApp\").getOrCreate()\n",
    "\n",
    "    # This is needed to save RDDs which is the only way to write nested Dataframes into CSV format\n",
    "    spark.sparkContext._jsc.hadoopConfiguration().set(\n",
    "        \"mapred.output.committer.class\", \"org.apache.hadoop.mapred.FileOutputCommitter\"\n",
    "    )\n",
    "    \n",
    "    # Defining the schema corresponding to the input data. The input data does not contain the headers\n",
    "    schema = StructType(\n",
    "        [\n",
    "            StructField(\"sex\", StringType(), True),\n",
    "            StructField(\"length\", DoubleType(), True),\n",
    "            StructField(\"diameter\", DoubleType(), True),\n",
    "            StructField(\"height\", DoubleType(), True),\n",
    "            StructField(\"whole_weight\", DoubleType(), True),\n",
    "            StructField(\"shucked_weight\", DoubleType(), True),\n",
    "            StructField(\"viscera_weight\", DoubleType(), True),\n",
    "            StructField(\"shell_weight\", DoubleType(), True),\n",
    "            StructField(\"rings\", DoubleType(), True),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Downloading the data from S3 into a Dataframe\n",
    "    total_df = spark.read.csv(\n",
    "        (\"s3://\" + os.path.join(args.s3_input_bucket, args.s3_input_key_prefix, \"abalone.csv\")),\n",
    "        header=False,\n",
    "        schema=schema,\n",
    "    )\n",
    "    \n",
    "    # import sql function pyspark\n",
    "    import pyspark.sql.functions as f\n",
    "\n",
    "    # null values in each column\n",
    "    data_agg = total_df.agg(*[f.count(f.when(f.isnull(c), c)).alias(c) for c in total_df.columns])\n",
    "    data_agg.show()\n",
    "    total_df.dropna(how='any')\n",
    "    \n",
    "    # StringIndexer on the sex column which has categorical value\n",
    "    sex_indexer = StringIndexer(inputCol=\"sex\", outputCol=\"indexed_sex\", handleInvalid=\"skip\")\n",
    "    \n",
    "    # one-hot-encoding is being performed on the string-indexed sex column (indexed_sex)\n",
    "    sex_encoder = OneHotEncoder(inputCol=\"indexed_sex\", outputCol=\"sex_vec\")\n",
    "\n",
    "    # vector-assembler will bring all the features to a 1D vector for us to save easily into CSV format\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=[\n",
    "            \"sex_vec\",\n",
    "            \"length\",\n",
    "            \"diameter\",\n",
    "            \"height\",\n",
    "            \"whole_weight\",\n",
    "            \"shucked_weight\",\n",
    "            \"viscera_weight\",\n",
    "            \"shell_weight\",\n",
    "        ],\n",
    "        outputCol=\"features\",\n",
    "    )\n",
    "\n",
    "    # The pipeline comprises of the steps added above\n",
    "    pipeline = Pipeline(stages=[sex_indexer, sex_encoder, assembler])\n",
    "\n",
    "    # This step trains the feature transformers\n",
    "    model = pipeline.fit(total_df)\n",
    "\n",
    "    # This step transforms the dataset with information obtained from the previous fit\n",
    "    transformed_total_df = model.transform(total_df)\n",
    "\n",
    "    # Split the overall dataset into 80-20 training and validation\n",
    "    (train_df, validation_df) = transformed_total_df.randomSplit([0.8, 0.2])\n",
    "    \n",
    "    # Convert the train dataframe to RDD to save in CSV format and upload to S3\n",
    "    train_rdd = train_df.rdd.map(lambda x: (x.rings, x.features))\n",
    "    train_lines = train_rdd.map(csv_line)\n",
    "    train_lines.saveAsTextFile(\n",
    "        \"s3://\" + os.path.join(args.s3_output_bucket, args.s3_output_key_prefix, \"train\")\n",
    "    )\n",
    "\n",
    "    # Convert the validation dataframe to RDD to save in CSV format and upload to S3\n",
    "    validation_rdd = validation_df.rdd.map(lambda x: (x.rings, x.features))\n",
    "    validation_lines = validation_rdd.map(csv_line)\n",
    "    validation_lines.saveAsTextFile(\n",
    "        \"s3://\" + os.path.join(args.s3_output_bucket, args.s3_output_key_prefix, \"validation\")\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b79745f",
   "metadata": {},
   "source": [
    "## Run the SageMaker Processing Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f3b9214",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating processing-job with name sm-spark-2021-12-05-17-53-36-279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  sm-spark-2021-12-05-17-53-36-279\n",
      "Inputs:  [{'InputName': 'code', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-193890026231/sm-spark-2021-12-05-17-53-36-279/input/code/preprocess.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'output-1', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-193890026231/sagemaker/spark-preprocess-demo/2021-12-05-17-53-35/spark_event_logs', 'LocalPath': '/opt/ml/processing/spark-events/', 'S3UploadMode': 'Continuous'}}]\n",
      ".........................\u001b[34m12-05 17:57 smspark.cli  INFO     Parsing arguments. argv: ['/usr/local/bin/smspark-submit', '--local-spark-event-logs-dir', '/opt/ml/processing/spark-events/', '/opt/ml/processing/input/code/preprocess.py', '--s3_input_bucket', 'sagemaker-us-east-1-193890026231', '--s3_input_key_prefix', 'sagemaker/spark-preprocess-demo/2021-12-05-17-53-35/input/raw/abalone', '--s3_output_bucket', 'sagemaker-us-east-1-193890026231', '--s3_output_key_prefix', 'sagemaker/spark-preprocess-demo/2021-12-05-17-53-35/input/preprocessed/abalone']\u001b[0m\n",
      "\u001b[34m12-05 17:57 smspark.cli  INFO     Raw spark options before processing: {'class_': None, 'jars': None, 'py_files': None, 'files': None, 'verbose': False}\u001b[0m\n",
      "\u001b[34m12-05 17:57 smspark.cli  INFO     App and app arguments: ['/opt/ml/processing/input/code/preprocess.py', '--s3_input_bucket', 'sagemaker-us-east-1-193890026231', '--s3_input_key_prefix', 'sagemaker/spark-preprocess-demo/2021-12-05-17-53-35/input/raw/abalone', '--s3_output_bucket', 'sagemaker-us-east-1-193890026231', '--s3_output_key_prefix', 'sagemaker/spark-preprocess-demo/2021-12-05-17-53-35/input/preprocessed/abalone']\u001b[0m\n",
      "\u001b[34m12-05 17:57 smspark.cli  INFO     Rendered spark options: {'class_': None, 'jars': None, 'py_files': None, 'files': None, 'verbose': False}\u001b[0m\n",
      "\u001b[34m12-05 17:57 smspark.cli  INFO     Initializing processing job.\u001b[0m\n",
      "\u001b[34m12-05 17:57 smspark-submit INFO     {'current_host': 'algo-1', 'hosts': ['algo-1', 'algo-2']}\u001b[0m\n",
      "\u001b[34m12-05 17:57 smspark-submit INFO     {'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:193890026231:processing-job/sm-spark-2021-12-05-17-53-36-279', 'ProcessingJobName': 'sm-spark-2021-12-05-17-53-36-279', 'AppSpecification': {'ImageUri': '173754725891.dkr.ecr.us-east-1.amazonaws.com/sagemaker-spark-processing:2.4-cpu', 'ContainerEntrypoint': ['smspark-submit', '--local-spark-event-logs-dir', '/opt/ml/processing/spark-events/', '/opt/ml/processing/input/code/preprocess.py'], 'ContainerArguments': ['--s3_input_bucket', 'sagemaker-us-east-1-193890026231', '--s3_input_key_prefix', 'sagemaker/spark-preprocess-demo/2021-12-05-17-53-35/input/raw/abalone', '--s3_output_bucket', 'sagemaker-us-east-1-193890026231', '--s3_output_key_prefix', 'sagemaker/spark-preprocess-demo/2021-12-05-17-53-35/input/preprocessed/abalone']}, 'ProcessingInputs': [{'InputName': 'code', 'AppManaged': False, 'S3Input': {'LocalPath': '/opt/ml/processing/input/code', 'S3Uri': 's3://sagemaker-us-east-1-193890026231/sm-spark-2021-12-05-17-53-36-279/input/code/preprocess.py', 'S3DataDistributionType': 'FullyReplicated', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3CompressionType': 'None', 'S3DownloadMode': 'StartOfJob'}, 'DatasetDefinition': None}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'output-1', 'AppManaged': False, 'S3Output': {'LocalPath': '/opt/ml/processing/spark-events/', 'S3Uri': 's3://sagemaker-us-east-1-193890026231/sagemaker/spark-preprocess-demo/2021-12-05-17-53-35/spark_event_logs', 'S3UploadMode': 'Continuous'}, 'FeatureStoreOutput': None}], 'KmsKeyId': None}, 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 2, 'InstanceType': 'ml.m5.xlarge', 'VolumeSizeInGB': 30, 'VolumeKmsKeyId': None}}, 'RoleArn': 'arn:aws:iam::193890026231:role/service-role/AmazonSageMaker-ExecutionRole-20211204T143129', 'StoppingCondition': {'MaxRuntimeInSeconds': 1200}}\u001b[0m\n",
      "\u001b[34m12-05 17:57 smspark.cli  INFO     running spark submit command: spark-submit --master yarn --deploy-mode client /opt/ml/processing/input/code/preprocess.py --s3_input_bucket sagemaker-us-east-1-193890026231 --s3_input_key_prefix sagemaker/spark-preprocess-demo/2021-12-05-17-53-35/input/raw/abalone --s3_output_bucket sagemaker-us-east-1-193890026231 --s3_output_key_prefix sagemaker/spark-preprocess-demo/2021-12-05-17-53-35/input/preprocessed/abalone\u001b[0m\n",
      "\u001b[34m12-05 17:57 smspark-submit INFO     waiting for hosts\u001b[0m\n",
      "\u001b[34m12-05 17:57 smspark-submit INFO     starting status server\u001b[0m\n",
      "\u001b[34m12-05 17:57 smspark-submit INFO     Status server listening on algo-1:5555\u001b[0m\n",
      "\u001b[34m12-05 17:57 smspark-submit INFO     bootstrapping cluster\u001b[0m\n",
      "\u001b[34m12-05 17:57 smspark-submit INFO     transitioning from status INITIALIZING to BOOTSTRAPPING\u001b[0m\n",
      "\u001b[34m12-05 17:57 smspark-submit INFO     copying aws jars\u001b[0m\n",
      "\u001b[34mServing on http://algo-1:5555\u001b[0m\n",
      "\u001b[34m12-05 17:57 smspark-submit INFO     Found hadoop jar hadoop-aws.jar\u001b[0m\n",
      "\u001b[34m12-05 17:57 smspark-submit INFO     Copying optional jar jets3t-0.9.0.jar from /usr/lib/hadoop/lib to /usr/lib/spark/jars\u001b[0m\n",
      "\u001b[34m12-05 17:57 smspark-submit INFO     copying cluster config\u001b[0m\n",
      "\u001b[34m12-05 17:57 smspark-submit INFO     copying /opt/hadoop-config/hdfs-site.xml to /usr/lib/hadoop/etc/hadoop/hdfs-site.xml\u001b[0m\n",
      "\u001b[34m12-05 17:57 smspark-submit INFO     copying /opt/hadoop-config/core-site.xml to /usr/lib/hadoop/etc/hadoop/core-site.xml\u001b[0m\n",
      "\u001b[34m12-05 17:57 smspark-submit INFO     copying /opt/hadoop-config/yarn-site.xml to /usr/lib/hadoop/etc/hadoop/yarn-site.xml\u001b[0m\n",
      "\u001b[34m12-05 17:57 smspark-submit INFO     copying /opt/hadoop-config/spark-defaults.conf to /usr/lib/spark/conf/spark-defaults.conf\u001b[0m\n",
      "\u001b[34m12-05 17:57 smspark-submit INFO     copying /opt/hadoop-config/spark-env.sh to /usr/lib/spark/conf/spark-env.sh\u001b[0m\n",
      "\u001b[34m12-05 17:57 root         INFO     Detected instance type: m5.xlarge with total memory: 16384M and total cores: 4\u001b[0m\n",
      "\u001b[34m12-05 17:57 root         INFO     Writing default config to /usr/lib/hadoop/etc/hadoop/yarn-site.xml\u001b[0m\n",
      "\u001b[34m12-05 17:57 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/yarn-site.xml is: \u001b[0m\n",
      "\u001b[34m<?xml version=\"1.0\"?>\u001b[0m\n",
      "\u001b[34m<!-- Site specific YARN configuration properties -->\n",
      " <configuration>\n",
      "     <property>\n",
      "         <name>yarn.resourcemanager.hostname</name>\n",
      "         <value>10.2.226.32</value>\n",
      "         <description>The hostname of the RM.</description>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.nodemanager.hostname</name>\n",
      "         <value>algo-1</value>\n",
      "         <description>The hostname of the NM.</description>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.nodemanager.webapp.address</name>\n",
      "         <value>algo-1:8042</value>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.nodemanager.vmem-pmem-ratio</name>\n",
      "         <value>5</value>\n",
      "         <description>Ratio between virtual memory to physical memory.</description>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.resourcemanager.am.max-attempts</name>\n",
      "         <value>1</value>\n",
      "         <description>The maximum number of application attempts.</description>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.nodemanager.env-whitelist</name>\n",
      "         <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,YARN_HOME,AWS_CONTAINER_CREDENTIALS_RELATIVE_URI</value>\n",
      "         <description>Environment variable whitelist</description>\n",
      "     </property>\n",
      " \n",
      "  <property>\n",
      "    <name>yarn.scheduler.minimum-allocation-mb</name>\n",
      "    <value>1</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.scheduler.maximum-allocation-mb</name>\n",
      "    <value>15892</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.scheduler.minimum-allocation-vcores</name>\n",
      "    <value>1</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.scheduler.maximum-allocation-vcores</name>\n",
      "    <value>4</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.nodemanager.resource.memory-mb</name>\n",
      "    <value>15892</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.nodemanager.resource.cpu-vcores</name>\n",
      "    <value>4</value>\n",
      "  </property>\u001b[0m\n",
      "\u001b[34m</configuration>\u001b[0m\n",
      "\u001b[34m12-05 17:57 root         INFO     Writing default config to /usr/lib/spark/conf/spark-defaults.conf\u001b[0m\n",
      "\u001b[34m12-05 17:57 root         INFO     Configuration at /usr/lib/spark/conf/spark-defaults.conf is: \u001b[0m\n",
      "\u001b[34mspark.driver.extraClassPath      /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar\u001b[0m\n",
      "\u001b[34mspark.driver.extraLibraryPath    /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native\u001b[0m\n",
      "\u001b[34mspark.executor.extraClassPath    /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar\u001b[0m\n",
      "\u001b[34mspark.executor.extraLibraryPath  /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native\u001b[0m\n",
      "\u001b[34mspark.driver.host=10.2.226.32\u001b[0m\n",
      "\u001b[34mspark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2\u001b[0m\n",
      "\u001b[34mspark.driver.memory 2048m\u001b[0m\n",
      "\u001b[34mspark.driver.memoryOverhead 204m\u001b[0m\n",
      "\u001b[34mspark.driver.defaultJavaOptions -XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled\u001b[0m\n",
      "\u001b[34mspark.executor.memory 12399m\u001b[0m\n",
      "\u001b[34mspark.executor.memoryOverhead 1239m\u001b[0m\n",
      "\u001b[34mspark.executor.cores 4\u001b[0m\n",
      "\u001b[34mspark.executor.defaultJavaOptions -verbose:gc -XX:OnOutOfMemoryError='kill -9 %p' -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:ConcGCThreads=1 -XX:ParallelGCThreads=3 \u001b[0m\n",
      "\u001b[34mspark.executor.instances 2\u001b[0m\n",
      "\u001b[34mspark.default.parallelism 16\u001b[0m\n",
      "\u001b[34m12-05 17:57 root         INFO     Finished Yarn configuration files setup.\u001b[0m\n",
      "\u001b[34m12-05 17:57 root         INFO     No file at /opt/ml/processing/input/conf/configuration.json exists, skipping user configuration\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:33 INFO namenode.NameNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NameNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.2.226.32\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = [-format, -force]\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 2.10.0-amzn-0\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/commons-compress-1.19.jar:/usr/lib/hadoop/lib/xmlenc-0.52.jar:/usr/lib/hadoop/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop/lib/commons-codec-1.4.jar:/usr/lib/hadoop/lib/jersey-server-1.9.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop/lib/jersey-json-1.9.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/activation-1.1.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/mockito-all-1.8.5.jar:/usr/lib/hadoop/lib/asm-3.2.jar:/usr/lib/hadoop/lib/jersey-core-1.9.jar:/usr/lib/hadoop/lib/commons-lang-2.6.jar:/usr/lib/hadoop/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop/lib/commons-lang3-3.4.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/jets3t-0.9.0.jar:/usr/lib/hadoop/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop/lib/stax-api-1.0-2.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/commons-digester-1.8.jar:/usr/lib/hadoop/lib/commons-net-3.1.jar:/usr/lib/hadoop/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop/lib/json-smart-1.3.1.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop/lib/junit-4.11.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/commons-configuration-1.6.jar:/usr/lib/hadoop/lib/servlet-api-2.5.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/commons-io-2.4.jar:/usr/lib/hadoop/lib/curator-client-2.7.1.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-rumen-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-archives-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-common-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-sls-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-openstack-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-ant.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-streaming-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-auth-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-datajoin-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-annotations-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop/.//hadoop-yarn-api.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-azure-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-extras-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-archive-logs-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-nfs-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-aliyun-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-ant-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop/.//hadoop-gridmix-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-distcp-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-yarn-registry.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-aws-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-common.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/xmlenc-0.52.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.4.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.9.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-hdfs/lib/asm-3.2.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.9.jar:/usr/lib/hadoop-hdfs/lib/commons-lang-2.6.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.23.Final.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/xml-apis-1.4.01.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/servlet-api-2.5.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.4.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/xercesImpl-2.12.0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/commons-compress-1.19.jar:/usr/lib/hadoop-yarn/lib/xmlenc-0.52.jar:/usr/lib/hadoop-yarn/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-yarn/lib/paranamer-2.3.jar:/usr/lib/hadoop-yarn/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-yarn/lib/jsch-0.1.54.jar:/usr/lib/hadoop-yarn/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop-yarn/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-codec-1.4.jar:/usr/lib/hadoop-yarn/lib/jersey-server-1.9.jar:/usr/lib/hadoop-yarn/lib/jsp-api-2.1.jar:/usr/lib/hadoop-yarn/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-yarn/lib/jersey-json-1.9.jar:/usr/lib/hadoop-yarn/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-yarn/lib/log4j-1.2.17.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-yarn/lib/jettison-1.1.jar:/usr/lib/hadoop-yarn/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-yarn/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-yarn/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop-yarn/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-yarn/lib/activation-1.1.jar:/usr/lib/hadoop-yarn/lib/avro-1.7.7.jar:/usr/lib/hadoop-yarn/lib/asm-3.2.jar:/usr/lib/hadoop-yarn/lib/jersey-core-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/commons-lang3-3.4.jar:/usr/lib/hadoop-yarn/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-yarn/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-yarn/lib/jets3t-0.9.0.jar:/usr/lib/hadoop-yarn/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop-yarn/lib/stax-api-1.0-2.jar:/usr/lib/hadoop-yarn/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-yarn/lib/commons-digester-1.8.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-net-3.1.jar:/usr/lib/hadoop-yarn/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop-yarn/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.0.1.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop-yarn/lib/guice-3.0.jar:/usr/lib/hadoop-yarn/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/json-smart-1.3.1.jar:/usr/lib/hadoop-yarn/lib/gson-2.2.4.jar:/usr/lib/hadoop-yarn/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-yarn/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-yarn/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-yarn/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-yarn/lib/commons-configuration-1.6.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/servlet-api-2.5.jar:/usr/lib/hadoop-yarn/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-yarn/lib/commons-cli-1.2.jar:/usr/lib/hadoop-yarn/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-yarn/lib/commons-io-2.4.jar:/usr/lib/hadoop-yarn/lib/curator-client-2.7.1.jar:/usr/lib/hadoop-yarn/lib/guava-11.0.2.jar:/usr/lib/hadoop-yarn/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop-yarn/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/commons-compress-1.19.jar:/usr/lib/hadoop-mapreduce/lib/paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/lib/aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/lib/jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-mapreduce/lib/log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/lib/avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/lib/asm-3.2.jar:/usr/lib/hadoop-mapreduce/lib/jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/lib/javax.inject-1.jar:/usr/lib/hadoop-mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/guice-3.0.jar:/usr/lib/hadoop-mapreduce/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/lib/junit-4.11.jar:/usr/lib/hadoop-mapreduce/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-mapreduce/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop-mapreduce/lib/commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/.//mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-mapreduce/.//HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//commons-compress-1.19.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//xmlenc-0.52.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/.//commons-httpclient-3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/.//jsch-0.1.54.jar:/usr/lib/hadoop-mapreduce/.//audience-annotations-0.5.0.jar:/usr/lib/hadoop-mapreduce/.//jaxb-api-2.2.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/.//jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/.//commons-codec-1.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/.//jsp-api-2.1.jar:/usr/lib/hadoop-mapreduce/\u001b[0m\n",
      "\u001b[34m.//netty-3.10.6.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jersey-json-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//httpcore-4.4.11.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//json-io-2.5.1.jar:/usr/lib/hadoop-mapreduce/.//jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//zookeeper-3.4.14.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.3.jar:/usr/lib/hadoop-mapreduce/.//jettison-1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-math3-3.1.1.jar:/usr/lib/hadoop-mapreduce/.//jcip-annotations-1.0-1.jar:/usr/lib/hadoop-mapreduce/.//curator-recipes-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//jackson-xc-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//activation-1.1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/.//asm-3.2.jar:/usr/lib/hadoop-mapreduce/.//jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/.//commons-lang-2.6.jar:/usr/lib/hadoop-mapreduce/.//jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//ehcache-3.3.1.jar:/usr/lib/hadoop-mapreduce/.//commons-lang3-3.4.jar:/usr/lib/hadoop-mapreduce/.//commons-collections-3.2.2.jar:/usr/lib/hadoop-mapreduce/.//jsr305-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//jets3t-0.9.0.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-0.8.0.jar:/usr/lib/hadoop-mapreduce/.//spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop-mapreduce/.//stax-api-1.0-2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-mapreduce/.//commons-digester-1.8.jar:/usr/lib/hadoop-mapreduce/.//guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-mapreduce/.//jersey-client-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-api.jar:/usr/lib/hadoop-mapreduce/.//commons-net-3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//curator-framework-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//jetty-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//metrics-core-3.0.1.jar:/usr/lib/hadoop-mapreduce/.//javax.inject-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-mapreduce/.//java-util-1.9.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//java-xmlbuilder-0.4.jar:/usr/lib/hadoop-mapreduce/.//guice-3.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//json-smart-1.3.1.jar:/usr/lib/hadoop-mapreduce/.//gson-2.2.4.jar:/usr/lib/hadoop-mapreduce/.//woodstox-core-5.0.3.jar:/usr/lib/hadoop-mapreduce/.//commons-logging-1.1.3.jar:/usr/lib/hadoop-mapreduce/.//leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//api-util-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//snappy-java-1.1.7.3.jar:/usr/lib/hadoop-mapreduce/.//commons-configuration-1.6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//fst-2.50.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//servlet-api-2.5.jar:/usr/lib/hadoop-mapreduce/.//httpclient-4.5.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-5.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//jackson-annotations-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-mapreduce/.//jackson-databind-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//commons-cli-1.2.jar:/usr/lib/hadoop-mapreduce/.//stax2-api-3.1.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//curator-client-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//guava-11.0.2.jar:/usr/lib/hadoop-mapreduce/.//commons-beanutils-1.9.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.852.jar:/usr/lib/hadoop-mapreduce/.//nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-common.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r d1e860a34cc1aea3d600c57c5c0270ea41579e8c; compiled by 'ec2-user' on 2020-09-19T02:05Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_272\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:33 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:33 INFO namenode.NameNode: createNameNode [-format, -force]\u001b[0m\n",
      "\u001b[34mFormatting using clusterid: CID-1ffba91e-a3d7-4022-b7f8-8a4b34be365d\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:33 INFO namenode.FSEditLog: Edit logging is async:true\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:33 INFO namenode.FSNamesystem: KeyProvider: null\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:33 INFO namenode.FSNamesystem: fsLock is fair: true\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:33 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:33 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:33 INFO namenode.FSNamesystem: supergroup          = supergroup\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:33 INFO namenode.FSNamesystem: isPermissionEnabled = true\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:33 INFO namenode.FSNamesystem: HA Enabled: false\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:33 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:33 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:33 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:33 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:33 INFO blockmanagement.BlockManager: The block deletion will start around 2021 Dec 05 17:57:33\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:33 INFO util.GSet: Computing capacity for map BlocksMap\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:33 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:33 INFO util.GSet: 2.0% max memory 889 MB = 17.8 MB\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:33 INFO util.GSet: capacity      = 2^21 = 2097152 entries\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:33 INFO blockmanagement.BlockManager: dfs.block.access.token.enable=false\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:33 WARN conf.Configuration: No unit for dfs.heartbeat.interval(3) assuming SECONDS\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:33 WARN conf.Configuration: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:33 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:33 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:33 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:33 INFO blockmanagement.BlockManager: defaultReplication         = 3\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:33 INFO blockmanagement.BlockManager: maxReplication             = 512\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:33 INFO blockmanagement.BlockManager: minReplication             = 1\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:33 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:33 INFO blockmanagement.BlockManager: replicationRecheckInterval = 3000\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:33 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:33 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:33 INFO namenode.FSNamesystem: Append Enabled: true\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:33 INFO namenode.FSDirectory: GLOBAL serial map: bits=24 maxEntries=16777215\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:33 INFO util.GSet: Computing capacity for map INodeMap\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:33 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:33 INFO util.GSet: 1.0% max memory 889 MB = 8.9 MB\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:33 INFO util.GSet: capacity      = 2^20 = 1048576 entries\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:33 INFO namenode.FSDirectory: ACLs enabled? false\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:33 INFO namenode.FSDirectory: XAttrs enabled? true\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:33 INFO namenode.NameNode: Caching file names occurring more than 10 times\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:33 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: falseskipCaptureAccessTimeOnlyChange: false\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:33 INFO util.GSet: Computing capacity for map cachedBlocks\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:33 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:33 INFO util.GSet: 0.25% max memory 889 MB = 2.2 MB\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:33 INFO util.GSet: capacity      = 2^18 = 262144 entries\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:33 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:33 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:33 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:33 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:33 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:33 INFO util.GSet: Computing capacity for map NameNodeRetryCache\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:33 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:33 INFO util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:33 INFO util.GSet: capacity      = 2^15 = 32768 entries\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:33 INFO namenode.FSImage: Allocated new BlockPoolId: BP-876741428-10.2.226.32-1638727053679\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:33 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/namenode has been successfully formatted.\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:33 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:33 INFO namenode.FSImageFormatProtobuf: Image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 323 bytes saved in 0 seconds .\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:33 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:33 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid = 0 when meet shutdown.\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:33 INFO namenode.NameNode: SHUTDOWN_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSHUTDOWN_MSG: Shutting down NameNode at algo-1/10.2.226.32\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m12-05 17:57 smspark-submit INFO     waiting for cluster to be up\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:35 INFO namenode.NameNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NameNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.2.226.32\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = []\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 2.10.0-amzn-0\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/commons-compress-1.19.jar:/usr/lib/hadoop/lib/xmlenc-0.52.jar:/usr/lib/hadoop/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop/lib/commons-codec-1.4.jar:/usr/lib/hadoop/lib/jersey-server-1.9.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop/lib/jersey-json-1.9.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/activation-1.1.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/mockito-all-1.8.5.jar:/usr/lib/hadoop/lib/asm-3.2.jar:/usr/lib/hadoop/lib/jersey-core-1.9.jar:/usr/lib/hadoop/lib/commons-lang-2.6.jar:/usr/lib/hadoop/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop/lib/commons-lang3-3.4.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/jets3t-0.9.0.jar:/usr/lib/hadoop/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop/lib/stax-api-1.0-2.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/commons-digester-1.8.jar:/usr/lib/hadoop/lib/commons-net-3.1.jar:/usr/lib/hadoop/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop/lib/json-smart-1.3.1.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop/lib/junit-4.11.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/commons-configuration-1.6.jar:/usr/lib/hadoop/lib/servlet-api-2.5.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/commons-io-2.4.jar:/usr/lib/hadoop/lib/curator-client-2.7.1.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-rumen-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-archives-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-common-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-sls-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-openstack-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-ant.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-streaming-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-auth-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-datajoin-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-annotations-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop/.//hadoop-yarn-api.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-azure-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-extras-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-archive-logs-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-nfs-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-aliyun-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-ant-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop/.//hadoop-gridmix-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-distcp-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-yarn-registry.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-aws-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-common.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/xmlenc-0.52.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.4.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.9.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-hdfs/lib/asm-3.2.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.9.jar:/usr/lib/hadoop-hdfs/lib/commons-lang-2.6.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.23.Final.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/xml-apis-1.4.01.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/servlet-api-2.5.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.4.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/xercesImpl-2.12.0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/commons-compress-1.19.jar:/usr/lib/hadoop-yarn/lib/xmlenc-0.52.jar:/usr/lib/hadoop-yarn/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-yarn/lib/paranamer-2.3.jar:/usr/lib/hadoop-yarn/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-yarn/lib/jsch-0.1.54.jar:/usr/lib/hadoop-yarn/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop-yarn/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-codec-1.4.jar:/usr/lib/hadoop-yarn/lib/jersey-server-1.9.jar:/usr/lib/hadoop-yarn/lib/jsp-api-2.1.jar:/usr/lib/hadoop-yarn/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-yarn/lib/jersey-json-1.9.jar:/usr/lib/hadoop-yarn/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-yarn/lib/log4j-1.2.17.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-yarn/lib/jettison-1.1.jar:/usr/lib/hadoop-yarn/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-yarn/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-yarn/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop-yarn/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-yarn/lib/activation-1.1.jar:/usr/lib/hadoop-yarn/lib/avro-1.7.7.jar:/usr/lib/hadoop-yarn/lib/asm-3.2.jar:/usr/lib/hadoop-yarn/lib/jersey-core-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/commons-lang3-3.4.jar:/usr/lib/hadoop-yarn/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-yarn/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-yarn/lib/jets3t-0.9.0.jar:/usr/lib/hadoop-yarn/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop-yarn/lib/stax-api-1.0-2.jar:/usr/lib/hadoop-yarn/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-yarn/lib/commons-digester-1.8.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-net-3.1.jar:/usr/lib/hadoop-yarn/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop-yarn/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.0.1.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop-yarn/lib/guice-3.0.jar:/usr/lib/hadoop-yarn/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/json-smart-1.3.1.jar:/usr/lib/hadoop-yarn/lib/gson-2.2.4.jar:/usr/lib/hadoop-yarn/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-yarn/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-yarn/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-yarn/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-yarn/lib/commons-configuration-1.6.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/servlet-api-2.5.jar:/usr/lib/hadoop-yarn/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-yarn/lib/commons-cli-1.2.jar:/usr/lib/hadoop-yarn/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-yarn/lib/commons-io-2.4.jar:/usr/lib/hadoop-yarn/lib/curator-client-2.7.1.jar:/usr/lib/hadoop-yarn/lib/guava-11.0.2.jar:/usr/lib/hadoop-yarn/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop-yarn/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/commons-compress-1.19.jar:/usr/lib/hadoop-mapreduce/lib/paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/lib/aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/lib/jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-mapreduce/lib/log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/lib/avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/lib/asm-3.2.jar:/usr/lib/hadoop-mapreduce/lib/jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/lib/javax.inject-1.jar:/usr/lib/hadoop-mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/guice-3.0.jar:/usr/lib/hadoop-mapreduce/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/lib/junit-4.11.jar:/usr/lib/hadoop-mapreduce/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-mapreduce/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop-mapreduce/lib/commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/.//mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-mapreduce/.//HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//commons-compress-1.19.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//xmlenc-0.52.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/.//commons-httpclient-3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/.//jsch-0.1.54.jar:/usr/lib/hadoop-mapreduce/.//audience-annotations-0.5.0.jar:/usr/lib/hadoop-mapreduce/.//jaxb-api-2.2.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/.//jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/.//commons-codec-1.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/.//jsp-api-2.1.jar:/usr/lib/hadoop-mapreduce/\u001b[0m\n",
      "\u001b[34m.//netty-3.10.6.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jersey-json-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//httpcore-4.4.11.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//json-io-2.5.1.jar:/usr/lib/hadoop-mapreduce/.//jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//zookeeper-3.4.14.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.3.jar:/usr/lib/hadoop-mapreduce/.//jettison-1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-math3-3.1.1.jar:/usr/lib/hadoop-mapreduce/.//jcip-annotations-1.0-1.jar:/usr/lib/hadoop-mapreduce/.//curator-recipes-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//jackson-xc-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//activation-1.1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/.//asm-3.2.jar:/usr/lib/hadoop-mapreduce/.//jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/.//commons-lang-2.6.jar:/usr/lib/hadoop-mapreduce/.//jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//ehcache-3.3.1.jar:/usr/lib/hadoop-mapreduce/.//commons-lang3-3.4.jar:/usr/lib/hadoop-mapreduce/.//commons-collections-3.2.2.jar:/usr/lib/hadoop-mapreduce/.//jsr305-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//jets3t-0.9.0.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-0.8.0.jar:/usr/lib/hadoop-mapreduce/.//spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop-mapreduce/.//stax-api-1.0-2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-mapreduce/.//commons-digester-1.8.jar:/usr/lib/hadoop-mapreduce/.//guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-mapreduce/.//jersey-client-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-api.jar:/usr/lib/hadoop-mapreduce/.//commons-net-3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//curator-framework-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//jetty-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//metrics-core-3.0.1.jar:/usr/lib/hadoop-mapreduce/.//javax.inject-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-mapreduce/.//java-util-1.9.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//java-xmlbuilder-0.4.jar:/usr/lib/hadoop-mapreduce/.//guice-3.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//json-smart-1.3.1.jar:/usr/lib/hadoop-mapreduce/.//gson-2.2.4.jar:/usr/lib/hadoop-mapreduce/.//woodstox-core-5.0.3.jar:/usr/lib/hadoop-mapreduce/.//commons-logging-1.1.3.jar:/usr/lib/hadoop-mapreduce/.//leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//api-util-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//snappy-java-1.1.7.3.jar:/usr/lib/hadoop-mapreduce/.//commons-configuration-1.6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//fst-2.50.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//servlet-api-2.5.jar:/usr/lib/hadoop-mapreduce/.//httpclient-4.5.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-5.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//jackson-annotations-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-mapreduce/.//jackson-databind-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//commons-cli-1.2.jar:/usr/lib/hadoop-mapreduce/.//stax2-api-3.1.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//curator-client-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//guava-11.0.2.jar:/usr/lib/hadoop-mapreduce/.//commons-beanutils-1.9.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.852.jar:/usr/lib/hadoop-mapreduce/.//nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-common.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r d1e860a34cc1aea3d600c57c5c0270ea41579e8c; compiled by 'ec2-user' on 2020-09-19T02:05Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_272\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:35 INFO resourcemanager.ResourceManager: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting ResourceManager\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.2.226.32\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = []\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 2.10.0-amzn-0\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/commons-compress-1.19.jar:/usr/lib/hadoop/lib/xmlenc-0.52.jar:/usr/lib/hadoop/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop/lib/commons-codec-1.4.jar:/usr/lib/hadoop/lib/jersey-server-1.9.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop/lib/jersey-json-1.9.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/activation-1.1.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/mockito-all-1.8.5.jar:/usr/lib/hadoop/lib/asm-3.2.jar:/usr/lib/hadoop/lib/jersey-core-1.9.jar:/usr/lib/hadoop/lib/commons-lang-2.6.jar:/usr/lib/hadoop/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop/lib/commons-lang3-3.4.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/jets3t-0.9.0.jar:/usr/lib/hadoop/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop/lib/stax-api-1.0-2.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/commons-digester-1.8.jar:/usr/lib/hadoop/lib/commons-net-3.1.jar:/usr/lib/hadoop/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop/lib/json-smart-1.3.1.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop/lib/junit-4.11.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/commons-configuration-1.6.jar:/usr/lib/hadoop/lib/servlet-api-2.5.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/commons-io-2.4.jar:/usr/lib/hadoop/lib/curator-client-2.7.1.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-rumen-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-archives-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-common-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-sls-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-openstack-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-ant.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-streaming-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-auth-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-datajoin-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-annotations-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop/.//hadoop-yarn-api.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-azure-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-extras-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-archive-logs-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-nfs-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-aliyun-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-ant-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop/.//hadoop-gridmix-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-distcp-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-yarn-registry.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-aws-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-common.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/xmlenc-0.52.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.4.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.9.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-hdfs/lib/asm-3.2.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.9.jar:/usr/lib/hadoop-hdfs/lib/commons-lang-2.6.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.23.Final.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/xml-apis-1.4.01.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/servlet-api-2.5.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.4.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/xercesImpl-2.12.0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/commons-compress-1.19.jar:/usr/lib/hadoop-yarn/lib/xmlenc-0.52.jar:/usr/lib/hadoop-yarn/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-yarn/lib/paranamer-2.3.jar:/usr/lib/hadoop-yarn/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-yarn/lib/jsch-0.1.54.jar:/usr/lib/hadoop-yarn/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop-yarn/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-codec-1.4.jar:/usr/lib/hadoop-yarn/lib/jersey-server-1.9.jar:/usr/lib/hadoop-yarn/lib/jsp-api-2.1.jar:/usr/lib/hadoop-yarn/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-yarn/lib/jersey-json-1.9.jar:/usr/lib/hadoop-yarn/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-yarn/lib/log4j-1.2.17.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-yarn/lib/jettison-1.1.jar:/usr/lib/hadoop-yarn/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-yarn/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-yarn/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop-yarn/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-yarn/lib/activation-1.1.jar:/usr/lib/hadoop-yarn/lib/avro-1.7.7.jar:/usr/lib/hadoop-yarn/lib/asm-3.2.jar:/usr/lib/hadoop-yarn/lib/jersey-core-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/commons-lang3-3.4.jar:/usr/lib/hadoop-yarn/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-yarn/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-yarn/lib/jets3t-0.9.0.jar:/usr/lib/hadoop-yarn/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop-yarn/lib/stax-api-1.0-2.jar:/usr/lib/hadoop-yarn/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-yarn/lib/commons-digester-1.8.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-net-3.1.jar:/usr/lib/hadoop-yarn/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop-yarn/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.0.1.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop-yarn/lib/guice-3.0.jar:/usr/lib/hadoop-yarn/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/json-smart-1.3.1.jar:/usr/lib/hadoop-yarn/lib/gson-2.2.4.jar:/usr/lib/hadoop-yarn/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-yarn/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-yarn/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-yarn/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-yarn/lib/commons-configuration-1.6.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/servlet-api-2.5.jar:/usr/lib/hadoop-yarn/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-yarn/lib/commons-cli-1.2.jar:/usr/lib/hadoop-yarn/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-yarn/lib/commons-io-2.4.jar:/usr/lib/hadoop-yarn/lib/curator-client-2.7.1.jar:/usr/lib/hadoop-yarn/lib/guava-11.0.2.jar:/usr/lib/hadoop-yarn/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop-yarn/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/commons-compress-1.19.jar:/usr/lib/hadoop-mapreduce/lib/paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/lib/aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/lib/jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-mapreduce/lib/log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/lib/avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/lib/asm-3.2.jar:/usr/lib/hadoop-mapreduce/lib/jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/lib/javax.inject-1.jar:/usr/lib/hadoop-mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/guice-3.0.jar:/usr/lib/hadoop-mapreduce/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/lib/junit-4.11.jar:/usr/lib/hadoop-mapreduce/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-mapreduce/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop-mapreduce/lib/commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/.//mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-mapreduce/.//HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//commons-compress-1.19.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//xmlenc-0.52.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/.//commons-httpclient-3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/.//jsch-0.1.54.jar:/usr/lib/hadoop-mapreduce/.//audience-annotations-0.5.0.jar:/usr/lib/hadoop-mapreduce/.//jaxb-api-2.2.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/.//jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/.//commons-codec-1.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//jersey-server-1.9.jar:/usr/lib/hadoop-m\u001b[0m\n",
      "\u001b[34mapreduce/.//jsp-api-2.1.jar:/usr/lib/hadoop-mapreduce/.//netty-3.10.6.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jersey-json-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//httpcore-4.4.11.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//json-io-2.5.1.jar:/usr/lib/hadoop-mapreduce/.//jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//zookeeper-3.4.14.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.3.jar:/usr/lib/hadoop-mapreduce/.//jettison-1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-math3-3.1.1.jar:/usr/lib/hadoop-mapreduce/.//jcip-annotations-1.0-1.jar:/usr/lib/hadoop-mapreduce/.//curator-recipes-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//jackson-xc-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//activation-1.1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/.//asm-3.2.jar:/usr/lib/hadoop-mapreduce/.//jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/.//commons-lang-2.6.jar:/usr/lib/hadoop-mapreduce/.//jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//ehcache-3.3.1.jar:/usr/lib/hadoop-mapreduce/.//commons-lang3-3.4.jar:/usr/lib/hadoop-mapreduce/.//commons-collections-3.2.2.jar:/usr/lib/hadoop-mapreduce/.//jsr305-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//jets3t-0.9.0.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-0.8.0.jar:/usr/lib/hadoop-mapreduce/.//spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop-mapreduce/.//stax-api-1.0-2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-mapreduce/.//commons-digester-1.8.jar:/usr/lib/hadoop-mapreduce/.//guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-mapreduce/.//jersey-client-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-api.jar:/usr/lib/hadoop-mapreduce/.//commons-net-3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//curator-framework-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//jetty-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//metrics-core-3.0.1.jar:/usr/lib/hadoop-mapreduce/.//javax.inject-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-mapreduce/.//java-util-1.9.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//java-xmlbuilder-0.4.jar:/usr/lib/hadoop-mapreduce/.//guice-3.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//json-smart-1.3.1.jar:/usr/lib/hadoop-mapreduce/.//gson-2.2.4.jar:/usr/lib/hadoop-mapreduce/.//woodstox-core-5.0.3.jar:/usr/lib/hadoop-mapreduce/.//commons-logging-1.1.3.jar:/usr/lib/hadoop-mapreduce/.//leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//api-util-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//snappy-java-1.1.7.3.jar:/usr/lib/hadoop-mapreduce/.//commons-configuration-1.6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//fst-2.50.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//servlet-api-2.5.jar:/usr/lib/hadoop-mapreduce/.//httpclient-4.5.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-5.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//jackson-annotations-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-mapreduce/.//jackson-databind-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//commons-cli-1.2.jar:/usr/lib/hadoop-mapreduce/.//stax2-api-3.1.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//curator-client-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//guava-11.0.2.jar:/usr/lib/hadoop-mapreduce/.//commons-beanutils-1.9.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.852.jar:/usr/lib/hadoop-mapreduce/.//nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/commons-compress-1.19.jar:/usr/lib/hadoop-yarn/lib/xmlenc-0.52.jar:/usr/lib/hadoop-yarn/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-yarn/lib/paranamer-2.3.jar:/usr/lib/hadoop-yarn/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-yarn/lib/jsch-0.1.54.jar:/usr/lib/hadoop-yarn/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop-yarn/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-codec-1.4.jar:/usr/lib/hadoop-yarn/lib/jersey-server-1.9.jar:/usr/lib/hadoop-yarn/lib/jsp-api-2.1.jar:/usr/lib/hadoop-yarn/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-yarn/lib/jersey-json-1.9.jar:/usr/lib/hadoop-yarn/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-yarn/lib/log4j-1.2.17.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-yarn/lib/jettison-1.1.jar:/usr/lib/hadoop-yarn/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-yarn/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-yarn/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop-yarn/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-yarn/lib/activation-1.1.jar:/usr/lib/hadoop-yarn/lib/avro-1.7.7.jar:/usr/lib/hadoop-yarn/lib/asm-3.2.jar:/usr/lib/hadoop-yarn/lib/jersey-core-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/commons-lang3-3.4.jar:/usr/lib/hadoop-yarn/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-yarn/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-yarn/lib/jets3t-0.9.0.jar:/usr/lib/hadoop-yarn/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop-yarn/lib/stax-api-1.0-2.jar:/usr/lib/hadoop-yarn/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-yarn/lib/commons-digester-1.8.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-net-3.1.jar:/usr/lib/hadoop-yarn/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop-yarn/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.0.1.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop-yarn/lib/guice-3.0.jar:/usr/lib/hadoop-yarn/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/json-smart-1.3.1.jar:/usr/lib/hadoop-yarn/lib/gson-2.2.4.jar:/usr/lib/hadoop-yarn/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-yarn/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-yarn/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-yarn/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-yarn/lib/commons-configuration-1.6.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/servlet-api-2.5.jar:/usr/lib/hadoop-yarn/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-yarn/lib/commons-cli-1.2.jar:/usr/lib/hadoop-yarn/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-yarn/lib/commons-io-2.4.jar:/usr/lib/hadoop-yarn/lib/curator-client-2.7.1.jar:/usr/lib/hadoop-yarn/lib/guava-11.0.2.jar:/usr/lib/hadoop-yarn/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop-yarn/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop/etc/hadoop/rm-config/log4j.properties:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-coprocessor-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-protocol-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/metrics-core-2.2.0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jackson-core-2.6.7.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-annotations-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/htrace-core-3.1.0-incubating.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/netty-all-4.0.23.Final.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/joni-2.1.2.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jcodings-1.0.8.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-client-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-common-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-csv-1.0.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r d1e860a34cc1aea3d600c57c5c0270ea41579e8c; compiled by 'ec2-user' on 2020-09-19T02:05Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_272\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:35 INFO datanode.DataNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting DataNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.2.226.32\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = []\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 2.10.0-amzn-0\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/commons-compress-1.19.jar:/usr/lib/hadoop/lib/xmlenc-0.52.jar:/usr/lib/hadoop/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop/lib/commons-codec-1.4.jar:/usr/lib/hadoop/lib/jersey-server-1.9.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop/lib/jersey-json-1.9.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/activation-1.1.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/mockito-all-1.8.5.jar:/usr/lib/hadoop/lib/asm-3.2.jar:/usr/lib/hadoop/lib/jersey-core-1.9.jar:/usr/lib/hadoop/lib/commons-lang-2.6.jar:/usr/lib/hadoop/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop/lib/commons-lang3-3.4.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/jets3t-0.9.0.jar:/usr/lib/hadoop/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop/lib/stax-api-1.0-2.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/commons-digester-1.8.jar:/usr/lib/hadoop/lib/commons-net-3.1.jar:/usr/lib/hadoop/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop/lib/json-smart-1.3.1.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop/lib/junit-4.11.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/commons-configuration-1.6.jar:/usr/lib/hadoop/lib/servlet-api-2.5.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/commons-io-2.4.jar:/usr/lib/hadoop/lib/curator-client-2.7.1.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-rumen-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-archives-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-common-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-sls-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-openstack-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-ant.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-streaming-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-auth-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-datajoin-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-annotations-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop/.//hadoop-yarn-api.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-azure-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-extras-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-archive-logs-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-nfs-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-aliyun-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-ant-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop/.//hadoop-gridmix-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-distcp-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-yarn-registry.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-aws-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-common.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/xmlenc-0.52.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.4.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.9.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-hdfs/lib/asm-3.2.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.9.jar:/usr/lib/hadoop-hdfs/lib/commons-lang-2.6.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.23.Final.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/xml-apis-1.4.01.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/servlet-api-2.5.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.4.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/xercesImpl-2.12.0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/commons-compress-1.19.jar:/usr/lib/hadoop-yarn/lib/xmlenc-0.52.jar:/usr/lib/hadoop-yarn/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-yarn/lib/paranamer-2.3.jar:/usr/lib/hadoop-yarn/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-yarn/lib/jsch-0.1.54.jar:/usr/lib/hadoop-yarn/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop-yarn/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-codec-1.4.jar:/usr/lib/hadoop-yarn/lib/jersey-server-1.9.jar:/usr/lib/hadoop-yarn/lib/jsp-api-2.1.jar:/usr/lib/hadoop-yarn/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-yarn/lib/jersey-json-1.9.jar:/usr/lib/hadoop-yarn/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-yarn/lib/log4j-1.2.17.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-yarn/lib/jettison-1.1.jar:/usr/lib/hadoop-yarn/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-yarn/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-yarn/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop-yarn/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-yarn/lib/activation-1.1.jar:/usr/lib/hadoop-yarn/lib/avro-1.7.7.jar:/usr/lib/hadoop-yarn/lib/asm-3.2.jar:/usr/lib/hadoop-yarn/lib/jersey-core-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/commons-lang3-3.4.jar:/usr/lib/hadoop-yarn/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-yarn/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-yarn/lib/jets3t-0.9.0.jar:/usr/lib/hadoop-yarn/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop-yarn/lib/stax-api-1.0-2.jar:/usr/lib/hadoop-yarn/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-yarn/lib/commons-digester-1.8.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-net-3.1.jar:/usr/lib/hadoop-yarn/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop-yarn/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.0.1.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop-yarn/lib/guice-3.0.jar:/usr/lib/hadoop-yarn/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/json-smart-1.3.1.jar:/usr/lib/hadoop-yarn/lib/gson-2.2.4.jar:/usr/lib/hadoop-yarn/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-yarn/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-yarn/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-yarn/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-yarn/lib/commons-configuration-1.6.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/servlet-api-2.5.jar:/usr/lib/hadoop-yarn/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-yarn/lib/commons-cli-1.2.jar:/usr/lib/hadoop-yarn/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-yarn/lib/commons-io-2.4.jar:/usr/lib/hadoop-yarn/lib/curator-client-2.7.1.jar:/usr/lib/hadoop-yarn/lib/guava-11.0.2.jar:/usr/lib/hadoop-yarn/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop-yarn/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/commons-compress-1.19.jar:/usr/lib/hadoop-mapreduce/lib/paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/lib/aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/lib/jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-mapreduce/lib/log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/lib/avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/lib/asm-3.2.jar:/usr/lib/hadoop-mapreduce/lib/jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/lib/javax.inject-1.jar:/usr/lib/hadoop-mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/guice-3.0.jar:/usr/lib/hadoop-mapreduce/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/lib/junit-4.11.jar:/usr/lib/hadoop-mapreduce/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-mapreduce/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop-mapreduce/lib/commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/.//mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-mapreduce/.//HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//commons-compress-1.19.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//xmlenc-0.52.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/.//commons-httpclient-3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/.//jsch-0.1.54.jar:/usr/lib/hadoop-mapreduce/.//audience-annotations-0.5.0.jar:/usr/lib/hadoop-mapreduce/.//jaxb-api-2.2.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/.//jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/.//commons-codec-1.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/.//jsp-api-2.1.jar:/usr/lib/hadoop-mapreduce/\u001b[0m\n",
      "\u001b[34m.//netty-3.10.6.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jersey-json-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//httpcore-4.4.11.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//json-io-2.5.1.jar:/usr/lib/hadoop-mapreduce/.//jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//zookeeper-3.4.14.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.3.jar:/usr/lib/hadoop-mapreduce/.//jettison-1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-math3-3.1.1.jar:/usr/lib/hadoop-mapreduce/.//jcip-annotations-1.0-1.jar:/usr/lib/hadoop-mapreduce/.//curator-recipes-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//jackson-xc-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//activation-1.1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/.//asm-3.2.jar:/usr/lib/hadoop-mapreduce/.//jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/.//commons-lang-2.6.jar:/usr/lib/hadoop-mapreduce/.//jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//ehcache-3.3.1.jar:/usr/lib/hadoop-mapreduce/.//commons-lang3-3.4.jar:/usr/lib/hadoop-mapreduce/.//commons-collections-3.2.2.jar:/usr/lib/hadoop-mapreduce/.//jsr305-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//jets3t-0.9.0.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-0.8.0.jar:/usr/lib/hadoop-mapreduce/.//spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop-mapreduce/.//stax-api-1.0-2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-mapreduce/.//commons-digester-1.8.jar:/usr/lib/hadoop-mapreduce/.//guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-mapreduce/.//jersey-client-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-api.jar:/usr/lib/hadoop-mapreduce/.//commons-net-3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//curator-framework-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//jetty-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//metrics-core-3.0.1.jar:/usr/lib/hadoop-mapreduce/.//javax.inject-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-mapreduce/.//java-util-1.9.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//java-xmlbuilder-0.4.jar:/usr/lib/hadoop-mapreduce/.//guice-3.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//json-smart-1.3.1.jar:/usr/lib/hadoop-mapreduce/.//gson-2.2.4.jar:/usr/lib/hadoop-mapreduce/.//woodstox-core-5.0.3.jar:/usr/lib/hadoop-mapreduce/.//commons-logging-1.1.3.jar:/usr/lib/hadoop-mapreduce/.//leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//api-util-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//snappy-java-1.1.7.3.jar:/usr/lib/hadoop-mapreduce/.//commons-configuration-1.6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//fst-2.50.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//servlet-api-2.5.jar:/usr/lib/hadoop-mapreduce/.//httpclient-4.5.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-5.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//jackson-annotations-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-mapreduce/.//jackson-databind-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//commons-cli-1.2.jar:/usr/lib/hadoop-mapreduce/.//stax2-api-3.1.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//curator-client-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//guava-11.0.2.jar:/usr/lib/hadoop-mapreduce/.//commons-beanutils-1.9.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.852.jar:/usr/lib/hadoop-mapreduce/.//nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-common.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r d1e860a34cc1aea3d600c57c5c0270ea41579e8c; compiled by 'ec2-user' on 2020-09-19T02:05Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_272\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:35 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:35 INFO resourcemanager.ResourceManager: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:35 INFO datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:35 INFO nodemanager.NodeManager: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NodeManager\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.2.226.32\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = []\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 2.10.0-amzn-0\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/commons-compress-1.19.jar:/usr/lib/hadoop/lib/xmlenc-0.52.jar:/usr/lib/hadoop/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop/lib/commons-codec-1.4.jar:/usr/lib/hadoop/lib/jersey-server-1.9.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop/lib/jersey-json-1.9.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/activation-1.1.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/mockito-all-1.8.5.jar:/usr/lib/hadoop/lib/asm-3.2.jar:/usr/lib/hadoop/lib/jersey-core-1.9.jar:/usr/lib/hadoop/lib/commons-lang-2.6.jar:/usr/lib/hadoop/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop/lib/commons-lang3-3.4.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/jets3t-0.9.0.jar:/usr/lib/hadoop/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop/lib/stax-api-1.0-2.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/commons-digester-1.8.jar:/usr/lib/hadoop/lib/commons-net-3.1.jar:/usr/lib/hadoop/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop/lib/json-smart-1.3.1.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop/lib/junit-4.11.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/commons-configuration-1.6.jar:/usr/lib/hadoop/lib/servlet-api-2.5.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/commons-io-2.4.jar:/usr/lib/hadoop/lib/curator-client-2.7.1.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-rumen-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-archives-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-common-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-sls-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-openstack-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-ant.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-streaming-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-auth-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-datajoin-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-annotations-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop/.//hadoop-yarn-api.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-azure-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-extras-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-archive-logs-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-nfs-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-aliyun-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-ant-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop/.//hadoop-gridmix-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-distcp-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-yarn-registry.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-aws-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-common.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/xmlenc-0.52.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.4.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.9.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-hdfs/lib/asm-3.2.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.9.jar:/usr/lib/hadoop-hdfs/lib/commons-lang-2.6.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.23.Final.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/xml-apis-1.4.01.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/servlet-api-2.5.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.4.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/xercesImpl-2.12.0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/commons-compress-1.19.jar:/usr/lib/hadoop-yarn/lib/xmlenc-0.52.jar:/usr/lib/hadoop-yarn/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-yarn/lib/paranamer-2.3.jar:/usr/lib/hadoop-yarn/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-yarn/lib/jsch-0.1.54.jar:/usr/lib/hadoop-yarn/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop-yarn/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-codec-1.4.jar:/usr/lib/hadoop-yarn/lib/jersey-server-1.9.jar:/usr/lib/hadoop-yarn/lib/jsp-api-2.1.jar:/usr/lib/hadoop-yarn/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-yarn/lib/jersey-json-1.9.jar:/usr/lib/hadoop-yarn/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-yarn/lib/log4j-1.2.17.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-yarn/lib/jettison-1.1.jar:/usr/lib/hadoop-yarn/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-yarn/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-yarn/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop-yarn/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-yarn/lib/activation-1.1.jar:/usr/lib/hadoop-yarn/lib/avro-1.7.7.jar:/usr/lib/hadoop-yarn/lib/asm-3.2.jar:/usr/lib/hadoop-yarn/lib/jersey-core-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/commons-lang3-3.4.jar:/usr/lib/hadoop-yarn/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-yarn/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-yarn/lib/jets3t-0.9.0.jar:/usr/lib/hadoop-yarn/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop-yarn/lib/stax-api-1.0-2.jar:/usr/lib/hadoop-yarn/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-yarn/lib/commons-digester-1.8.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-net-3.1.jar:/usr/lib/hadoop-yarn/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop-yarn/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.0.1.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop-yarn/lib/guice-3.0.jar:/usr/lib/hadoop-yarn/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/json-smart-1.3.1.jar:/usr/lib/hadoop-yarn/lib/gson-2.2.4.jar:/usr/lib/hadoop-yarn/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-yarn/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-yarn/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-yarn/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-yarn/lib/commons-configuration-1.6.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/servlet-api-2.5.jar:/usr/lib/hadoop-yarn/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-yarn/lib/commons-cli-1.2.jar:/usr/lib/hadoop-yarn/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-yarn/lib/commons-io-2.4.jar:/usr/lib/hadoop-yarn/lib/curator-client-2.7.1.jar:/usr/lib/hadoop-yarn/lib/guava-11.0.2.jar:/usr/lib/hadoop-yarn/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop-yarn/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/commons-compress-1.19.jar:/usr/lib/hadoop-mapreduce/lib/paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/lib/aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/lib/jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-mapreduce/lib/log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/lib/avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/lib/asm-3.2.jar:/usr/lib/hadoop-mapreduce/lib/jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/lib/javax.inject-1.jar:/usr/lib/hadoop-mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/guice-3.0.jar:/usr/lib/hadoop-mapreduce/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/lib/junit-4.11.jar:/usr/lib/hadoop-mapreduce/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-mapreduce/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop-mapreduce/lib/commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/.//mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-mapreduce/.//HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//commons-compress-1.19.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//xmlenc-0.52.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/.//commons-httpclient-3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/.//jsch-0.1.54.jar:/usr/lib/hadoop-mapreduce/.//audience-annotations-0.5.0.jar:/usr/lib/hadoop-mapreduce/.//jaxb-api-2.2.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/.//jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/.//commons-codec-1.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//jersey-server-1.9.jar:/usr/lib/hadoop-m\u001b[0m\n",
      "\u001b[34mapreduce/.//jsp-api-2.1.jar:/usr/lib/hadoop-mapreduce/.//netty-3.10.6.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jersey-json-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//httpcore-4.4.11.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//json-io-2.5.1.jar:/usr/lib/hadoop-mapreduce/.//jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//zookeeper-3.4.14.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.3.jar:/usr/lib/hadoop-mapreduce/.//jettison-1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-math3-3.1.1.jar:/usr/lib/hadoop-mapreduce/.//jcip-annotations-1.0-1.jar:/usr/lib/hadoop-mapreduce/.//curator-recipes-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//jackson-xc-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//activation-1.1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/.//asm-3.2.jar:/usr/lib/hadoop-mapreduce/.//jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/.//commons-lang-2.6.jar:/usr/lib/hadoop-mapreduce/.//jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//ehcache-3.3.1.jar:/usr/lib/hadoop-mapreduce/.//commons-lang3-3.4.jar:/usr/lib/hadoop-mapreduce/.//commons-collections-3.2.2.jar:/usr/lib/hadoop-mapreduce/.//jsr305-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//jets3t-0.9.0.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-0.8.0.jar:/usr/lib/hadoop-mapreduce/.//spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop-mapreduce/.//stax-api-1.0-2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-mapreduce/.//commons-digester-1.8.jar:/usr/lib/hadoop-mapreduce/.//guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-mapreduce/.//jersey-client-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-api.jar:/usr/lib/hadoop-mapreduce/.//commons-net-3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//curator-framework-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//jetty-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//metrics-core-3.0.1.jar:/usr/lib/hadoop-mapreduce/.//javax.inject-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-mapreduce/.//java-util-1.9.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//java-xmlbuilder-0.4.jar:/usr/lib/hadoop-mapreduce/.//guice-3.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//json-smart-1.3.1.jar:/usr/lib/hadoop-mapreduce/.//gson-2.2.4.jar:/usr/lib/hadoop-mapreduce/.//woodstox-core-5.0.3.jar:/usr/lib/hadoop-mapreduce/.//commons-logging-1.1.3.jar:/usr/lib/hadoop-mapreduce/.//leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//api-util-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//snappy-java-1.1.7.3.jar:/usr/lib/hadoop-mapreduce/.//commons-configuration-1.6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//fst-2.50.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//servlet-api-2.5.jar:/usr/lib/hadoop-mapreduce/.//httpclient-4.5.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-5.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//jackson-annotations-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-mapreduce/.//jackson-databind-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//commons-cli-1.2.jar:/usr/lib/hadoop-mapreduce/.//stax2-api-3.1.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//curator-client-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//guava-11.0.2.jar:/usr/lib/hadoop-mapreduce/.//commons-beanutils-1.9.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.852.jar:/usr/lib/hadoop-mapreduce/.//nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/commons-compress-1.19.jar:/usr/lib/hadoop-yarn/lib/xmlenc-0.52.jar:/usr/lib/hadoop-yarn/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-yarn/lib/paranamer-2.3.jar:/usr/lib/hadoop-yarn/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-yarn/lib/jsch-0.1.54.jar:/usr/lib/hadoop-yarn/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop-yarn/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-codec-1.4.jar:/usr/lib/hadoop-yarn/lib/jersey-server-1.9.jar:/usr/lib/hadoop-yarn/lib/jsp-api-2.1.jar:/usr/lib/hadoop-yarn/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-yarn/lib/jersey-json-1.9.jar:/usr/lib/hadoop-yarn/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-yarn/lib/log4j-1.2.17.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-yarn/lib/jettison-1.1.jar:/usr/lib/hadoop-yarn/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-yarn/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-yarn/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop-yarn/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-yarn/lib/activation-1.1.jar:/usr/lib/hadoop-yarn/lib/avro-1.7.7.jar:/usr/lib/hadoop-yarn/lib/asm-3.2.jar:/usr/lib/hadoop-yarn/lib/jersey-core-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/commons-lang3-3.4.jar:/usr/lib/hadoop-yarn/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-yarn/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-yarn/lib/jets3t-0.9.0.jar:/usr/lib/hadoop-yarn/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop-yarn/lib/stax-api-1.0-2.jar:/usr/lib/hadoop-yarn/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-yarn/lib/commons-digester-1.8.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-net-3.1.jar:/usr/lib/hadoop-yarn/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop-yarn/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.0.1.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop-yarn/lib/guice-3.0.jar:/usr/lib/hadoop-yarn/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/json-smart-1.3.1.jar:/usr/lib/hadoop-yarn/lib/gson-2.2.4.jar:/usr/lib/hadoop-yarn/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-yarn/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-yarn/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-yarn/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-yarn/lib/commons-configuration-1.6.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/servlet-api-2.5.jar:/usr/lib/hadoop-yarn/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-yarn/lib/commons-cli-1.2.jar:/usr/lib/hadoop-yarn/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-yarn/lib/commons-io-2.4.jar:/usr/lib/hadoop-yarn/lib/curator-client-2.7.1.jar:/usr/lib/hadoop-yarn/lib/guava-11.0.2.jar:/usr/lib/hadoop-yarn/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop-yarn/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop/etc/hadoop/nm-config/log4j.properties:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-coprocessor-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-protocol-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/metrics-core-2.2.0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jackson-core-2.6.7.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-annotations-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/htrace-core-3.1.0-incubating.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/netty-all-4.0.23.Final.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/joni-2.1.2.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jcodings-1.0.8.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-client-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-common-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-csv-1.0.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r d1e860a34cc1aea3d600c57c5c0270ea41579e8c; compiled by 'ec2-user' on 2020-09-19T02:05Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_272\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:35 INFO nodemanager.NodeManager: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:35 INFO namenode.NameNode: createNameNode []\u001b[0m\n",
      "\u001b[35m12-05 17:57 smspark.cli  INFO     Parsing arguments. argv: ['/usr/local/bin/smspark-submit', '--local-spark-event-logs-dir', '/opt/ml/processing/spark-events/', '/opt/ml/processing/input/code/preprocess.py', '--s3_input_bucket', 'sagemaker-us-east-1-193890026231', '--s3_input_key_prefix', 'sagemaker/spark-preprocess-demo/2021-12-05-17-53-35/input/raw/abalone', '--s3_output_bucket', 'sagemaker-us-east-1-193890026231', '--s3_output_key_prefix', 'sagemaker/spark-preprocess-demo/2021-12-05-17-53-35/input/preprocessed/abalone']\u001b[0m\n",
      "\u001b[35m12-05 17:57 smspark.cli  INFO     Raw spark options before processing: {'class_': None, 'jars': None, 'py_files': None, 'files': None, 'verbose': False}\u001b[0m\n",
      "\u001b[35m12-05 17:57 smspark.cli  INFO     App and app arguments: ['/opt/ml/processing/input/code/preprocess.py', '--s3_input_bucket', 'sagemaker-us-east-1-193890026231', '--s3_input_key_prefix', 'sagemaker/spark-preprocess-demo/2021-12-05-17-53-35/input/raw/abalone', '--s3_output_bucket', 'sagemaker-us-east-1-193890026231', '--s3_output_key_prefix', 'sagemaker/spark-preprocess-demo/2021-12-05-17-53-35/input/preprocessed/abalone']\u001b[0m\n",
      "\u001b[35m12-05 17:57 smspark.cli  INFO     Rendered spark options: {'class_': None, 'jars': None, 'py_files': None, 'files': None, 'verbose': False}\u001b[0m\n",
      "\u001b[35m12-05 17:57 smspark.cli  INFO     Initializing processing job.\u001b[0m\n",
      "\u001b[35m12-05 17:57 smspark-submit INFO     {'current_host': 'algo-2', 'hosts': ['algo-1', 'algo-2']}\u001b[0m\n",
      "\u001b[35m12-05 17:57 smspark-submit INFO     {'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:193890026231:processing-job/sm-spark-2021-12-05-17-53-36-279', 'ProcessingJobName': 'sm-spark-2021-12-05-17-53-36-279', 'AppSpecification': {'ImageUri': '173754725891.dkr.ecr.us-east-1.amazonaws.com/sagemaker-spark-processing:2.4-cpu', 'ContainerEntrypoint': ['smspark-submit', '--local-spark-event-logs-dir', '/opt/ml/processing/spark-events/', '/opt/ml/processing/input/code/preprocess.py'], 'ContainerArguments': ['--s3_input_bucket', 'sagemaker-us-east-1-193890026231', '--s3_input_key_prefix', 'sagemaker/spark-preprocess-demo/2021-12-05-17-53-35/input/raw/abalone', '--s3_output_bucket', 'sagemaker-us-east-1-193890026231', '--s3_output_key_prefix', 'sagemaker/spark-preprocess-demo/2021-12-05-17-53-35/input/preprocessed/abalone']}, 'ProcessingInputs': [{'InputName': 'code', 'AppManaged': False, 'S3Input': {'LocalPath': '/opt/ml/processing/input/code', 'S3Uri': 's3://sagemaker-us-east-1-193890026231/sm-spark-2021-12-05-17-53-36-279/input/code/preprocess.py', 'S3DataDistributionType': 'FullyReplicated', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3CompressionType': 'None', 'S3DownloadMode': 'StartOfJob'}, 'DatasetDefinition': None}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'output-1', 'AppManaged': False, 'S3Output': {'LocalPath': '/opt/ml/processing/spark-events/', 'S3Uri': 's3://sagemaker-us-east-1-193890026231/sagemaker/spark-preprocess-demo/2021-12-05-17-53-35/spark_event_logs', 'S3UploadMode': 'Continuous'}, 'FeatureStoreOutput': None}], 'KmsKeyId': None}, 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 2, 'InstanceType': 'ml.m5.xlarge', 'VolumeSizeInGB': 30, 'VolumeKmsKeyId': None}}, 'RoleArn': 'arn:aws:iam::193890026231:role/service-role/AmazonSageMaker-ExecutionRole-20211204T143129', 'StoppingCondition': {'MaxRuntimeInSeconds': 1200}}\u001b[0m\n",
      "\u001b[35m12-05 17:57 smspark.cli  INFO     running spark submit command: spark-submit --master yarn --deploy-mode client /opt/ml/processing/input/code/preprocess.py --s3_input_bucket sagemaker-us-east-1-193890026231 --s3_input_key_prefix sagemaker/spark-preprocess-demo/2021-12-05-17-53-35/input/raw/abalone --s3_output_bucket sagemaker-us-east-1-193890026231 --s3_output_key_prefix sagemaker/spark-preprocess-demo/2021-12-05-17-53-35/input/preprocessed/abalone\u001b[0m\n",
      "\u001b[35m12-05 17:57 smspark-submit INFO     waiting for hosts\u001b[0m\n",
      "\u001b[35m12-05 17:57 smspark-submit INFO     starting status server\u001b[0m\n",
      "\u001b[35m12-05 17:57 smspark-submit INFO     Status server listening on algo-2:5555\u001b[0m\n",
      "\u001b[35m12-05 17:57 smspark-submit INFO     bootstrapping cluster\u001b[0m\n",
      "\u001b[35m12-05 17:57 smspark-submit INFO     transitioning from status INITIALIZING to BOOTSTRAPPING\u001b[0m\n",
      "\u001b[35m12-05 17:57 smspark-submit INFO     copying aws jars\u001b[0m\n",
      "\u001b[35mServing on http://algo-2:5555\u001b[0m\n",
      "\u001b[35m12-05 17:57 smspark-submit INFO     Found hadoop jar hadoop-aws.jar\u001b[0m\n",
      "\u001b[35m12-05 17:57 smspark-submit INFO     Copying optional jar jets3t-0.9.0.jar from /usr/lib/hadoop/lib to /usr/lib/spark/jars\u001b[0m\n",
      "\u001b[35m12-05 17:57 smspark-submit INFO     copying cluster config\u001b[0m\n",
      "\u001b[35m12-05 17:57 smspark-submit INFO     copying /opt/hadoop-config/hdfs-site.xml to /usr/lib/hadoop/etc/hadoop/hdfs-site.xml\u001b[0m\n",
      "\u001b[35m12-05 17:57 smspark-submit INFO     copying /opt/hadoop-config/core-site.xml to /usr/lib/hadoop/etc/hadoop/core-site.xml\u001b[0m\n",
      "\u001b[35m12-05 17:57 smspark-submit INFO     copying /opt/hadoop-config/yarn-site.xml to /usr/lib/hadoop/etc/hadoop/yarn-site.xml\u001b[0m\n",
      "\u001b[35m12-05 17:57 smspark-submit INFO     copying /opt/hadoop-config/spark-defaults.conf to /usr/lib/spark/conf/spark-defaults.conf\u001b[0m\n",
      "\u001b[35m12-05 17:57 smspark-submit INFO     copying /opt/hadoop-config/spark-env.sh to /usr/lib/spark/conf/spark-env.sh\u001b[0m\n",
      "\u001b[35m12-05 17:57 root         INFO     Detected instance type: m5.xlarge with total memory: 16384M and total cores: 4\u001b[0m\n",
      "\u001b[35m12-05 17:57 root         INFO     Writing default config to /usr/lib/hadoop/etc/hadoop/yarn-site.xml\u001b[0m\n",
      "\u001b[35m12-05 17:57 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/yarn-site.xml is: \u001b[0m\n",
      "\u001b[35m<?xml version=\"1.0\"?>\u001b[0m\n",
      "\u001b[35m<!-- Site specific YARN configuration properties -->\n",
      " <configuration>\n",
      "     <property>\n",
      "         <name>yarn.resourcemanager.hostname</name>\n",
      "         <value>10.2.226.32</value>\n",
      "         <description>The hostname of the RM.</description>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.nodemanager.hostname</name>\n",
      "         <value>algo-2</value>\n",
      "         <description>The hostname of the NM.</description>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.nodemanager.webapp.address</name>\n",
      "         <value>algo-2:8042</value>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.nodemanager.vmem-pmem-ratio</name>\n",
      "         <value>5</value>\n",
      "         <description>Ratio between virtual memory to physical memory.</description>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.resourcemanager.am.max-attempts</name>\n",
      "         <value>1</value>\n",
      "         <description>The maximum number of application attempts.</description>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.nodemanager.env-whitelist</name>\n",
      "         <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,YARN_HOME,AWS_CONTAINER_CREDENTIALS_RELATIVE_URI</value>\n",
      "         <description>Environment variable whitelist</description>\n",
      "     </property>\n",
      " \n",
      "  <property>\n",
      "    <name>yarn.scheduler.minimum-allocation-mb</name>\n",
      "    <value>1</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.scheduler.maximum-allocation-mb</name>\n",
      "    <value>15892</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.scheduler.minimum-allocation-vcores</name>\n",
      "    <value>1</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.scheduler.maximum-allocation-vcores</name>\n",
      "    <value>4</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.nodemanager.resource.memory-mb</name>\n",
      "    <value>15892</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.nodemanager.resource.cpu-vcores</name>\n",
      "    <value>4</value>\n",
      "  </property>\u001b[0m\n",
      "\u001b[35m</configuration>\u001b[0m\n",
      "\u001b[35m12-05 17:57 root         INFO     Writing default config to /usr/lib/spark/conf/spark-defaults.conf\u001b[0m\n",
      "\u001b[35m12-05 17:57 root         INFO     Configuration at /usr/lib/spark/conf/spark-defaults.conf is: \u001b[0m\n",
      "\u001b[35mspark.driver.extraClassPath      /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar\u001b[0m\n",
      "\u001b[35mspark.driver.extraLibraryPath    /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native\u001b[0m\n",
      "\u001b[35mspark.executor.extraClassPath    /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar\u001b[0m\n",
      "\u001b[35mspark.executor.extraLibraryPath  /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native\u001b[0m\n",
      "\u001b[35mspark.driver.host=10.2.226.32\u001b[0m\n",
      "\u001b[35mspark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2\u001b[0m\n",
      "\u001b[35mspark.driver.memory 2048m\u001b[0m\n",
      "\u001b[35mspark.driver.memoryOverhead 204m\u001b[0m\n",
      "\u001b[35mspark.driver.defaultJavaOptions -XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled\u001b[0m\n",
      "\u001b[35mspark.executor.memory 12399m\u001b[0m\n",
      "\u001b[35mspark.executor.memoryOverhead 1239m\u001b[0m\n",
      "\u001b[35mspark.executor.cores 4\u001b[0m\n",
      "\u001b[35mspark.executor.defaultJavaOptions -verbose:gc -XX:OnOutOfMemoryError='kill -9 %p' -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:ConcGCThreads=1 -XX:ParallelGCThreads=3 \u001b[0m\n",
      "\u001b[35mspark.executor.instances 2\u001b[0m\n",
      "\u001b[35mspark.default.parallelism 16\u001b[0m\n",
      "\u001b[35m12-05 17:57 root         INFO     Finished Yarn configuration files setup.\u001b[0m\n",
      "\u001b[35m12-05 17:57 root         INFO     No file at /opt/ml/processing/input/conf/configuration.json exists, skipping user configuration\u001b[0m\n",
      "\u001b[35m12-05 17:57 smspark-submit INFO     waiting for cluster to be up\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:33 INFO nodemanager.NodeManager: STARTUP_MSG: \u001b[0m\n",
      "\u001b[35m/************************************************************\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG: Starting NodeManager\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   host = algo-2/10.2.213.203\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   args = []\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   version = 2.10.0-amzn-0\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop/lib/jersey-json-1.9.jar:/usr/lib/hadoop/lib/commons-net-3.1.jar:/usr/lib/hadoop/lib/mockito-all-1.8.5.jar:/usr/lib/hadoop/lib/asm-3.2.jar:/usr/lib/hadoop/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/stax-api-1.0-2.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/xmlenc-0.52.jar:/usr/lib/hadoop/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop/lib/commons-digester-1.8.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/curator-client-2.7.1.jar:/usr/lib/hadoop/lib/json-smart-1.3.1.jar:/usr/lib/hadoop/lib/commons-lang3-3.4.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/jersey-server-1.9.jar:/usr/lib/hadoop/lib/commons-configuration-1.6.jar:/usr/lib/hadoop/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop/lib/commons-compress-1.19.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop/lib/commons-codec-1.4.jar:/usr/lib/hadoop/lib/jets3t-0.9.0.jar:/usr/lib/hadoop/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/jersey-core-1.9.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/servlet-api-2.5.jar:/usr/lib/hadoop/lib/commons-lang-2.6.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/commons-io-2.4.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/activation-1.1.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/junit-4.11.jar:/usr/lib/hadoop/.//hadoop-auth-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-aliyun-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-distcp-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-archive-logs-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop/.//hadoop-datajoin-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-sls-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-nfs-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-api.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-gridmix-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-yarn-registry.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-openstack-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-rumen-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-ant.jar:/usr/lib/hadoop/.//hadoop-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-streaming-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-common-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-yarn-common.jar:/usr/lib/hadoop/.//hadoop-archives-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-azure-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-ant-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-aws-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-extras-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-annotations-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/asm-3.2.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/xercesImpl-2.12.0.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/xmlenc-0.52.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.23.Final.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.9.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/xml-apis-1.4.01.jar:/usr/lib/hadoop-hdfs/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.4.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.9.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/servlet-api-2.5.jar:/usr/lib/hadoop-hdfs/lib/commons-lang-2.6.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.4.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.6.7.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-yarn/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/jersey-json-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-net-3.1.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-yarn/lib/asm-3.2.jar:/usr/lib/hadoop-yarn/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-yarn/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-yarn/lib/jsp-api-2.1.jar:/usr/lib/hadoop-yarn/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-yarn/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop-yarn/lib/jettison-1.1.jar:/usr/lib/hadoop-yarn/lib/paranamer-2.3.jar:/usr/lib/hadoop-yarn/lib/avro-1.7.7.jar:/usr/lib/hadoop-yarn/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-yarn/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-yarn/lib/stax-api-1.0-2.jar:/usr/lib/hadoop-yarn/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-yarn/lib/xmlenc-0.52.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop-yarn/lib/commons-digester-1.8.jar:/usr/lib/hadoop-yarn/lib/commons-cli-1.2.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-yarn/lib/guice-3.0.jar:/usr/lib/hadoop-yarn/lib/curator-client-2.7.1.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/json-smart-1.3.1.jar:/usr/lib/hadoop-yarn/lib/commons-lang3-3.4.jar:/usr/lib/hadoop-yarn/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-yarn/lib/jersey-server-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-configuration-1.6.jar:/usr/lib/hadoop-yarn/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-yarn/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/commons-compress-1.19.jar:/usr/lib/hadoop-yarn/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-yarn/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-yarn/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop-yarn/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-yarn/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.0.1.jar:/usr/lib/hadoop-yarn/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.9.jar:/usr/lib/hadoop-yarn/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/commons-codec-1.4.jar:/usr/lib/hadoop-yarn/lib/jets3t-0.9.0.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop-yarn/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-yarn/lib/jersey-core-1.9.jar:/usr/lib/hadoop-yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-yarn/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-yarn/lib/servlet-api-2.5.jar:/usr/lib/hadoop-yarn/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/lib/guava-11.0.2.jar:/usr/lib/hadoop-yarn/lib/log4j-1.2.17.jar:/usr/lib/hadoop-yarn/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-yarn/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop-yarn/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop-yarn/lib/jsch-0.1.54.jar:/usr/lib/hadoop-yarn/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-yarn/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-yarn/lib/commons-io-2.4.jar:/usr/lib/hadoop-yarn/lib/gson-2.2.4.jar:/usr/lib/hadoop-yarn/lib/activation-1.1.jar:/usr/lib/hadoop-yarn/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-mapreduce/lib/aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/lib/asm-3.2.jar:/usr/lib/hadoop-mapreduce/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-mapreduce/lib/paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/lib/avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop-mapreduce/lib/guice-3.0.jar:/usr/lib/hadoop-mapreduce/lib/jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/lib/commons-compress-1.19.jar:/usr/lib/hadoop-mapreduce/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-mapreduce/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/lib/log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/lib/commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/lib/junit-4.11.jar:/usr/lib/hadoop-mapreduce/lib/javax.inject-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//jersey-json-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-mapreduce/.//commons-net-3.1.jar:/usr/lib/hadoop-mapreduce/.//jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//asm-3.2.jar:/usr/lib/hadoop-mapreduce/.//netty-3.10.6.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jackson-annotations-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//commons-logging-1.1.3.jar:/usr/lib/hadoop-mapreduce/.//jsp-api-2.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//stax2-api-3.1.4.jar:/usr/lib/hadoop-mapreduce/.//spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jettison-1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-2.10.0-amzn-0.jar:/us\u001b[0m\n",
      "\u001b[35mr/lib/hadoop-mapreduce/.//hadoop-yarn-api.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//zookeeper-3.4.14.jar:/usr/lib/hadoop-mapreduce/.//htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//stax-api-1.0-2.jar:/usr/lib/hadoop-mapreduce/.//jcip-annotations-1.0-1.jar:/usr/lib/hadoop-mapreduce/.//xmlenc-0.52.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//java-util-1.9.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-mapreduce/.//audience-annotations-0.5.0.jar:/usr/lib/hadoop-mapreduce/.//commons-digester-1.8.jar:/usr/lib/hadoop-mapreduce/.//commons-cli-1.2.jar:/usr/lib/hadoop-mapreduce/.//json-io-2.5.1.jar:/usr/lib/hadoop-mapreduce/.//jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//guice-3.0.jar:/usr/lib/hadoop-mapreduce/.//curator-client-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-5.4.0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-mapreduce/.//json-smart-1.3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-lang3-3.4.jar:/usr/lib/hadoop-mapreduce/.//nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-mapreduce/.//jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/.//commons-configuration-1.6.jar:/usr/lib/hadoop-mapreduce/.//jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.3.jar:/usr/lib/hadoop-mapreduce/.//ehcache-3.3.1.jar:/usr/lib/hadoop-mapreduce/.//apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//httpclient-4.5.9.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//api-util-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//commons-compress-1.19.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//snappy-java-1.1.7.3.jar:/usr/lib/hadoop-mapreduce/.//commons-math3-3.1.1.jar:/usr/lib/hadoop-mapreduce/.//curator-framework-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//jetty-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//fst-2.50.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/.//metrics-core-3.0.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-common.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//curator-recipes-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jersey-client-1.9.jar:/usr/lib/hadoop-mapreduce/.//api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//commons-codec-1.4.jar:/usr/lib/hadoop-mapreduce/.//jets3t-0.9.0.jar:/usr/lib/hadoop-mapreduce/.//HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-mapreduce/.//java-xmlbuilder-0.4.jar:/usr/lib/hadoop-mapreduce/.//httpcore-4.4.11.jar:/usr/lib/hadoop-mapreduce/.//jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/.//jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/.//commons-httpclient-3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-0.8.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//woodstox-core-5.0.3.jar:/usr/lib/hadoop-mapreduce/.//servlet-api-2.5.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-lang-2.6.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//guava-11.0.2.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.852.jar:/usr/lib/hadoop-mapreduce/.//log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//jsr305-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-beanutils-1.9.4.jar:/usr/lib/hadoop-mapreduce/.//jaxb-api-2.2.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//jsch-0.1.54.jar:/usr/lib/hadoop-mapreduce/.//commons-collections-3.2.2.jar:/usr/lib/hadoop-mapreduce/.//jackson-xc-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//gson-2.2.4.jar:/usr/lib/hadoop-mapreduce/.//activation-1.1.jar:/usr/lib/hadoop-mapreduce/.//protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//jackson-databind-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//javax.inject-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/jersey-json-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-net-3.1.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-yarn/lib/asm-3.2.jar:/usr/lib/hadoop-yarn/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-yarn/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-yarn/lib/jsp-api-2.1.jar:/usr/lib/hadoop-yarn/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-yarn/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop-yarn/lib/jettison-1.1.jar:/usr/lib/hadoop-yarn/lib/paranamer-2.3.jar:/usr/lib/hadoop-yarn/lib/avro-1.7.7.jar:/usr/lib/hadoop-yarn/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-yarn/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-yarn/lib/stax-api-1.0-2.jar:/usr/lib/hadoop-yarn/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-yarn/lib/xmlenc-0.52.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop-yarn/lib/commons-digester-1.8.jar:/usr/lib/hadoop-yarn/lib/commons-cli-1.2.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-yarn/lib/guice-3.0.jar:/usr/lib/hadoop-yarn/lib/curator-client-2.7.1.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/json-smart-1.3.1.jar:/usr/lib/hadoop-yarn/lib/commons-lang3-3.4.jar:/usr/lib/hadoop-yarn/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-yarn/lib/jersey-server-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-configuration-1.6.jar:/usr/lib/hadoop-yarn/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-yarn/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/commons-compress-1.19.jar:/usr/lib/hadoop-yarn/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-yarn/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-yarn/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop-yarn/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-yarn/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.0.1.jar:/usr/lib/hadoop-yarn/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.9.jar:/usr/lib/hadoop-yarn/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/commons-codec-1.4.jar:/usr/lib/hadoop-yarn/lib/jets3t-0.9.0.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop-yarn/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-yarn/lib/jersey-core-1.9.jar:/usr/lib/hadoop-yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-yarn/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-yarn/lib/servlet-api-2.5.jar:/usr/lib/hadoop-yarn/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/lib/guava-11.0.2.jar:/usr/lib/hadoop-yarn/lib/log4j-1.2.17.jar:/usr/lib/hadoop-yarn/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-yarn/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop-yarn/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop-yarn/lib/jsch-0.1.54.jar:/usr/lib/hadoop-yarn/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-yarn/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-yarn/lib/commons-io-2.4.jar:/usr/lib/hadoop-yarn/lib/gson-2.2.4.jar:/usr/lib/hadoop-yarn/lib/activation-1.1.jar:/usr/lib/hadoop-yarn/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop/etc/hadoop/nm-config/log4j.properties:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-coprocessor-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-common-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/htrace-core-3.1.0-incubating.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/netty-all-4.0.23.Final.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-client-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jcodings-1.0.8.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/joni-2.1.2.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/metrics-core-2.2.0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jackson-core-2.6.7.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-csv-1.0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-annotations-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-protocol-1.2.6.jar\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r d1e860a34cc1aea3d600c57c5c0270ea41579e8c; compiled by 'ec2-user' on 2020-09-19T02:05Z\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   java = 1.8.0_272\u001b[0m\n",
      "\u001b[35m************************************************************/\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:33 INFO nodemanager.NodeManager: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:33 INFO datanode.DataNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[35m/************************************************************\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG: Starting DataNode\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   host = algo-2/10.2.213.203\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   args = []\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   version = 2.10.0-amzn-0\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop/lib/jersey-json-1.9.jar:/usr/lib/hadoop/lib/commons-net-3.1.jar:/usr/lib/hadoop/lib/mockito-all-1.8.5.jar:/usr/lib/hadoop/lib/asm-3.2.jar:/usr/lib/hadoop/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/stax-api-1.0-2.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/xmlenc-0.52.jar:/usr/lib/hadoop/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop/lib/commons-digester-1.8.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/curator-client-2.7.1.jar:/usr/lib/hadoop/lib/json-smart-1.3.1.jar:/usr/lib/hadoop/lib/commons-lang3-3.4.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/jersey-server-1.9.jar:/usr/lib/hadoop/lib/commons-configuration-1.6.jar:/usr/lib/hadoop/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop/lib/commons-compress-1.19.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop/lib/commons-codec-1.4.jar:/usr/lib/hadoop/lib/jets3t-0.9.0.jar:/usr/lib/hadoop/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/jersey-core-1.9.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/servlet-api-2.5.jar:/usr/lib/hadoop/lib/commons-lang-2.6.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/commons-io-2.4.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/activation-1.1.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/junit-4.11.jar:/usr/lib/hadoop/.//hadoop-auth-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-aliyun-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-distcp-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-archive-logs-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop/.//hadoop-datajoin-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-sls-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-nfs-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-api.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-gridmix-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-yarn-registry.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-openstack-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-rumen-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-ant.jar:/usr/lib/hadoop/.//hadoop-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-streaming-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-common-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-yarn-common.jar:/usr/lib/hadoop/.//hadoop-archives-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-azure-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-ant-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-aws-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-extras-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-annotations-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/asm-3.2.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/lib/xercesImpl-2.12.0.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/xmlenc-0.52.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.23.Final.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.9.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/xml-apis-1.4.01.jar:/usr/lib/hadoop-hdfs/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.4.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.9.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/servlet-api-2.5.jar:/usr/lib/hadoop-hdfs/lib/commons-lang-2.6.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.4.jar:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.6.7.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-yarn/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/jersey-json-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-net-3.1.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-yarn/lib/asm-3.2.jar:/usr/lib/hadoop-yarn/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-yarn/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-yarn/lib/jsp-api-2.1.jar:/usr/lib/hadoop-yarn/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-yarn/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop-yarn/lib/jettison-1.1.jar:/usr/lib/hadoop-yarn/lib/paranamer-2.3.jar:/usr/lib/hadoop-yarn/lib/avro-1.7.7.jar:/usr/lib/hadoop-yarn/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-yarn/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-yarn/lib/stax-api-1.0-2.jar:/usr/lib/hadoop-yarn/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-yarn/lib/xmlenc-0.52.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop-yarn/lib/commons-digester-1.8.jar:/usr/lib/hadoop-yarn/lib/commons-cli-1.2.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-yarn/lib/guice-3.0.jar:/usr/lib/hadoop-yarn/lib/curator-client-2.7.1.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/json-smart-1.3.1.jar:/usr/lib/hadoop-yarn/lib/commons-lang3-3.4.jar:/usr/lib/hadoop-yarn/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-yarn/lib/jersey-server-1.9.jar:/usr/lib/hadoop-yarn/lib/commons-configuration-1.6.jar:/usr/lib/hadoop-yarn/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-yarn/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/commons-compress-1.19.jar:/usr/lib/hadoop-yarn/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-yarn/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-yarn/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop-yarn/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-yarn/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.0.1.jar:/usr/lib/hadoop-yarn/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.9.jar:/usr/lib/hadoop-yarn/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/commons-codec-1.4.jar:/usr/lib/hadoop-yarn/lib/jets3t-0.9.0.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop-yarn/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-yarn/lib/jersey-core-1.9.jar:/usr/lib/hadoop-yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-yarn/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-yarn/lib/servlet-api-2.5.jar:/usr/lib/hadoop-yarn/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/lib/guava-11.0.2.jar:/usr/lib/hadoop-yarn/lib/log4j-1.2.17.jar:/usr/lib/hadoop-yarn/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-yarn/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop-yarn/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop-yarn/lib/jsch-0.1.54.jar:/usr/lib/hadoop-yarn/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-yarn/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-yarn/lib/commons-io-2.4.jar:/usr/lib/hadoop-yarn/lib/gson-2.2.4.jar:/usr/lib/hadoop-yarn/lib/activation-1.1.jar:/usr/lib/hadoop-yarn/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-mapreduce/lib/aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/lib/asm-3.2.jar:/usr/lib/hadoop-mapreduce/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-mapreduce/lib/paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/lib/avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop-mapreduce/lib/guice-3.0.jar:/usr/lib/hadoop-mapreduce/lib/jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/lib/commons-compress-1.19.jar:/usr/lib/hadoop-mapreduce/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-mapreduce/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/lib/log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/lib/commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/lib/junit-4.11.jar:/usr/lib/hadoop-mapreduce/lib/javax.inject-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//jersey-json-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-mapreduce/.//commons-net-3.1.jar:/usr/lib/hadoop-mapreduce/.//jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//asm-3.2.jar:/usr/lib/hadoop-mapreduce/.//netty-3.10.6.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jackson-annotations-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//commons-logging-1.1.3.jar:/usr/lib/hadoop-mapreduce/.//jsp-api-2.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//stax2-api-3.1.4.jar:/usr/lib/hadoop-mapreduce/.//spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jettison-1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-api.jar:/usr/lib\u001b[0m\n",
      "\u001b[35m/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//zookeeper-3.4.14.jar:/usr/lib/hadoop-mapreduce/.//htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//stax-api-1.0-2.jar:/usr/lib/hadoop-mapreduce/.//jcip-annotations-1.0-1.jar:/usr/lib/hadoop-mapreduce/.//xmlenc-0.52.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//java-util-1.9.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-mapreduce/.//audience-annotations-0.5.0.jar:/usr/lib/hadoop-mapreduce/.//commons-digester-1.8.jar:/usr/lib/hadoop-mapreduce/.//commons-cli-1.2.jar:/usr/lib/hadoop-mapreduce/.//json-io-2.5.1.jar:/usr/lib/hadoop-mapreduce/.//jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//guice-3.0.jar:/usr/lib/hadoop-mapreduce/.//curator-client-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-5.4.0.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-mapreduce/.//json-smart-1.3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-lang3-3.4.jar:/usr/lib/hadoop-mapreduce/.//nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-mapreduce/.//jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/.//commons-configuration-1.6.jar:/usr/lib/hadoop-mapreduce/.//jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.3.jar:/usr/lib/hadoop-mapreduce/.//ehcache-3.3.1.jar:/usr/lib/hadoop-mapreduce/.//apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//httpclient-4.5.9.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//api-util-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//commons-compress-1.19.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//snappy-java-1.1.7.3.jar:/usr/lib/hadoop-mapreduce/.//commons-math3-3.1.1.jar:/usr/lib/hadoop-mapreduce/.//curator-framework-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//jetty-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//fst-2.50.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/.//metrics-core-3.0.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-common.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//curator-recipes-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jersey-client-1.9.jar:/usr/lib/hadoop-mapreduce/.//api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//commons-codec-1.4.jar:/usr/lib/hadoop-mapreduce/.//jets3t-0.9.0.jar:/usr/lib/hadoop-mapreduce/.//HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-mapreduce/.//java-xmlbuilder-0.4.jar:/usr/lib/hadoop-mapreduce/.//httpcore-4.4.11.jar:/usr/lib/hadoop-mapreduce/.//jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/.//jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/.//commons-httpclient-3.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-0.8.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//woodstox-core-5.0.3.jar:/usr/lib/hadoop-mapreduce/.//servlet-api-2.5.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-lang-2.6.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//guava-11.0.2.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.852.jar:/usr/lib/hadoop-mapreduce/.//log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//jsr305-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//commons-beanutils-1.9.4.jar:/usr/lib/hadoop-mapreduce/.//jaxb-api-2.2.2.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//jsch-0.1.54.jar:/usr/lib/hadoop-mapreduce/.//commons-collections-3.2.2.jar:/usr/lib/hadoop-mapreduce/.//jackson-xc-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//gson-2.2.4.jar:/usr/lib/hadoop-mapreduce/.//activation-1.1.jar:/usr/lib/hadoop-mapreduce/.//protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//jackson-databind-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//javax.inject-1.jar\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r d1e860a34cc1aea3d600c57c5c0270ea41579e8c; compiled by 'ec2-user' on 2020-09-19T02:05Z\u001b[0m\n",
      "\u001b[35mSTARTUP_MSG:   java = 1.8.0_272\u001b[0m\n",
      "\u001b[35m************************************************************/\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:33 INFO datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:33 INFO checker.ThrottledAsyncChecker: Scheduling a check for [DISK]file:/opt/amazon/hadoop/hdfs/datanode/\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 INFO nodemanager.NodeManager: Node Manager health check script is not available or doesn't have execute permission, so not starting the node health script runner.\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizationEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$LocalizationEventHandlerWrapper\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServicesEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncherEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerSchedulerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerScheduler\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.ContainerManagerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.NodeManagerEventType for class org.apache.hadoop.yarn.server.nodemanager.NodeManager\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 INFO impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 INFO impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 INFO impl.MetricsSystemImpl: DataNode metrics system started\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 INFO impl.MetricsSystemImpl: NodeManager metrics system started\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 INFO nodemanager.DirectoryCollection: Disk Validator: yarn.nodemanager.disk-validator is loaded.\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 INFO nodemanager.DirectoryCollection: Disk Validator: yarn.nodemanager.disk-validator is loaded.\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 INFO nodemanager.NodeResourceMonitorImpl:  Using ResourceCalculatorPlugin : org.apache.hadoop.yarn.util.ResourceCalculatorPlugin@4a83a74a\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.event.LogHandlerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadService\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 INFO containermanager.ContainerManagerImpl: AMRMProxyService is disabled\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 INFO localizer.ResourceLocalizationService: per directory file limit = 8192\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 INFO localizer.ResourceLocalizationService: Disk Validator: yarn.nodemanager.disk-validator is loaded.\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 INFO datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerTracker\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 INFO datanode.DataNode: Configured hostname is algo-2\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 WARN conf.Configuration: No unit for dfs.datanode.outliers.report.interval(1800000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 INFO monitor.ContainersMonitorImpl:  Using ResourceCalculatorPlugin : org.apache.hadoop.yarn.util.ResourceCalculatorPlugin@710c2b53\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 INFO monitor.ContainersMonitorImpl:  Using ResourceCalculatorProcessTree : null\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 INFO monitor.ContainersMonitorImpl: Physical memory check enabled: true\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 INFO monitor.ContainersMonitorImpl: Virtual memory check enabled: true\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 INFO monitor.ContainersMonitorImpl: ContainersMonitor enabled: true\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 INFO datanode.DataNode: Starting DataNode with maxLockedMemory = 0\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 WARN monitor.ContainersMonitorImpl: NodeManager configured with 15.5 G physical memory allocated to containers, which is more than 80% of the total physical memory available (15.2 G). Thrashing might happen.\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 INFO containermanager.ContainerManagerImpl: Not a recoverable state store. Nothing to recover.\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 INFO datanode.DataNode: Opened streaming server at /0.0.0.0:50010\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 INFO datanode.DataNode: Balancing bandwidth is 10485760 bytes/s\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 INFO datanode.DataNode: Number threads for balancing is 50\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 INFO resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 INFO resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 INFO conf.Configuration: node-resources.xml not found\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 INFO resource.ResourceUtils: Unable to find 'node-resources.xml'.\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 INFO resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 INFO resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 INFO nodemanager.NodeStatusUpdaterImpl: Nodemanager resources is set to: <memory:15892, vCores:4>\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 INFO nodemanager.NodeStatusUpdaterImpl: Initialized nodemanager with : physical-memory=15892 virtual-memory=79460 virtual-cores=4\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 INFO mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 2000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 INFO ipc.Server: Starting Socket Reader #1 for port 0\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 INFO http.HttpRequestLog: Http request log for http.requests.datanode is not defined\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 INFO http.HttpServer2: Jetty bound to port 34305\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 INFO mortbay.log: jetty-6.1.26-emr\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ContainerManagementProtocolPB to the server\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 INFO ipc.Server: IPC Server listener on 0: starting\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 INFO security.NMContainerTokenSecretManager: Updating node address : algo-2:38921\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 500 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 INFO ipc.Server: Starting Socket Reader #1 for port 8040\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.nodemanager.api.LocalizationProtocolPB to the server\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 INFO ipc.Server: IPC Server listener on 8040: starting\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 INFO localizer.ResourceLocalizationService: Localizer started on port 8040\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 INFO containermanager.ContainerManagerImpl: ContainerManager started at /10.2.213.203:38921\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 INFO containermanager.ContainerManagerImpl: ContainerManager bound to algo-2/10.2.213.203:0\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:34 INFO webapp.WebServer: Instantiating NMWebApp at algo-2:8042\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:35 INFO mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:35 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:35 INFO http.HttpRequestLog: Http request log for http.requests.nodemanager is not defined\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:35 INFO mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:34305\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:35 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:35 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context node\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:35 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:35 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:35 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.AuthenticationWithProxyUserFilter) to context node\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:35 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.AuthenticationWithProxyUserFilter) to context logs\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:35 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.AuthenticationWithProxyUserFilter) to context static\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:35 INFO http.HttpServer2: adding path spec: /node/*\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:35 INFO http.HttpServer2: adding path spec: /ws/*\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:35 INFO web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:35 INFO util.JvmPauseMonitor: Starting JVM pause monitor\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:35 INFO datanode.DataNode: dnUserName = root\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:35 INFO datanode.DataNode: supergroup = supergroup\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:35 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:35 INFO ipc.Server: Starting Socket Reader #1 for port 50020\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:35 INFO datanode.DataNode: Opened IPC server at /0.0.0.0:50020\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:35 INFO datanode.DataNode: Refresh request received for nameservices: null\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:35 INFO datanode.DataNode: Starting BPOfferServices for nameservices: <default>\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:35 INFO datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to algo-1/10.2.226.32:8020 starting to offer service\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:35 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:35 INFO ipc.Server: IPC Server listener on 50020: starting\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:35 INFO webapp.WebApps: Registered webapp guice modules\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:35 INFO http.HttpServer2: Jetty bound to port 8042\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:35 INFO mortbay.log: jetty-6.1.26-emr\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:35 INFO mortbay.log: Extract jar:file:/usr/lib/hadoop/hadoop-yarn-common-2.10.0-amzn-0.jar!/webapps/node to work/Jetty_algo.2_8042_node____.8nf84m/webapp\u001b[0m\n",
      "\u001b[35mDec 05, 2021 5:57:35 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[35mINFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices as a root resource class\u001b[0m\n",
      "\u001b[35mDec 05, 2021 5:57:35 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[35mINFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class\u001b[0m\n",
      "\u001b[35mDec 05, 2021 5:57:35 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[35mINFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver as a provider class\u001b[0m\n",
      "\u001b[35mDec 05, 2021 5:57:35 PM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate\u001b[0m\n",
      "\u001b[35mINFO: Initiating Jersey application, version 'Jersey: 1.9 09/02/2011 11:17 AM'\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:35 INFO impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:35 INFO conf.Configuration: found resource core-site.xml at file:/etc/hadoop/conf.empty/core-site.xml\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:35 INFO security.Groups: clearing userToGroupsMap cache\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:35 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:35 INFO impl.MetricsSystemImpl: NameNode metrics system started\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:35 INFO namenode.NameNode: fs.defaultFS is hdfs://10.2.226.32/\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:36 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:36 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:36 INFO resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:36 INFO resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:36 INFO conf.Configuration: found resource yarn-site.xml at file:/etc/hadoop/conf.empty/yarn-site.xml\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:36 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMFatalEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMFatalEventDispatcher\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:36 INFO security.NMTokenSecretManagerInRM: NMTokenKeyRollingInterval: 86400000ms and NMTokenKeyActivationDelay: 900000ms\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:36 INFO security.RMContainerTokenSecretManager: ContainerTokenKeyRollingInterval: 86400000ms and ContainerTokenKeyActivationDelay: 900000ms\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:36 INFO security.AMRMTokenSecretManager: AMRMTokenKeyRollingInterval: 86400000ms and AMRMTokenKeyActivationDelay: 900000 ms\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:36 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEventType for class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$ForwardingEventHandler\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:36 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.NodesListManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.NodesListManager\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:36 INFO resourcemanager.ResourceManager: Using Scheduler: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:36 INFO util.JvmPauseMonitor: Starting JVM pause monitor\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:36 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.SchedulerEventType for class org.apache.hadoop.yarn.event.EventDispatcher\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:36 INFO hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:36 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:36 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:36 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$NodeEventDispatcher\u001b[0m\n",
      "\u001b[35mDec 05, 2021 5:57:35 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[35mINFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[35mDec 05, 2021 5:57:35 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[35mINFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[35mDec 05, 2021 5:57:36 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[35mINFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:36 INFO mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@algo-2:8042\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:36 INFO webapp.WebApps: Web app node started at 8042\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:36 INFO nodemanager.NodeStatusUpdaterImpl: Node ID assigned is : algo-2:38921\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:36 INFO util.JvmPauseMonitor: Starting JVM pause monitor\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:36 INFO client.RMProxy: Connecting to ResourceManager at /10.2.226.32:8031\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:36 INFO nodemanager.NodeStatusUpdaterImpl: Sending out 0 NM container statuses: []\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:36 INFO nodemanager.NodeStatusUpdaterImpl: Registering with RM using containers :[]\u001b[0m\n",
      "\u001b[35m12-05 17:57 smspark-submit INFO     cluster is up\u001b[0m\n",
      "\u001b[35m12-05 17:57 smspark-submit INFO     transitioning from status BOOTSTRAPPING to WAITING\u001b[0m\n",
      "\u001b[35m12-05 17:57 smspark-submit INFO     starting executor logs watcher\u001b[0m\n",
      "\u001b[35mStarting executor logs watcher on log_dir: /var/log/yarn\u001b[0m\n",
      "\u001b[35m12-05 17:57 smspark-submit INFO     waiting for the primary to come up\u001b[0m\n",
      "\u001b[35m12-05 17:57 smspark-submit INFO     waiting for the primary to go down\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:36 INFO ipc.Client: Retrying connect to server: algo-1/10.2.226.32:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:36 INFO checker.ThrottledAsyncChecker: Scheduling a check for [DISK]file:/opt/amazon/hadoop/hdfs/datanode/\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:36 INFO nodemanager.NodeManager: Node Manager health check script is not available or doesn't have execute permission, so not starting the node health script runner.\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:36 INFO mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:36 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:36 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:36 INFO impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:36 INFO impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:36 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:36 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizationEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$LocalizationEventHandlerWrapper\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:36 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServicesEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:36 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:36 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncherEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:36 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerSchedulerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerScheduler\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:36 INFO http.HttpRequestLog: Http request log for http.requests.namenode is not defined\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:36 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:36 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:36 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:36 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:36 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.ContainerManagerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:36 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.NodeManagerEventType for class org.apache.hadoop.yarn.server.nodemanager.NodeManager\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:36 INFO impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:36 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:36 INFO impl.MetricsSystemImpl: ResourceManager metrics system started\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:36 INFO security.YarnAuthorizationProvider: org.apache.hadoop.yarn.security.ConfiguredYarnAuthorizer is instantiated.\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:36 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMAppManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.RMAppManager\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:36 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:36 INFO impl.MetricsSystemImpl: DataNode metrics system started\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:36 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncherEventType for class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.ApplicationMasterLauncher\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:36 INFO resourcemanager.RMNMInfo: Registered RMNMInfo MBean\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:36 INFO monitor.RMAppLifetimeMonitor: Application lifelime monitor interval set to 3000 ms.\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:36 INFO util.HostsFileReader: Refreshing hosts (include/exclude) list\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:36 INFO conf.Configuration: found resource capacity-scheduler.xml at file:/etc/hadoop/conf.empty/capacity-scheduler.xml\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:36 INFO http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:36 INFO http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:36 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:36 INFO impl.MetricsSystemImpl: NodeManager metrics system started\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:36 INFO scheduler.AbstractYarnScheduler: Minimum allocation = <memory:1, vCores:1>\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:36 INFO scheduler.AbstractYarnScheduler: Maximum allocation = <memory:15892, vCores:4>\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:36 INFO http.HttpServer2: Jetty bound to port 50070\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:36 INFO mortbay.log: jetty-6.1.26-emr\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO nodemanager.DirectoryCollection: Disk Validator: yarn.nodemanager.disk-validator is loaded.\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO nodemanager.DirectoryCollection: Disk Validator: yarn.nodemanager.disk-validator is loaded.\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root is undefined\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root is undefined\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO capacity.ParentQueue: root, capacity=1.0, absoluteCapacity=1.0, maxCapacity=1.0, absoluteMaxCapacity=1.0, state=RUNNING, acls=ADMINISTER_QUEUE:*SUBMIT_APP:*, labels=*,\u001b[0m\n",
      "\u001b[34m, reservationsContinueLooking=true, orderingPolicy=utilization, priority=0\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO capacity.ParentQueue: Initialized parent-queue root name=root, fullname=root\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root.default is undefined\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root.default is undefined\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO capacity.LeafQueue: Initializing default\u001b[0m\n",
      "\u001b[34mcapacity = 1.0 [= (float) configuredCapacity / 100 ]\u001b[0m\n",
      "\u001b[34mabsoluteCapacity = 1.0 [= parentAbsoluteCapacity * capacity ]\u001b[0m\n",
      "\u001b[34mmaxCapacity = 1.0 [= configuredMaxCapacity ]\u001b[0m\n",
      "\u001b[34mabsoluteMaxCapacity = 1.0 [= 1.0 maximumCapacity undefined, (parentAbsoluteMaxCapacity * maximumCapacity) / 100 otherwise ]\u001b[0m\n",
      "\u001b[34muserLimit = 100 [= configuredUserLimit ]\u001b[0m\n",
      "\u001b[34muserLimitFactor = 1.0 [= configuredUserLimitFactor ]\u001b[0m\n",
      "\u001b[34mmaxApplications = 10000 [= configuredMaximumSystemApplicationsPerQueue or (int)(configuredMaximumSystemApplications * absoluteCapacity)]\u001b[0m\n",
      "\u001b[34mmaxApplicationsPerUser = 10000 [= (int)(maxApplications * (userLimit / 100.0f) * userLimitFactor) ]\u001b[0m\n",
      "\u001b[34musedCapacity = 0.0 [= usedResourcesMemory / (clusterResourceMemory * absoluteCapacity)]\u001b[0m\n",
      "\u001b[34mabsoluteUsedCapacity = 0.0 [= usedResourcesMemory / clusterResourceMemory]\u001b[0m\n",
      "\u001b[34mmaxAMResourcePerQueuePercent = 0.1 [= configuredMaximumAMResourcePercent ]\u001b[0m\n",
      "\u001b[34mminimumAllocationFactor = 0.99993706 [= (float)(maximumAllocationMemory - minimumAllocationMemory) / maximumAllocationMemory ]\u001b[0m\n",
      "\u001b[34mmaximumAllocation = <memory:15892, vCores:4> [= configuredMaxAllocation ]\u001b[0m\n",
      "\u001b[34mnumContainers = 0 [= currentNumContainers ]\u001b[0m\n",
      "\u001b[34mstate = RUNNING [= configuredState ]\u001b[0m\n",
      "\u001b[34macls = ADMINISTER_QUEUE:*SUBMIT_APP:* [= configuredAcls ]\u001b[0m\n",
      "\u001b[34mnodeLocalityDelay = 40\u001b[0m\n",
      "\u001b[34mrackLocalityAdditionalDelay = -1\u001b[0m\n",
      "\u001b[34mlabels=*,\u001b[0m\n",
      "\u001b[34mreservationsContinueLooking = true\u001b[0m\n",
      "\u001b[34mpreemptionDisabled = true\u001b[0m\n",
      "\u001b[34mdefaultAppPriorityPerQueue = 0\u001b[0m\n",
      "\u001b[34mpriority = 0\u001b[0m\n",
      "\u001b[34mmaxLifetime = -1 seconds\u001b[0m\n",
      "\u001b[34mdefaultLifetime = -1 seconds\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO capacity.CapacitySchedulerQueueManager: Initialized queue: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=0, numContainers=0\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO capacity.CapacitySchedulerQueueManager: Initialized queue: root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO capacity.CapacitySchedulerQueueManager: Initialized root queue root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO placement.UserGroupMappingPlacementRule: Initialized queue mappings, override: false\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO capacity.WorkflowPriorityMappingsManager: Initialized workflow priority mappings, override: false\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO capacity.CapacityScheduler: Initialized CapacityScheduler with calculator=class org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator, minimumAllocation=<<memory:1, vCores:1>>, maximumAllocation=<<memory:15892, vCores:4>>, asynchronousScheduling=false, asyncScheduleInterval=5ms\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO conf.Configuration: dynamic-resources.xml not found\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO resourcemanager.AMSProcessingChain: Initializing AMS Processing chain. Root Processor=[org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor].\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO resourcemanager.ResourceManager: TimelineServicePublisher is not configured\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO nodemanager.NodeResourceMonitorImpl:  Using ResourceCalculatorPlugin : org.apache.hadoop.yarn.util.ResourceCalculatorPlugin@4a83a74a\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.event.LogHandlerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadService\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO containermanager.ContainerManagerImpl: AMRMProxyService is disabled\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO localizer.ResourceLocalizationService: per directory file limit = 8192\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO datanode.DataNode: Configured hostname is algo-1\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 WARN conf.Configuration: No unit for dfs.datanode.outliers.report.interval(1800000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO datanode.DataNode: Starting DataNode with maxLockedMemory = 0\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO datanode.DataNode: Opened streaming server at /0.0.0.0:50010\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO localizer.ResourceLocalizationService: Disk Validator: yarn.nodemanager.disk-validator is loaded.\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerTracker\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO monitor.ContainersMonitorImpl:  Using ResourceCalculatorPlugin : org.apache.hadoop.yarn.util.ResourceCalculatorPlugin@710c2b53\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO monitor.ContainersMonitorImpl:  Using ResourceCalculatorProcessTree : null\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO monitor.ContainersMonitorImpl: Physical memory check enabled: true\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO monitor.ContainersMonitorImpl: Virtual memory check enabled: true\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO monitor.ContainersMonitorImpl: ContainersMonitor enabled: true\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO datanode.DataNode: Balancing bandwidth is 10485760 bytes/s\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO datanode.DataNode: Number threads for balancing is 50\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 WARN monitor.ContainersMonitorImpl: NodeManager configured with 15.5 G physical memory allocated to containers, which is more than 80% of the total physical memory available (15.2 G). Thrashing might happen.\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO containermanager.ContainerManagerImpl: Not a recoverable state store. Nothing to recover.\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO http.HttpRequestLog: Http request log for http.requests.resourcemanager is not defined\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:37 INFO ipc.Client: Retrying connect to server: algo-1/10.2.226.32:8031. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:37 INFO ipc.Client: Retrying connect to server: algo-1/10.2.226.32:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context cluster\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context logs\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context static\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context cluster\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO http.HttpServer2: adding path spec: /cluster/*\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO http.HttpServer2: adding path spec: /ws/*\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO conf.Configuration: node-resources.xml not found\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO resource.ResourceUtils: Unable to find 'node-resources.xml'.\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO nodemanager.NodeStatusUpdaterImpl: Nodemanager resources is set to: <memory:15892, vCores:4>\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO nodemanager.NodeStatusUpdaterImpl: Initialized nodemanager with : physical-memory=15892 virtual-memory=79460 virtual-cores=4\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 2000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO ipc.Server: Starting Socket Reader #1 for port 0\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO http.HttpRequestLog: Http request log for http.requests.datanode is not defined\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO http.HttpServer2: Jetty bound to port 35827\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:37 INFO mortbay.log: jetty-6.1.26-emr\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 WARN namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 WARN namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ContainerManagementProtocolPB to the server\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO ipc.Server: IPC Server listener on 0: starting\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO security.NMContainerTokenSecretManager: Updating node address : algo-1:37461\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO namenode.FSEditLog: Edit logging is async:true\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO namenode.FSNamesystem: KeyProvider: null\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO namenode.FSNamesystem: fsLock is fair: true\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO namenode.FSNamesystem: supergroup          = supergroup\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO namenode.FSNamesystem: isPermissionEnabled = true\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO namenode.FSNamesystem: HA Enabled: false\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 500 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO webapp.WebApps: Registered webapp guice modules\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO ipc.Server: Starting Socket Reader #1 for port 8040\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO http.HttpServer2: Jetty bound to port 8088\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO mortbay.log: jetty-6.1.26-emr\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.nodemanager.api.LocalizationProtocolPB to the server\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO ipc.Server: IPC Server listener on 8040: starting\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO localizer.ResourceLocalizationService: Localizer started on port 8040\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO blockmanagement.BlockManager: The block deletion will start around 2021 Dec 05 17:57:38\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO util.GSet: Computing capacity for map BlocksMap\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO util.GSet: 2.0% max memory 889 MB = 17.8 MB\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO util.GSet: capacity      = 2^21 = 2097152 entries\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO blockmanagement.BlockManager: dfs.block.access.token.enable=false\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 WARN conf.Configuration: No unit for dfs.heartbeat.interval(3) assuming SECONDS\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 WARN conf.Configuration: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO blockmanagement.BlockManager: defaultReplication         = 3\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO blockmanagement.BlockManager: maxReplication             = 512\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO blockmanagement.BlockManager: minReplication             = 1\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO blockmanagement.BlockManager: replicationRecheckInterval = 3000\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO namenode.FSNamesystem: Append Enabled: true\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO namenode.FSDirectory: GLOBAL serial map: bits=24 maxEntries=16777215\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:35827\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO util.GSet: Computing capacity for map INodeMap\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO util.GSet: 1.0% max memory 889 MB = 8.9 MB\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO util.GSet: capacity      = 2^20 = 1048576 entries\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO namenode.FSDirectory: ACLs enabled? false\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO namenode.FSDirectory: XAttrs enabled? true\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO namenode.NameNode: Caching file names occurring more than 10 times\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: falseskipCaptureAccessTimeOnlyChange: false\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:38 INFO ipc.Client: Retrying connect to server: algo-1/10.2.226.32:8031. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:38 INFO ipc.Client: Retrying connect to server: algo-1/10.2.226.32:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO util.GSet: Computing capacity for map cachedBlocks\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO util.GSet: 0.25% max memory 889 MB = 2.2 MB\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO util.GSet: capacity      = 2^18 = 262144 entries\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO util.GSet: Computing capacity for map NameNodeRetryCache\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO util.GSet: capacity      = 2^15 = 32768 entries\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO containermanager.ContainerManagerImpl: ContainerManager started at /10.2.226.32:37461\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO containermanager.ContainerManagerImpl: ContainerManager bound to algo-1/10.2.226.32:0\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO webapp.WebServer: Instantiating NMWebApp at algo-1:8042\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO common.Storage: Lock on /opt/amazon/hadoop/hdfs/namenode/in_use.lock acquired by nodename 98@algo-1\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO mortbay.log: Extract jar:file:/usr/lib/hadoop/hadoop-yarn-common-2.10.0-amzn-0.jar!/webapps/cluster to work/Jetty_10_2_226_32_8088_cluster____.rd6fy7/webapp\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO namenode.FileJournalManager: Recovering unfinalized segments in /opt/amazon/hadoop/hdfs/namenode/current\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO namenode.FSImage: No edit log streams selected.\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO namenode.FSImage: Planning to load image: FSImageFile(file=/opt/amazon/hadoop/hdfs/namenode/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO datanode.DataNode: dnUserName = root\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO datanode.DataNode: supergroup = supergroup\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO util.JvmPauseMonitor: Starting JVM pause monitor\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO http.HttpRequestLog: Http request log for http.requests.nodemanager is not defined\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO namenode.FSImageFormatPBINode: Loading 1 INodes.\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO namenode.FSImageFormatPBINode: Successfully loaded 1 inodes\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO namenode.FSImage: Loaded image for txid 0 from /opt/amazon/hadoop/hdfs/namenode/current/fsimage_0000000000000000000\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO namenode.FSEditLog: Starting log segment at 1\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO ipc.Server: Starting Socket Reader #1 for port 50020\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context node\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.AuthenticationWithProxyUserFilter) to context node\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.AuthenticationWithProxyUserFilter) to context logs\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.AuthenticationWithProxyUserFilter) to context static\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO http.HttpServer2: adding path spec: /node/*\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO http.HttpServer2: adding path spec: /ws/*\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO datanode.DataNode: Opened IPC server at /0.0.0.0:50020\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO datanode.DataNode: Refresh request received for nameservices: null\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO datanode.DataNode: Starting BPOfferServices for nameservices: <default>\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO namenode.NameCache: initialized with 0 entries 0 lookups\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO namenode.FSNamesystem: Finished loading FSImage in 441 msecs\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:38 INFO datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to algo-1/10.2.226.32:8020 starting to offer service\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:39 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:39 INFO ipc.Server: IPC Server listener on 50020: starting\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:39 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:39 INFO delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:39 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:39 INFO namenode.NameNode: RPC server is binding to algo-1:8020\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:39 INFO namenode.NameNode: Enable NameNode state context:false\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:39 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:39 INFO ipc.Server: Starting Socket Reader #1 for port 8020\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:39 INFO namenode.NameNode: Clients are to use algo-1:8020 to access this namenode/service.\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:39 INFO namenode.FSNamesystem: Registered FSNamesystemState MBean\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:39 INFO webapp.WebApps: Registered webapp guice modules\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:39 INFO http.HttpServer2: Jetty bound to port 8042\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:39 INFO mortbay.log: jetty-6.1.26-emr\u001b[0m\n",
      "\u001b[34mDec 05, 2021 5:57:39 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[34mINFO: Registering org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver as a provider class\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:39 INFO namenode.LeaseManager: Number of blocks under construction: 0\u001b[0m\n",
      "\u001b[34mDec 05, 2021 5:57:39 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[34mINFO: Registering org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices as a root resource class\u001b[0m\n",
      "\u001b[34mDec 05, 2021 5:57:39 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[34mINFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class\u001b[0m\n",
      "\u001b[34mDec 05, 2021 5:57:39 PM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate\u001b[0m\n",
      "\u001b[34mINFO: Initiating Jersey application, version 'Jersey: 1.9 09/02/2011 11:17 AM'\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:39 INFO blockmanagement.BlockManager: initializing replication queues\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:39 INFO hdfs.StateChange: STATE* Leaving safe mode after 0 secs\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:39 INFO hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:39 INFO hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:39 INFO blockmanagement.BlockManager: Total number of blocks            = 0\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:39 INFO blockmanagement.BlockManager: Number of invalid blocks          = 0\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:39 INFO blockmanagement.BlockManager: Number of under-replicated blocks = 0\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:39 INFO blockmanagement.BlockManager: Number of  over-replicated blocks = 0\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:39 INFO blockmanagement.BlockManager: Number of blocks being written    = 0\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:39 INFO hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 18 msec\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:39 INFO ipc.Client: Retrying connect to server: algo-1/10.2.226.32:8031. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:39 INFO ipc.Client: Retrying connect to server: algo-1/10.2.226.32:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[34mDec 05, 2021 5:57:39 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[34mINFO: Binding org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:39 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:39 INFO ipc.Server: IPC Server listener on 8020: starting\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:39 INFO namenode.NameNode: NameNode RPC up at: algo-1/10.2.226.32:8020\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:39 INFO mortbay.log: Extract jar:file:/usr/lib/hadoop/hadoop-yarn-common-2.10.0-amzn-0.jar!/webapps/node to work/Jetty_algo.1_8042_node____.afclh/webapp\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:39 INFO namenode.FSNamesystem: Starting services required for active state\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:39 INFO namenode.FSDirectory: Initializing quota with 4 thread(s)\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:39 INFO namenode.FSDirectory: Quota initialization completed in 40 milliseconds\u001b[0m\n",
      "\u001b[34mname space=1\u001b[0m\n",
      "\u001b[34mstorage space=0\u001b[0m\n",
      "\u001b[34mstorage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:39 INFO blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:39 INFO datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to algo-1/10.2.226.32:8020\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:39 INFO common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:39 INFO common.Storage: Lock on /opt/amazon/hadoop/hdfs/datanode/in_use.lock acquired by nodename 99@algo-1\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:39 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/datanode is not formatted for namespace 439745700. Formatting...\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:39 INFO common.Storage: Generated new storageID DS-79ec295d-fe00-4a92-847c-18d2ea9618f1 for directory /opt/amazon/hadoop/hdfs/datanode\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:40 INFO common.Storage: Analyzing storage directories for bpid BP-876741428-10.2.226.32-1638727053679\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:40 INFO common.Storage: Locking is disabled for /opt/amazon/hadoop/hdfs/datanode/current/BP-876741428-10.2.226.32-1638727053679\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:40 INFO common.Storage: Block pool storage directory /opt/amazon/hadoop/hdfs/datanode/current/BP-876741428-10.2.226.32-1638727053679 is not formatted for BP-876741428-10.2.226.32-1638727053679. Formatting ...\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:40 INFO common.Storage: Formatting block pool BP-876741428-10.2.226.32-1638727053679 directory /opt/amazon/hadoop/hdfs/datanode/current/BP-876741428-10.2.226.32-1638727053679/current\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:40 INFO datanode.DataNode: Setting up storage: nsid=439745700;bpid=BP-876741428-10.2.226.32-1638727053679;lv=-57;nsInfo=lv=-63;cid=CID-1ffba91e-a3d7-4022-b7f8-8a4b34be365d;nsid=439745700;c=1638727053679;bpid=BP-876741428-10.2.226.32-1638727053679;dnuuid=null\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:40 INFO datanode.DataNode: Generated and persisted new Datanode UUID 100a6240-fd3c-447a-a080-ed60be2066aa\u001b[0m\n",
      "\u001b[34mDec 05, 2021 5:57:40 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[34mINFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices as a root resource class\u001b[0m\n",
      "\u001b[34mDec 05, 2021 5:57:40 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[34mINFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:40 INFO hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.2.213.203:50010, datanodeUuid=45a92e75-31bf-4fcc-b86f-724a83726562, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-57;cid=CID-1ffba91e-a3d7-4022-b7f8-8a4b34be365d;nsid=439745700;c=1638727053679) storage 45a92e75-31bf-4fcc-b86f-724a83726562\u001b[0m\n",
      "\u001b[34mDec 05, 2021 5:57:40 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[34mINFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver as a provider class\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:40 INFO net.NetworkTopology: Adding a new node: /default-rack/10.2.213.203:50010\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:40 INFO blockmanagement.BlockReportLeaseManager: Registered DN 45a92e75-31bf-4fcc-b86f-724a83726562 (10.2.213.203:50010).\u001b[0m\n",
      "\u001b[34mDec 05, 2021 5:57:40 PM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate\u001b[0m\n",
      "\u001b[34mINFO: Initiating Jersey application, version 'Jersey: 1.9 09/02/2011 11:17 AM'\u001b[0m\n",
      "\u001b[34mDec 05, 2021 5:57:40 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[34mINFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:40 INFO impl.FsDatasetImpl: Added new volume: DS-79ec295d-fe00-4a92-847c-18d2ea9618f1\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:40 INFO impl.FsDatasetImpl: Added volume - /opt/amazon/hadoop/hdfs/datanode/current, StorageType: DISK\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:40 INFO impl.FsDatasetImpl: Registered FSDatasetState MBean\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:40 INFO blockmanagement.DatanodeDescriptor: Adding new storage ID DS-b6c6f6ee-80d2-48df-be22-0f46a7f55a4b for DN 10.2.213.203:50010\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:40 INFO checker.ThrottledAsyncChecker: Scheduling a check for /opt/amazon/hadoop/hdfs/datanode/current\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:40 INFO BlockStateChange: BLOCK* processReport 0x3d30d4ff6521f373: Processing first storage report for DS-b6c6f6ee-80d2-48df-be22-0f46a7f55a4b from datanode 45a92e75-31bf-4fcc-b86f-724a83726562\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:40 INFO BlockStateChange: BLOCK* processReport 0x3d30d4ff6521f373: from storage DS-b6c6f6ee-80d2-48df-be22-0f46a7f55a4b node DatanodeRegistration(10.2.213.203:50010, datanodeUuid=45a92e75-31bf-4fcc-b86f-724a83726562, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-57;cid=CID-1ffba91e-a3d7-4022-b7f8-8a4b34be365d;nsid=439745700;c=1638727053679), blocks: 0, hasStaleStorage: false, processing time: 2 msecs, invalidatedBlocks: 0\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:40 INFO checker.DatasetVolumeChecker: Scheduled health check for volume /opt/amazon/hadoop/hdfs/datanode/current\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:40 INFO impl.FsDatasetImpl: Adding block pool BP-876741428-10.2.226.32-1638727053679\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:40 INFO impl.FsDatasetImpl: Scanning block pool BP-876741428-10.2.226.32-1638727053679 on volume /opt/amazon/hadoop/hdfs/datanode/current...\u001b[0m\n",
      "\u001b[34mDec 05, 2021 5:57:40 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[34mINFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:39 INFO datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to algo-1/10.2.226.32:8020\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:39 INFO common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:39 INFO common.Storage: Lock on /opt/amazon/hadoop/hdfs/datanode/in_use.lock acquired by nodename 18@algo-2\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:39 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/datanode is not formatted for namespace 439745700. Formatting...\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:39 INFO common.Storage: Generated new storageID DS-b6c6f6ee-80d2-48df-be22-0f46a7f55a4b for directory /opt/amazon/hadoop/hdfs/datanode\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:39 INFO common.Storage: Analyzing storage directories for bpid BP-876741428-10.2.226.32-1638727053679\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:39 INFO common.Storage: Locking is disabled for /opt/amazon/hadoop/hdfs/datanode/current/BP-876741428-10.2.226.32-1638727053679\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:39 INFO common.Storage: Block pool storage directory /opt/amazon/hadoop/hdfs/datanode/current/BP-876741428-10.2.226.32-1638727053679 is not formatted for BP-876741428-10.2.226.32-1638727053679. Formatting ...\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:39 INFO common.Storage: Formatting block pool BP-876741428-10.2.226.32-1638727053679 directory /opt/amazon/hadoop/hdfs/datanode/current/BP-876741428-10.2.226.32-1638727053679/current\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:39 INFO datanode.DataNode: Setting up storage: nsid=439745700;bpid=BP-876741428-10.2.226.32-1638727053679;lv=-57;nsInfo=lv=-63;cid=CID-1ffba91e-a3d7-4022-b7f8-8a4b34be365d;nsid=439745700;c=1638727053679;bpid=BP-876741428-10.2.226.32-1638727053679;dnuuid=null\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:39 INFO datanode.DataNode: Generated and persisted new Datanode UUID 45a92e75-31bf-4fcc-b86f-724a83726562\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:39 INFO impl.FsDatasetImpl: Added new volume: DS-b6c6f6ee-80d2-48df-be22-0f46a7f55a4b\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:39 INFO impl.FsDatasetImpl: Added volume - /opt/amazon/hadoop/hdfs/datanode/current, StorageType: DISK\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:39 INFO impl.FsDatasetImpl: Registered FSDatasetState MBean\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:39 INFO checker.ThrottledAsyncChecker: Scheduling a check for /opt/amazon/hadoop/hdfs/datanode/current\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:39 INFO checker.DatasetVolumeChecker: Scheduled health check for volume /opt/amazon/hadoop/hdfs/datanode/current\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:39 INFO impl.FsDatasetImpl: Adding block pool BP-876741428-10.2.226.32-1638727053679\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:39 INFO impl.FsDatasetImpl: Scanning block pool BP-876741428-10.2.226.32-1638727053679 on volume /opt/amazon/hadoop/hdfs/datanode/current...\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:39 INFO impl.FsDatasetImpl: Time taken to scan block pool BP-876741428-10.2.226.32-1638727053679 on /opt/amazon/hadoop/hdfs/datanode/current: 24ms\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:39 INFO impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-876741428-10.2.226.32-1638727053679: 26ms\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:39 INFO impl.FsDatasetImpl: Adding replicas to map for block pool BP-876741428-10.2.226.32-1638727053679 on volume /opt/amazon/hadoop/hdfs/datanode/current...\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:39 INFO impl.BlockPoolSlice: Replica Cache file: /opt/amazon/hadoop/hdfs/datanode/current/BP-876741428-10.2.226.32-1638727053679/current/replicas doesn't exist \u001b[0m\n",
      "\u001b[35m21/12/05 17:57:39 INFO impl.FsDatasetImpl: Time to add replicas to map for block pool BP-876741428-10.2.226.32-1638727053679 on volume /opt/amazon/hadoop/hdfs/datanode/current: 1ms\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:39 INFO impl.FsDatasetImpl: Total time to add all replicas to map for block pool BP-876741428-10.2.226.32-1638727053679: 2ms\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:39 INFO datanode.VolumeScanner: Now scanning bpid BP-876741428-10.2.226.32-1638727053679 on volume /opt/amazon/hadoop/hdfs/datanode\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:39 INFO datanode.VolumeScanner: VolumeScanner(/opt/amazon/hadoop/hdfs/datanode, DS-b6c6f6ee-80d2-48df-be22-0f46a7f55a4b): finished scanning block pool BP-876741428-10.2.226.32-1638727053679\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:40 INFO datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 12/5/21 6:03 PM with interval of 21600000ms\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:40 INFO datanode.DataNode: Block pool BP-876741428-10.2.226.32-1638727053679 (Datanode Uuid 45a92e75-31bf-4fcc-b86f-724a83726562) service to algo-1/10.2.226.32:8020 beginning handshake with NN\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:40 INFO datanode.VolumeScanner: VolumeScanner(/opt/amazon/hadoop/hdfs/datanode, DS-b6c6f6ee-80d2-48df-be22-0f46a7f55a4b): no suitable block pools found to scan.  Waiting 1814399969 ms.\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:40 INFO datanode.DataNode: Block pool Block pool BP-876741428-10.2.226.32-1638727053679 (Datanode Uuid 45a92e75-31bf-4fcc-b86f-724a83726562) service to algo-1/10.2.226.32:8020 successfully registered with NN\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:40 INFO datanode.DataNode: For namenode algo-1/10.2.226.32:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:40 INFO ipc.Client: Retrying connect to server: algo-1/10.2.226.32:8031. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:40 INFO datanode.DataNode: Successfully sent block report 0x3d30d4ff6521f373,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 4 msec to generate and 96 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:40 INFO datanode.DataNode: Got finalize command for block pool BP-876741428-10.2.226.32-1638727053679\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:40 INFO impl.FsDatasetImpl: Time taken to scan block pool BP-876741428-10.2.226.32-1638727053679 on /opt/amazon/hadoop/hdfs/datanode/current: 180ms\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:40 INFO impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-876741428-10.2.226.32-1638727053679: 196ms\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:40 INFO impl.FsDatasetImpl: Adding replicas to map for block pool BP-876741428-10.2.226.32-1638727053679 on volume /opt/amazon/hadoop/hdfs/datanode/current...\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:40 INFO impl.BlockPoolSlice: Replica Cache file: /opt/amazon/hadoop/hdfs/datanode/current/BP-876741428-10.2.226.32-1638727053679/current/replicas doesn't exist \u001b[0m\n",
      "\u001b[34m21/12/05 17:57:40 INFO impl.FsDatasetImpl: Time to add replicas to map for block pool BP-876741428-10.2.226.32-1638727053679 on volume /opt/amazon/hadoop/hdfs/datanode/current: 32ms\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:40 INFO impl.FsDatasetImpl: Total time to add all replicas to map for block pool BP-876741428-10.2.226.32-1638727053679: 56ms\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:40 INFO datanode.VolumeScanner: Now scanning bpid BP-876741428-10.2.226.32-1638727053679 on volume /opt/amazon/hadoop/hdfs/datanode\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:40 INFO datanode.VolumeScanner: VolumeScanner(/opt/amazon/hadoop/hdfs/datanode, DS-79ec295d-fe00-4a92-847c-18d2ea9618f1): finished scanning block pool BP-876741428-10.2.226.32-1638727053679\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:40 INFO datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 12/5/21 7:47 PM with interval of 21600000ms\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:40 INFO datanode.DataNode: Block pool BP-876741428-10.2.226.32-1638727053679 (Datanode Uuid 100a6240-fd3c-447a-a080-ed60be2066aa) service to algo-1/10.2.226.32:8020 beginning handshake with NN\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:40 INFO datanode.VolumeScanner: VolumeScanner(/opt/amazon/hadoop/hdfs/datanode, DS-79ec295d-fe00-4a92-847c-18d2ea9618f1): no suitable block pools found to scan.  Waiting 1814399896 ms.\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:40 INFO hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.2.226.32:50010, datanodeUuid=100a6240-fd3c-447a-a080-ed60be2066aa, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-57;cid=CID-1ffba91e-a3d7-4022-b7f8-8a4b34be365d;nsid=439745700;c=1638727053679) storage 100a6240-fd3c-447a-a080-ed60be2066aa\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:40 INFO net.NetworkTopology: Adding a new node: /default-rack/10.2.226.32:50010\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:40 INFO blockmanagement.BlockReportLeaseManager: Registered DN 100a6240-fd3c-447a-a080-ed60be2066aa (10.2.226.32:50010).\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:40 INFO datanode.DataNode: Block pool Block pool BP-876741428-10.2.226.32-1638727053679 (Datanode Uuid 100a6240-fd3c-447a-a080-ed60be2066aa) service to algo-1/10.2.226.32:8020 successfully registered with NN\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:40 INFO datanode.DataNode: For namenode algo-1/10.2.226.32:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:40 INFO blockmanagement.DatanodeDescriptor: Adding new storage ID DS-79ec295d-fe00-4a92-847c-18d2ea9618f1 for DN 10.2.226.32:50010\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:40 INFO BlockStateChange: BLOCK* processReport 0x4f6e08169cb486b1: Processing first storage report for DS-79ec295d-fe00-4a92-847c-18d2ea9618f1 from datanode 100a6240-fd3c-447a-a080-ed60be2066aa\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:40 INFO BlockStateChange: BLOCK* processReport 0x4f6e08169cb486b1: from storage DS-79ec295d-fe00-4a92-847c-18d2ea9618f1 node DatanodeRegistration(10.2.226.32:50010, datanodeUuid=100a6240-fd3c-447a-a080-ed60be2066aa, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-57;cid=CID-1ffba91e-a3d7-4022-b7f8-8a4b34be365d;nsid=439745700;c=1638727053679), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:40 INFO datanode.DataNode: Successfully sent block report 0x4f6e08169cb486b1,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 4 msec to generate and 37 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:40 INFO datanode.DataNode: Got finalize command for block pool BP-876741428-10.2.226.32-1638727053679\u001b[0m\n",
      "\u001b[34mDec 05, 2021 5:57:40 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[34mINFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[34mDec 05, 2021 5:57:41 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[34mINFO: Binding org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:41 INFO mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@10.2.226.32:8088\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:41 INFO webapp.WebApps: Web app cluster started at 8088\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:41 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 100 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:41 INFO ipc.Server: Starting Socket Reader #1 for port 8033\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:41 INFO ipc.Client: Retrying connect to server: algo-1/10.2.226.32:8031. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:41 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.api.ResourceManagerAdministrationProtocolPB to the server\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:41 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:41 INFO resourcemanager.ResourceManager: Transitioning to active state\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:41 INFO ipc.Server: IPC Server listener on 8033: starting\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:41 INFO recovery.RMStateStore: Updating AMRMToken\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:41 INFO security.RMContainerTokenSecretManager: Rolling master-key for container-tokens\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:41 INFO security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:41 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:41 INFO security.RMDelegationTokenSecretManager: storing master key with keyID 1\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:41 INFO recovery.RMStateStore: Storing RMDTMasterKey.\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:41 INFO delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:41 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:41 INFO security.RMDelegationTokenSecretManager: storing master key with keyID 2\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:41 INFO recovery.RMStateStore: Storing RMDTMasterKey.\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:41 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.nodelabels.event.NodeLabelsStoreEventType for class org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager$ForwardingEventHandler\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:41 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 5000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:41 INFO ipc.Server: Starting Socket Reader #1 for port 8031\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:41 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.api.ResourceTrackerPB to the server\u001b[0m\n",
      "\u001b[34mDec 05, 2021 5:57:41 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[34mINFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:41 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:41 INFO ipc.Server: IPC Server listener on 8031: starting\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:41 INFO mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@algo-1:8042\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:41 INFO webapp.WebApps: Web app node started at 8042\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:41 INFO nodemanager.NodeStatusUpdaterImpl: Node ID assigned is : algo-1:37461\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:41 INFO util.JvmPauseMonitor: Starting JVM pause monitor\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:41 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 5000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:41 INFO client.RMProxy: Connecting to ResourceManager at /10.2.226.32:8031\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:41 INFO util.JvmPauseMonitor: Starting JVM pause monitor\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:41 INFO ipc.Server: Starting Socket Reader #1 for port 8030\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:41 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB to the server\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:41 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:41 INFO ipc.Server: IPC Server listener on 8030: starting\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:41 INFO nodemanager.NodeStatusUpdaterImpl: Sending out 0 NM container statuses: []\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:41 INFO nodemanager.NodeStatusUpdaterImpl: Registering with RM using containers :[]\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:41 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 5000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:41 INFO ipc.Server: Starting Socket Reader #1 for port 8032\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:41 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ApplicationClientProtocolPB to the server\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:41 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:41 INFO ipc.Server: IPC Server listener on 8032: starting\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:41 INFO resourcemanager.ResourceManager: Transitioned to active state\u001b[0m\n",
      "\u001b[34m12-05 17:57 smspark-submit INFO     cluster is up\u001b[0m\n",
      "\u001b[34m12-05 17:57 smspark-submit INFO     transitioning from status BOOTSTRAPPING to WAITING\u001b[0m\n",
      "\u001b[34m12-05 17:57 smspark-submit INFO     starting executor logs watcher\u001b[0m\n",
      "\u001b[34m12-05 17:57 smspark-submit INFO     start log event log publisher\u001b[0m\n",
      "\u001b[34mStarting executor logs watcher on log_dir: /var/log/yarn\u001b[0m\n",
      "\u001b[34m12-05 17:57 sagemaker-spark-event-logs-publisher INFO     Start to copy the spark event logs file.\u001b[0m\n",
      "\u001b[34m12-05 17:57 smspark-submit INFO     Waiting for hosts to bootstrap: ['algo-1', 'algo-2']\u001b[0m\n",
      "\u001b[34m12-05 17:57 sagemaker-spark-event-logs-publisher INFO     Writing event log config to spark-defaults.conf\u001b[0m\n",
      "\u001b[34m12-05 17:57 sagemaker-spark-event-logs-publisher INFO     Event log file does not exist.\u001b[0m\n",
      "\u001b[34m12-05 17:57 smspark-submit INFO     Received host statuses: dict_items([('algo-1', StatusMessage(status='WAITING', timestamp='2021-12-05T17:57:41.949778')), ('algo-2', StatusMessage(status='WAITING', timestamp='2021-12-05T17:57:41.953275'))])\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:42 INFO resourcemanager.ResourceTrackerService: NodeManager from node algo-1(cmPort: 37461 httpPort: 8042) registered with capability: <memory:15892, vCores:4>, assigned nodeId algo-1:37461\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:42 INFO rmnode.RMNodeImpl: algo-1:37461 Node Transitioned from NEW to RUNNING\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:42 INFO capacity.CapacityScheduler: Added node algo-1:37461 clusterResource: <memory:15892, vCores:4>\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:42 INFO security.NMContainerTokenSecretManager: Rolling master-key for container-tokens, got key with id 2085305571\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:42 INFO security.NMTokenSecretManagerInNM: Rolling master-key for container-tokens, got key with id 1404500395\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:42 INFO nodemanager.NodeStatusUpdaterImpl: Registered with ResourceManager as algo-1:37461 with total resource of <memory:15892, vCores:4>\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:42 INFO resourcemanager.ResourceTrackerService: NodeManager from node algo-2(cmPort: 38921 httpPort: 8042) registered with capability: <memory:15892, vCores:4>, assigned nodeId algo-2:38921\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:42 INFO rmnode.RMNodeImpl: algo-2:38921 Node Transitioned from NEW to RUNNING\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:42 INFO capacity.CapacityScheduler: Added node algo-2:38921 clusterResource: <memory:31784, vCores:8>\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:42 INFO ipc.Client: Retrying connect to server: algo-1/10.2.226.32:8031. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:42 INFO security.NMContainerTokenSecretManager: Rolling master-key for container-tokens, got key with id 2085305571\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:42 INFO security.NMTokenSecretManagerInNM: Rolling master-key for container-tokens, got key with id 1404500395\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:42 INFO nodemanager.NodeStatusUpdaterImpl: Registered with ResourceManager as algo-2:38921 with total resource of <memory:15892, vCores:4>\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:44 INFO spark.SparkContext: Running Spark version 2.4.6-amzn-0\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:44 INFO spark.SparkContext: Submitted application: PySparkApp\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:44 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:44 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:44 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m21/12/05 17:57:44 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m21/12/05 17:57:44 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:44 INFO util.Utils: Successfully started service 'sparkDriver' on port 43569.\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:44 INFO spark.SparkEnv: Registering MapOutputTracker\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:44 INFO spark.SparkEnv: Registering BlockManagerMaster\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:44 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:44 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:44 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-9ee22092-1b13-419b-bdce-a9b4e3bb6451\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:44 INFO memory.MemoryStore: MemoryStore started with capacity 1028.8 MB\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:44 INFO spark.SparkEnv: Registering OutputCommitCoordinator\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:44 INFO util.log: Logging initialized @2709ms\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:45 INFO server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:45 INFO server.Server: Started @2795ms\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:45 INFO server.AbstractConnector: Started ServerConnector@f70a399{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:45 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@33ada12d{/jobs,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7482b8b8{/jobs/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2b7a34cc{/jobs/job,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2ce5772d{/jobs/job/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4d435d71{/stages,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3765dc41{/stages/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@38bb69a0{/stages/stage,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7181194b{/stages/stage/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@121498b2{/stages/pool,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2ca61360{/stages/pool/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3e958816{/storage,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@472785ce{/storage/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@71c04490{/storage/rdd,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6d6174db{/storage/rdd/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7f269459{/environment,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4d47786{/environment/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@77c35748{/executors,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@116222c7{/executors/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5a2de8ee{/executors/threadDump,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7de2cc6a{/executors/threadDump/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1076bd89{/static,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@14f70638{/,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@190928df{/api,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4c6b78d3{/jobs/job/kill,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:45 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5cb0b52d{/stages/stage/kill,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:45 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.2.226.32:4040\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:45 INFO client.RMProxy: Connecting to ResourceManager at /10.2.226.32:8032\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:46 INFO yarn.Client: Requesting a new application from cluster with 2 NodeManagers\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:46 INFO resourcemanager.ClientRMService: Allocated new applicationId: 1\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:46 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:46 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:46 INFO resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:46 INFO resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:46 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (15892 MB per container)\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:46 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:46 INFO yarn.Client: Setting up container launch context for our AM\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:46 INFO yarn.Client: Setting up the launch environment for our AM container\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:46 INFO yarn.Client: Preparing resources for our AM container\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:46 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:51 INFO yarn.Client: Uploading resource file:/tmp/spark-3caa1682-3b63-4ce7-8308-01767f4d1ed1/__spark_libs__1439911859139969978.zip -> hdfs://10.2.226.32/user/root/.sparkStaging/application_1638727061555_0001/__spark_libs__1439911859139969978.zip\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:51 INFO hdfs.StateChange: BLOCK* allocate blk_1073741825_1001, replicas=10.2.226.32:50010, 10.2.213.203:50010 for /user/root/.sparkStaging/application_1638727061555_0001/__spark_libs__1439911859139969978.zip\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:51 INFO datanode.DataNode: Receiving BP-876741428-10.2.226.32-1638727053679:blk_1073741825_1001 src: /10.2.226.32:58268 dest: /10.2.226.32:50010\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:52 INFO DataNode.clienttrace: src: /10.2.226.32:58268, dest: /10.2.226.32:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1662121403_18, offset: 0, srvID: 100a6240-fd3c-447a-a080-ed60be2066aa, blockid: BP-876741428-10.2.226.32-1638727053679:blk_1073741825_1001, duration(ns): 636305055\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:52 INFO datanode.DataNode: PacketResponder: BP-876741428-10.2.226.32-1638727053679:blk_1073741825_1001, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.2.213.203:50010] terminating\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:52 INFO hdfs.StateChange: BLOCK* allocate blk_1073741826_1002, replicas=10.2.226.32:50010, 10.2.213.203:50010 for /user/root/.sparkStaging/application_1638727061555_0001/__spark_libs__1439911859139969978.zip\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:52 INFO datanode.DataNode: Receiving BP-876741428-10.2.226.32-1638727053679:blk_1073741826_1002 src: /10.2.226.32:58272 dest: /10.2.226.32:50010\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:53 INFO DataNode.clienttrace: src: /10.2.226.32:58272, dest: /10.2.226.32:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1662121403_18, offset: 0, srvID: 100a6240-fd3c-447a-a080-ed60be2066aa, blockid: BP-876741428-10.2.226.32-1638727053679:blk_1073741826_1002, duration(ns): 458842336\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:53 INFO datanode.DataNode: PacketResponder: BP-876741428-10.2.226.32-1638727053679:blk_1073741826_1002, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.2.213.203:50010] terminating\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:53 INFO hdfs.StateChange: BLOCK* allocate blk_1073741827_1003, replicas=10.2.226.32:50010, 10.2.213.203:50010 for /user/root/.sparkStaging/application_1638727061555_0001/__spark_libs__1439911859139969978.zip\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:53 INFO datanode.DataNode: Receiving BP-876741428-10.2.226.32-1638727053679:blk_1073741827_1003 src: /10.2.226.32:58276 dest: /10.2.226.32:50010\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:53 INFO DataNode.clienttrace: src: /10.2.226.32:58276, dest: /10.2.226.32:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1662121403_18, offset: 0, srvID: 100a6240-fd3c-447a-a080-ed60be2066aa, blockid: BP-876741428-10.2.226.32-1638727053679:blk_1073741827_1003, duration(ns): 355199330\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:53 INFO datanode.DataNode: PacketResponder: BP-876741428-10.2.226.32-1638727053679:blk_1073741827_1003, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.2.213.203:50010] terminating\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:53 INFO hdfs.StateChange: BLOCK* allocate blk_1073741828_1004, replicas=10.2.226.32:50010, 10.2.213.203:50010 for /user/root/.sparkStaging/application_1638727061555_0001/__spark_libs__1439911859139969978.zip\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:53 INFO datanode.DataNode: Receiving BP-876741428-10.2.226.32-1638727053679:blk_1073741828_1004 src: /10.2.226.32:58280 dest: /10.2.226.32:50010\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:53 INFO DataNode.clienttrace: src: /10.2.226.32:58280, dest: /10.2.226.32:50010, bytes: 13532432, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1662121403_18, offset: 0, srvID: 100a6240-fd3c-447a-a080-ed60be2066aa, blockid: BP-876741428-10.2.226.32-1638727053679:blk_1073741828_1004, duration(ns): 28864077\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:53 INFO datanode.DataNode: PacketResponder: BP-876741428-10.2.226.32-1638727053679:blk_1073741828_1004, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.2.213.203:50010] terminating\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:53 INFO hdfs.StateChange: DIR* completeFile: /user/root/.sparkStaging/application_1638727061555_0001/__spark_libs__1439911859139969978.zip is closed by DFSClient_NONMAPREDUCE_-1662121403_18\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:53 INFO yarn.Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -> hdfs://10.2.226.32/user/root/.sparkStaging/application_1638727061555_0001/pyspark.zip\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:53 INFO hdfs.StateChange: BLOCK* allocate blk_1073741829_1005, replicas=10.2.226.32:50010, 10.2.213.203:50010 for /user/root/.sparkStaging/application_1638727061555_0001/pyspark.zip\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:53 INFO datanode.DataNode: Receiving BP-876741428-10.2.226.32-1638727053679:blk_1073741829_1005 src: /10.2.226.32:58284 dest: /10.2.226.32:50010\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:53 INFO DataNode.clienttrace: src: /10.2.226.32:58284, dest: /10.2.226.32:50010, bytes: 596339, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1662121403_18, offset: 0, srvID: 100a6240-fd3c-447a-a080-ed60be2066aa, blockid: BP-876741428-10.2.226.32-1638727053679:blk_1073741829_1005, duration(ns): 3065397\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:53 INFO datanode.DataNode: PacketResponder: BP-876741428-10.2.226.32-1638727053679:blk_1073741829_1005, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.2.213.203:50010] terminating\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:53 INFO hdfs.StateChange: DIR* completeFile: /user/root/.sparkStaging/application_1638727061555_0001/pyspark.zip is closed by DFSClient_NONMAPREDUCE_-1662121403_18\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:53 INFO yarn.Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.10.7-src.zip -> hdfs://10.2.226.32/user/root/.sparkStaging/application_1638727061555_0001/py4j-0.10.7-src.zip\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:53 INFO hdfs.StateChange: BLOCK* allocate blk_1073741830_1006, replicas=10.2.226.32:50010, 10.2.213.203:50010 for /user/root/.sparkStaging/application_1638727061555_0001/py4j-0.10.7-src.zip\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:53 INFO datanode.DataNode: Receiving BP-876741428-10.2.226.32-1638727053679:blk_1073741830_1006 src: /10.2.226.32:58288 dest: /10.2.226.32:50010\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:53 INFO DataNode.clienttrace: src: /10.2.226.32:58288, dest: /10.2.226.32:50010, bytes: 42437, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1662121403_18, offset: 0, srvID: 100a6240-fd3c-447a-a080-ed60be2066aa, blockid: BP-876741428-10.2.226.32-1638727053679:blk_1073741830_1006, duration(ns): 2189557\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:53 INFO datanode.DataNode: PacketResponder: BP-876741428-10.2.226.32-1638727053679:blk_1073741830_1006, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.2.213.203:50010] terminating\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:53 INFO hdfs.StateChange: DIR* completeFile: /user/root/.sparkStaging/application_1638727061555_0001/py4j-0.10.7-src.zip is closed by DFSClient_NONMAPREDUCE_-1662121403_18\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:53 INFO yarn.Client: Uploading resource file:/tmp/spark-3caa1682-3b63-4ce7-8308-01767f4d1ed1/__spark_conf__2859997387146419318.zip -> hdfs://10.2.226.32/user/root/.sparkStaging/application_1638727061555_0001/__spark_conf__.zip\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:53 INFO hdfs.StateChange: BLOCK* allocate blk_1073741831_1007, replicas=10.2.226.32:50010, 10.2.213.203:50010 for /user/root/.sparkStaging/application_1638727061555_0001/__spark_conf__.zip\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:53 INFO datanode.DataNode: Receiving BP-876741428-10.2.226.32-1638727053679:blk_1073741831_1007 src: /10.2.226.32:58292 dest: /10.2.226.32:50010\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:53 INFO DataNode.clienttrace: src: /10.2.226.32:58292, dest: /10.2.226.32:50010, bytes: 245177, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1662121403_18, offset: 0, srvID: 100a6240-fd3c-447a-a080-ed60be2066aa, blockid: BP-876741428-10.2.226.32-1638727053679:blk_1073741831_1007, duration(ns): 3094665\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:53 INFO datanode.DataNode: PacketResponder: BP-876741428-10.2.226.32-1638727053679:blk_1073741831_1007, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[10.2.213.203:50010] terminating\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:53 INFO hdfs.StateChange: DIR* completeFile: /user/root/.sparkStaging/application_1638727061555_0001/__spark_conf__.zip is closed by DFSClient_NONMAPREDUCE_-1662121403_18\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:53 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:53 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:53 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m21/12/05 17:57:53 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m21/12/05 17:57:53 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:55 INFO yarn.Client: Submitting application application_1638727061555_0001 to ResourceManager\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:55 INFO capacity.CapacityScheduler: Application 'application_1638727061555_0001' is submitted without priority hence considering default queue/cluster priority: 0\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:55 INFO capacity.CapacityScheduler: Priority '0' is acceptable in queue : default for application: application_1638727061555_0001\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:55 WARN rmapp.RMAppImpl: The specific max attempts: 0 for application: 1 is invalid, because it is out of the range [1, 1]. Use the global max attempts instead.\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:55 INFO resourcemanager.ClientRMService: Application with id 1 submitted by user root\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:55 INFO rmapp.RMAppImpl: Storing application with id application_1638727061555_0001\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:55 INFO resourcemanager.RMAuditLogger: USER=root#011IP=10.2.226.32#011OPERATION=Submit Application Request#011TARGET=ClientRMService#011RESULT=SUCCESS#011APPID=application_1638727061555_0001#011QUEUENAME=default\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:55 INFO recovery.RMStateStore: Storing info for app: application_1638727061555_0001\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:55 INFO rmapp.RMAppImpl: application_1638727061555_0001 State change from NEW to NEW_SAVING on event = START\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:55 INFO rmapp.RMAppImpl: application_1638727061555_0001 State change from NEW_SAVING to SUBMITTED on event = APP_NEW_SAVED\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:55 INFO capacity.ParentQueue: Application added - appId: application_1638727061555_0001 user: root leaf-queue of parent: root #applications: 1\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:55 INFO capacity.CapacityScheduler: Accepted application application_1638727061555_0001 from user: root, in queue: default\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:55 INFO rmapp.RMAppImpl: application_1638727061555_0001 State change from SUBMITTED to ACCEPTED on event = APP_ACCEPTED\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:55 INFO resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1638727061555_0001_000001\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:55 INFO attempt.RMAppAttemptImpl: appattempt_1638727061555_0001_000001 State change from NEW to SUBMITTED on event = START\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:55 INFO capacity.LeafQueue: Application application_1638727061555_0001 from user: root activated in queue: default\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:55 INFO capacity.LeafQueue: Application added - appId: application_1638727061555_0001 user: root, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:55 INFO capacity.CapacityScheduler: Added Application Attempt appattempt_1638727061555_0001_000001 to scheduler from user root in queue default\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:55 INFO attempt.RMAppAttemptImpl: appattempt_1638727061555_0001_000001 State change from SUBMITTED to SCHEDULED on event = ATTEMPT_ADDED\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:55 INFO impl.YarnClientImpl: Submitted application application_1638727061555_0001\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:55 INFO cluster.SchedulerExtensionServices: Starting Yarn extension services with app application_1638727061555_0001 and attemptId None\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:52 INFO datanode.DataNode: Receiving BP-876741428-10.2.226.32-1638727053679:blk_1073741825_1001 src: /10.2.226.32:48206 dest: /10.2.213.203:50010\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:52 INFO DataNode.clienttrace: src: /10.2.226.32:48206, dest: /10.2.213.203:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1662121403_18, offset: 0, srvID: 45a92e75-31bf-4fcc-b86f-724a83726562, blockid: BP-876741428-10.2.226.32-1638727053679:blk_1073741825_1001, duration(ns): 638712265\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:52 INFO datanode.DataNode: PacketResponder: BP-876741428-10.2.226.32-1638727053679:blk_1073741825_1001, type=LAST_IN_PIPELINE terminating\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:52 INFO datanode.DataNode: Receiving BP-876741428-10.2.226.32-1638727053679:blk_1073741826_1002 src: /10.2.226.32:48210 dest: /10.2.213.203:50010\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:53 INFO DataNode.clienttrace: src: /10.2.226.32:48210, dest: /10.2.213.203:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1662121403_18, offset: 0, srvID: 45a92e75-31bf-4fcc-b86f-724a83726562, blockid: BP-876741428-10.2.226.32-1638727053679:blk_1073741826_1002, duration(ns): 458588310\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:53 INFO datanode.DataNode: PacketResponder: BP-876741428-10.2.226.32-1638727053679:blk_1073741826_1002, type=LAST_IN_PIPELINE terminating\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:53 INFO datanode.DataNode: Receiving BP-876741428-10.2.226.32-1638727053679:blk_1073741827_1003 src: /10.2.226.32:48214 dest: /10.2.213.203:50010\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:53 INFO DataNode.clienttrace: src: /10.2.226.32:48214, dest: /10.2.213.203:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1662121403_18, offset: 0, srvID: 45a92e75-31bf-4fcc-b86f-724a83726562, blockid: BP-876741428-10.2.226.32-1638727053679:blk_1073741827_1003, duration(ns): 354132172\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:53 INFO datanode.DataNode: PacketResponder: BP-876741428-10.2.226.32-1638727053679:blk_1073741827_1003, type=LAST_IN_PIPELINE terminating\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:53 INFO datanode.DataNode: Receiving BP-876741428-10.2.226.32-1638727053679:blk_1073741828_1004 src: /10.2.226.32:48218 dest: /10.2.213.203:50010\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:53 INFO DataNode.clienttrace: src: /10.2.226.32:48218, dest: /10.2.213.203:50010, bytes: 13532432, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1662121403_18, offset: 0, srvID: 45a92e75-31bf-4fcc-b86f-724a83726562, blockid: BP-876741428-10.2.226.32-1638727053679:blk_1073741828_1004, duration(ns): 27906440\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:53 INFO datanode.DataNode: PacketResponder: BP-876741428-10.2.226.32-1638727053679:blk_1073741828_1004, type=LAST_IN_PIPELINE terminating\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:53 INFO datanode.DataNode: Receiving BP-876741428-10.2.226.32-1638727053679:blk_1073741829_1005 src: /10.2.226.32:48222 dest: /10.2.213.203:50010\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:53 INFO DataNode.clienttrace: src: /10.2.226.32:48222, dest: /10.2.213.203:50010, bytes: 596339, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1662121403_18, offset: 0, srvID: 45a92e75-31bf-4fcc-b86f-724a83726562, blockid: BP-876741428-10.2.226.32-1638727053679:blk_1073741829_1005, duration(ns): 2326944\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:53 INFO datanode.DataNode: PacketResponder: BP-876741428-10.2.226.32-1638727053679:blk_1073741829_1005, type=LAST_IN_PIPELINE terminating\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:53 INFO datanode.DataNode: Receiving BP-876741428-10.2.226.32-1638727053679:blk_1073741830_1006 src: /10.2.226.32:48226 dest: /10.2.213.203:50010\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:53 INFO DataNode.clienttrace: src: /10.2.226.32:48226, dest: /10.2.213.203:50010, bytes: 42437, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1662121403_18, offset: 0, srvID: 45a92e75-31bf-4fcc-b86f-724a83726562, blockid: BP-876741428-10.2.226.32-1638727053679:blk_1073741830_1006, duration(ns): 1556697\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:53 INFO datanode.DataNode: PacketResponder: BP-876741428-10.2.226.32-1638727053679:blk_1073741830_1006, type=LAST_IN_PIPELINE terminating\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:53 INFO datanode.DataNode: Receiving BP-876741428-10.2.226.32-1638727053679:blk_1073741831_1007 src: /10.2.226.32:48230 dest: /10.2.213.203:50010\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:53 INFO DataNode.clienttrace: src: /10.2.226.32:48230, dest: /10.2.213.203:50010, bytes: 245177, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-1662121403_18, offset: 0, srvID: 45a92e75-31bf-4fcc-b86f-724a83726562, blockid: BP-876741428-10.2.226.32-1638727053679:blk_1073741831_1007, duration(ns): 1968815\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:53 INFO datanode.DataNode: PacketResponder: BP-876741428-10.2.226.32-1638727053679:blk_1073741831_1007, type=LAST_IN_PIPELINE terminating\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:55 INFO ipc.Server: Auth successful for appattempt_1638727061555_0001_000001 (auth:SIMPLE)\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:55 INFO containermanager.ContainerManagerImpl: Start request for container_1638727061555_0001_01_000001 by user root\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:55 INFO containermanager.ContainerManagerImpl: Creating a new application reference for app application_1638727061555_0001\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:55 INFO application.ApplicationImpl: Application application_1638727061555_0001 transitioned from NEW to INITING\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:55 INFO nodemanager.NMAuditLogger: USER=root#011IP=10.2.226.32#011OPERATION=Start Container Request#011TARGET=ContainerManageImpl#011RESULT=SUCCESS#011APPID=application_1638727061555_0001#011CONTAINERID=container_1638727061555_0001_01_000001\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:55 INFO application.ApplicationImpl: Adding container_1638727061555_0001_01_000001 to application application_1638727061555_0001\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:55 INFO application.ApplicationImpl: Application application_1638727061555_0001 transitioned from INITING to RUNNING\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:55 INFO container.ContainerImpl: Container container_1638727061555_0001_01_000001 transitioned from NEW to LOCALIZING\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:55 INFO containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1638727061555_0001\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:55 INFO localizer.ResourceLocalizationService: Created localizer for container_1638727061555_0001_01_000001\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:55 INFO allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1638727061555_0001_000001 container=null queue=default clusterResource=<memory:31784, vCores:8> type=OFF_SWITCH requestedPartition=\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:55 INFO capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.0 absoluteUsedCapacity=0.0 used=<memory:0, vCores:0> cluster=<memory:31784, vCores:8>\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:55 INFO rmcontainer.RMContainerImpl: container_1638727061555_0001_01_000001 Container Transitioned from NEW to ALLOCATED\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:55 INFO resourcemanager.RMAuditLogger: USER=root#011OPERATION=AM Allocated Container#011TARGET=SchedulerApp#011RESULT=SUCCESS#011APPID=application_1638727061555_0001#011CONTAINERID=container_1638727061555_0001_01_000001#011RESOURCE=<memory:896, vCores:1>#011QUEUENAME=default\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:55 INFO security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : algo-2:38921 for container : container_1638727061555_0001_01_000001\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:55 INFO rmcontainer.RMContainerImpl: container_1638727061555_0001_01_000001 Container Transitioned from ALLOCATED to ACQUIRED\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:55 INFO security.NMTokenSecretManagerInRM: Clear node set for appattempt_1638727061555_0001_000001\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:55 INFO attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1638727061555_0001 AttemptId: appattempt_1638727061555_0001_000001 MasterContainer: Container: [ContainerId: container_1638727061555_0001_01_000001, AllocationRequestId: 0, Version: 0, NodeId: algo-2:38921, NodeHttpAddress: algo-2:8042, Resource: <memory:896, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.2.213.203:38921 }, ExecutionType: GUARANTEED, ]\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:55 INFO capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.028190285 absoluteUsedCapacity=0.028190285 used=<memory:896, vCores:1> cluster=<memory:31784, vCores:8>\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:55 INFO capacity.CapacityScheduler: Allocation proposal accepted\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:55 INFO attempt.RMAppAttemptImpl: appattempt_1638727061555_0001_000001 State change from SCHEDULED to ALLOCATED_SAVING on event = CONTAINER_ALLOCATED\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:55 INFO attempt.RMAppAttemptImpl: appattempt_1638727061555_0001_000001 State change from ALLOCATED_SAVING to ALLOCATED on event = ATTEMPT_NEW_SAVED\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:55 INFO amlauncher.AMLauncher: Launching masterappattempt_1638727061555_0001_000001\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:55 INFO amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_1638727061555_0001_01_000001, AllocationRequestId: 0, Version: 0, NodeId: algo-2:38921, NodeHttpAddress: algo-2:8042, Resource: <memory:896, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.2.213.203:38921 }, ExecutionType: GUARANTEED, ] for AM appattempt_1638727061555_0001_000001\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:55 INFO security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1638727061555_0001_000001\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:55 INFO security.AMRMTokenSecretManager: Creating password for appattempt_1638727061555_0001_000001\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:55 INFO amlauncher.AMLauncher: Done launching container Container: [ContainerId: container_1638727061555_0001_01_000001, AllocationRequestId: 0, Version: 0, NodeId: algo-2:38921, NodeHttpAddress: algo-2:8042, Resource: <memory:896, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.2.213.203:38921 }, ExecutionType: GUARANTEED, ] for AM appattempt_1638727061555_0001_000001\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:55 INFO attempt.RMAppAttemptImpl: appattempt_1638727061555_0001_000001 State change from ALLOCATED to LAUNCHED on event = LAUNCHED\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:55 INFO rmapp.RMAppImpl: update the launch time for applicationId: application_1638727061555_0001, attemptId: appattempt_1638727061555_0001_000001launchTime: 1638727075828\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:55 INFO recovery.RMStateStore: Updating info for app: application_1638727061555_0001\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:56 INFO yarn.Client: Application report for application_1638727061555_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:56 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: AM container is launched, waiting for AM container to Register with RM\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1638727075111\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1638727061555_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:55 INFO localizer.ResourceLocalizationService: Writing credentials to the nmPrivate file /tmp/hadoop-root/nm-local-dir/nmPrivate/container_1638727061555_0001_01_000001.tokens\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:55 INFO nodemanager.DefaultContainerExecutor: Initializing user root\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:55 INFO nodemanager.DefaultContainerExecutor: Copying from /tmp/hadoop-root/nm-local-dir/nmPrivate/container_1638727061555_0001_01_000001.tokens to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1638727061555_0001/container_1638727061555_0001_01_000001.tokens\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:55 INFO nodemanager.DefaultContainerExecutor: Localizer CWD set to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1638727061555_0001 = file:/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1638727061555_0001\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:55 INFO localizer.ContainerLocalizer: Disk Validator: yarn.nodemanager.disk-validator is loaded.\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:56 INFO rmcontainer.RMContainerImpl: container_1638727061555_0001_01_000001 Container Transitioned from ACQUIRED to RUNNING\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:57 INFO yarn.Client: Application report for application_1638727061555_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:58 INFO yarn.Client: Application report for application_1638727061555_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m21/12/05 17:57:59 INFO yarn.Client: Application report for application_1638727061555_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:59 INFO container.ContainerImpl: Container container_1638727061555_0001_01_000001 transitioned from LOCALIZING to SCHEDULED\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:59 INFO scheduler.ContainerScheduler: Starting container [container_1638727061555_0001_01_000001]\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:59 INFO container.ContainerImpl: Container container_1638727061555_0001_01_000001 transitioned from SCHEDULED to RUNNING\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:59 INFO monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1638727061555_0001_01_000001\u001b[0m\n",
      "\u001b[35m21/12/05 17:57:59 INFO nodemanager.DefaultContainerExecutor: launchContainer: [bash, /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1638727061555_0001/container_1638727061555_0001_01_000001/default_container_executor.sh]\u001b[0m\n",
      "\u001b[35mHandling create event for file: /var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000001/prelaunch.out\u001b[0m\n",
      "\u001b[35mHandling create event for file: /var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000001/prelaunch.err\u001b[0m\n",
      "\u001b[35mHandling create event for file: /var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000001/stdout\u001b[0m\n",
      "\u001b[35mHandling create event for file: /var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000001/stderr\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:00 INFO yarn.Client: Application report for application_1638727061555_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:01 INFO yarn.Client: Application report for application_1638727061555_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m12-05 17:58 sagemaker-spark-event-logs-publisher INFO     Event log file does not exist.\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:02 INFO yarn.Client: Application report for application_1638727061555_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[35m21/12/05 17:58:01 INFO monitor.ContainersMonitorImpl: container_1638727061555_0001_01_000001's ip = 10.2.213.203, and hostname = algo-2\u001b[0m\n",
      "\u001b[35m21/12/05 17:58:01 INFO monitor.ContainersMonitorImpl: Skipping monitoring container container_1638727061555_0001_01_000001 since CPU usage is not yet available.\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:03 INFO yarn.Client: Application report for application_1638727061555_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:03 INFO ipc.Server: Auth successful for appattempt_1638727061555_0001_000001 (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:03 INFO resourcemanager.DefaultAMSProcessor: AM registration appattempt_1638727061555_0001_000001\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:03 INFO resourcemanager.RMAuditLogger: USER=root#011IP=10.2.213.203#011OPERATION=Register App Master#011TARGET=ApplicationMasterService#011RESULT=SUCCESS#011APPID=application_1638727061555_0001#011APPATTEMPTID=appattempt_1638727061555_0001_000001\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:03 INFO attempt.RMAppAttemptImpl: appattempt_1638727061555_0001_000001 State change from LAUNCHED to RUNNING on event = REGISTERED\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:03 INFO rmapp.RMAppImpl: application_1638727061555_0001 State change from ACCEPTED to RUNNING on event = ATTEMPT_REGISTERED\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:04 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> algo-1, PROXY_URI_BASES -> http://algo-1:8088/proxy/application_1638727061555_0001), /proxy/application_1638727061555_0001\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:04 INFO yarn.Client: Application report for application_1638727061555_0001 (state: RUNNING)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:04 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: 10.2.213.203\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1638727075111\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1638727061555_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:04 INFO cluster.YarnClientSchedulerBackend: Application application_1638727061555_0001 has started running.\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:04 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34693.\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:04 INFO netty.NettyBlockTransferService: Server created on 10.2.226.32:34693\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:04 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:04 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.2.226.32, 34693, None)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:04 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.2.226.32:34693 with 1028.8 MB RAM, BlockManagerId(driver, 10.2.226.32, 34693, None)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:04 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.2.226.32, 34693, None)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:04 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.2.226.32, 34693, None)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:04 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:04 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /metrics/json.\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:04 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@389c9605{/metrics/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:04 INFO scheduler.EventLoggingListener: Logging events to file:/tmp/spark-events/application_1638727061555_0001\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:04 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:04 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/usr/lib/spark/spark-warehouse').\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:04 INFO internal.SharedState: Warehouse path is 'file:/usr/lib/spark/spark-warehouse'.\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:04 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL.\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:04 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1a00e2fc{/SQL,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:04 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/json.\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:04 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@57b232b5{/SQL/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:04 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/execution.\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:04 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@380c6bc2{/SQL/execution,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:04 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/execution/json.\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:04 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4c37ce0d{/SQL/execution/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:04 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /static/sql.\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:04 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5c83aa87{/static/sql,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:05 INFO allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1638727061555_0001_000001 container=null queue=default clusterResource=<memory:31784, vCores:8> type=OFF_SWITCH requestedPartition=\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:05 INFO capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.028190285 absoluteUsedCapacity=0.028190285 used=<memory:896, vCores:1> cluster=<memory:31784, vCores:8>\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:05 INFO rmcontainer.RMContainerImpl: container_1638727061555_0001_01_000002 Container Transitioned from NEW to ALLOCATED\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:05 INFO resourcemanager.RMAuditLogger: USER=root#011OPERATION=AM Allocated Container#011TARGET=SchedulerApp#011RESULT=SUCCESS#011APPID=application_1638727061555_0001#011CONTAINERID=container_1638727061555_0001_01_000002#011RESOURCE=<memory:13638, vCores:1>#011QUEUENAME=default\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:05 INFO capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.4572741 absoluteUsedCapacity=0.4572741 used=<memory:14534, vCores:2> cluster=<memory:31784, vCores:8>\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:05 INFO capacity.CapacityScheduler: Allocation proposal accepted\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:05 INFO state.StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000001/stderr] 21/12/05 17:58:01 INFO util.SignalUtils: Registered signal handler for TERM\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000001/stderr] 21/12/05 17:58:01 INFO util.SignalUtils: Registered signal handler for HUP\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000001/stderr] 21/12/05 17:58:01 INFO util.SignalUtils: Registered signal handler for INT\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000001/stderr] 21/12/05 17:58:01 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000001/stderr] 21/12/05 17:58:01 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000001/stderr] 21/12/05 17:58:01 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000001/stderr] 21/12/05 17:58:01 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000001/stderr] 21/12/05 17:58:01 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000001/stderr] 21/12/05 17:58:02 INFO yarn.ApplicationMaster: Preparing Local resources\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000001/stderr] 21/12/05 17:58:03 INFO yarn.ApplicationMaster: ApplicationAttemptId: appattempt_1638727061555_0001_000001\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000001/stderr] 21/12/05 17:58:03 INFO client.RMProxy: Connecting to ResourceManager at /10.2.226.32:8030\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000001/stderr] 21/12/05 17:58:03 INFO yarn.YarnRMClient: Registering the ApplicationMaster\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000001/stderr] 21/12/05 17:58:04 INFO client.TransportClientFactory: Successfully created connection to /10.2.226.32:43569 after 112 ms (0 ms spent in bootstraps)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000001/stderr] 21/12/05 17:58:04 INFO yarn.ApplicationMaster: \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000001/stderr] ===============================================================================\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000001/stderr] YARN executor launch context:\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000001/stderr]   env:\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000001/stderr]     CLASSPATH -> /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar<CPS>{{PWD}}<CPS>{{PWD}}/__spark_conf__<CPS>{{PWD}}/__spark_libs__/*<CPS>$HADOOP_CONF_DIR<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/*<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/lib/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/lib/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/lib/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*<CPS>{{PWD}}/__spark_conf__/__hadoop_conf__\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000001/stderr]     SPARK_YARN_STAGING_DIR -> hdfs://10.2.226.32/user/root/.sparkStaging/application_1638727061555_0001\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000001/stderr]     SPARK_NO_DAEMONIZE -> TRUE\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000001/stderr]     SPARK_USER -> root\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000001/stderr]     SPARK_MASTER_HOST -> 10.2.226.32\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000001/stderr]     SPARK_HOME -> /usr/lib/spark\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000001/stderr]     PYTHONPATH -> {{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.7-src.zip\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000001/stderr] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000001/stderr]   command:\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000001/stderr]     LD_LIBRARY_PATH=\\\"/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native:$LD_LIBRARY_PATH\\\" \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000001/stderr]       {{JAVA_HOME}}/bin/java \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000001/stderr]       -server \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000001/stderr]       -Xmx12399m \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000001/stderr]       '-verbose:gc' \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000001/stderr]       '-XX:OnOutOfMemoryError=kill -9 %p' \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000001/stderr]       '-XX:+PrintGCDetails' \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000001/stderr]       '-XX:+PrintGCDateStamps' \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000001/stderr]       '-XX:+UseParallelGC' \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000001/stderr]       '-XX:InitiatingHeapOccupancyPercent=70' \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000001/stderr]       '-XX:ConcGCThreads=1' \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000001/stderr]       '-XX:ParallelGCThreads=3' \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000001/stderr]       -Djava.io.tmpdir={{PWD}}/tmp \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000001/stderr]       '-Dspark.driver.port=43569' \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000001/stderr]       -Dspark.yarn.app.container.log.dir=<LOG_DIR> \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000001/stderr]       org.apache.spark.executor.CoarseGrainedExecutorBackend \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000001/stderr]       --driver-url \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000001/stderr]       spark://CoarseGrainedScheduler@10.2.226.32:43569 \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000001/stderr]       --executor-id \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000001/stderr]       <executorId> \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000001/stderr]       --hostname \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000001/stderr]       <hostname> \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000001/stderr]       --cores \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000001/stderr]       4 \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000001/stderr]       --app-id \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000001/stderr]       application_1638727061555_0001 \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000001/stderr]       --user-class-path \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000001/stderr]       file:$PWD/__app__.jar \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000001/stderr]       1><LOG_DIR>/stdout \\ \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000001/stderr]       2><LOG_DIR>/stderr\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000001/stderr] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000001/stderr]   resources:\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000001/stderr]     pyspark.zip -> resource { scheme: \"hdfs\" host: \"10.2.226.32\" port: -1 file: \"/user/root/.sparkStaging/application_1638727061555_0001/pyspark.zip\" } size: 596339 timestamp: 1638727073721 type: FILE visibility: PRIVATE\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000001/stderr]     py4j-0.10.7-src.zip -> resource { scheme: \"hdfs\" host: \"10.2.226.32\" port: -1 file: \"/user/root/.sparkStaging/application_1638727061555_0001/py4j-0.10.7-src.zip\" } size: 42437 timestamp: 1638727073742 type: FILE visibility: PRIVATE\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000001/stderr]     __spark_libs__ -> resource { scheme: \"hdfs\" host: \"10.2.226.32\" port: -1 file: \"/user/root/.sparkStaging/application_1638727061555_0001/__spark_libs__1439911859139969978.zip\" } size: 416185616 timestamp: 1638727073602 type: ARCHIVE visibility: PRIVATE\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000001/stderr]     __spark_conf__ -> resource { scheme: \"hdfs\" host: \"10.2.226.32\" port: -1 file: \"/user/root/.sparkStaging/application_1638727061555_0001/__spark_conf__.zip\" } size: 245177 timestamp: 1638727073885 type: ARCHIVE visibility: PRIVATE\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000001/stderr] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000001/stderr] ===============================================================================\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000001/stderr] 21/12/05 17:58:04 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000001/stderr] 21/12/05 17:58:04 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000001/stderr] 21/12/05 17:58:04 INFO resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000001/stderr] 21/12/05 17:58:04 INFO resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE\u001b[0m\n",
      "\u001b[35m21/12/05 17:58:05 INFO ipc.Server: Auth successful for appattempt_1638727061555_0001_000001 (auth:SIMPLE)\u001b[0m\n",
      "\u001b[35m21/12/05 17:58:05 INFO containermanager.ContainerManagerImpl: Start request for container_1638727061555_0001_01_000003 by user root\u001b[0m\n",
      "\u001b[35m21/12/05 17:58:05 INFO nodemanager.NMAuditLogger: USER=root#011IP=10.2.213.203#011OPERATION=Start Container Request#011TARGET=ContainerManageImpl#011RESULT=SUCCESS#011APPID=application_1638727061555_0001#011CONTAINERID=container_1638727061555_0001_01_000003\u001b[0m\n",
      "\u001b[35m21/12/05 17:58:05 INFO application.ApplicationImpl: Adding container_1638727061555_0001_01_000003 to application application_1638727061555_0001\u001b[0m\n",
      "\u001b[35m21/12/05 17:58:05 INFO container.ContainerImpl: Container container_1638727061555_0001_01_000003 transitioned from NEW to LOCALIZING\u001b[0m\n",
      "\u001b[35m21/12/05 17:58:05 INFO containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1638727061555_0001\u001b[0m\n",
      "\u001b[35m21/12/05 17:58:05 INFO container.ContainerImpl: Container container_1638727061555_0001_01_000003 transitioned from LOCALIZING to SCHEDULED\u001b[0m\n",
      "\u001b[35m21/12/05 17:58:05 INFO scheduler.ContainerScheduler: Starting container [container_1638727061555_0001_01_000003]\u001b[0m\n",
      "\u001b[35m21/12/05 17:58:05 INFO container.ContainerImpl: Container container_1638727061555_0001_01_000003 transitioned from SCHEDULED to RUNNING\u001b[0m\n",
      "\u001b[35m21/12/05 17:58:05 INFO monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1638727061555_0001_01_000003\u001b[0m\n",
      "\u001b[35m21/12/05 17:58:05 INFO nodemanager.DefaultContainerExecutor: launchContainer: [bash, /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1638727061555_0001/container_1638727061555_0001_01_000003/default_container_executor.sh]\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001Handling create event for file: /var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/prelaunch.out\u001b[0m\n",
      "\u001b[35mHandling create event for file: /var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/prelaunch.err\u001b[0m\n",
      "\u001b[35mHandling create event for file: /var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stdout\u001b[0m\n",
      "\u001b[35mHandling create event for file: /var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:05 INFO allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1638727061555_0001_000001 container=null queue=default clusterResource=<memory:31784, vCores:8> type=OFF_SWITCH requestedPartition=\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:05 INFO capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.4572741 absoluteUsedCapacity=0.4572741 used=<memory:14534, vCores:2> cluster=<memory:31784, vCores:8>\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:05 INFO rmcontainer.RMContainerImpl: container_1638727061555_0001_01_000003 Container Transitioned from NEW to ALLOCATED\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:05 INFO resourcemanager.RMAuditLogger: USER=root#011OPERATION=AM Allocated Container#011TARGET=SchedulerApp#011RESULT=SUCCESS#011APPID=application_1638727061555_0001#011CONTAINERID=container_1638727061555_0001_01_000003#011RESOURCE=<memory:13638, vCores:1>#011QUEUENAME=default\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:05 INFO capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.8863579 absoluteUsedCapacity=0.8863579 used=<memory:28172, vCores:3> cluster=<memory:31784, vCores:8>\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:05 INFO capacity.CapacityScheduler: Allocation proposal accepted\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:05 INFO security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : algo-1:37461 for container : container_1638727061555_0001_01_000002\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:05 INFO rmcontainer.RMContainerImpl: container_1638727061555_0001_01_000002 Container Transitioned from ALLOCATED to ACQUIRED\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:05 INFO security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : algo-2:38921 for container : container_1638727061555_0001_01_000003\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:05 INFO rmcontainer.RMContainerImpl: container_1638727061555_0001_01_000003 Container Transitioned from ALLOCATED to ACQUIRED\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:05 INFO ipc.Server: Auth successful for appattempt_1638727061555_0001_000001 (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:05 INFO containermanager.ContainerManagerImpl: Start request for container_1638727061555_0001_01_000002 by user root\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:05 INFO containermanager.ContainerManagerImpl: Creating a new application reference for app application_1638727061555_0001\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:06 INFO application.ApplicationImpl: Application application_1638727061555_0001 transitioned from NEW to INITING\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:06 INFO nodemanager.NMAuditLogger: USER=root#011IP=10.2.213.203#011OPERATION=Start Container Request#011TARGET=ContainerManageImpl#011RESULT=SUCCESS#011APPID=application_1638727061555_0001#011CONTAINERID=container_1638727061555_0001_01_000002\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:06 INFO application.ApplicationImpl: Adding container_1638727061555_0001_01_000002 to application application_1638727061555_0001\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:06 INFO application.ApplicationImpl: Application application_1638727061555_0001 transitioned from INITING to RUNNING\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:06 INFO container.ContainerImpl: Container container_1638727061555_0001_01_000002 transitioned from NEW to LOCALIZING\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:06 INFO containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1638727061555_0001\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:06 INFO localizer.ResourceLocalizationService: Created localizer for container_1638727061555_0001_01_000002\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:06 INFO localizer.ResourceLocalizationService: Writing credentials to the nmPrivate file /tmp/hadoop-root/nm-local-dir/nmPrivate/container_1638727061555_0001_01_000002.tokens\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:06 INFO nodemanager.DefaultContainerExecutor: Initializing user root\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:06 INFO nodemanager.DefaultContainerExecutor: Copying from /tmp/hadoop-root/nm-local-dir/nmPrivate/container_1638727061555_0001_01_000002.tokens to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1638727061555_0001/container_1638727061555_0001_01_000002.tokens\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:06 INFO nodemanager.DefaultContainerExecutor: Localizer CWD set to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1638727061555_0001 = file:/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1638727061555_0001\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:06 INFO localizer.ContainerLocalizer: Disk Validator: yarn.nodemanager.disk-validator is loaded.\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:06 INFO rmcontainer.RMContainerImpl: container_1638727061555_0001_01_000002 Container Transitioned from ACQUIRED to RUNNING\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:06 INFO rmcontainer.RMContainerImpl: container_1638727061555_0001_01_000003 Container Transitioned from ACQUIRED to RUNNING\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:06 INFO Configuration.deprecation: fs.s3a.server-side-encryption-key is deprecated. Instead, use fs.s3a.server-side-encryption.key\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:07 INFO datasources.InMemoryFileIndex: It took 148 ms to list leaf files for 1 paths.\u001b[0m\n",
      "\u001b[35m21/12/05 17:58:07 INFO monitor.ContainersMonitorImpl: container_1638727061555_0001_01_000003's ip = 10.2.213.203, and hostname = algo-2\u001b[0m\n",
      "\u001b[35m21/12/05 17:58:07 INFO monitor.ContainersMonitorImpl: Skipping monitoring container container_1638727061555_0001_01_000003 since CPU usage is not yet available.\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:06 INFO executor.CoarseGrainedExecutorBackend: Started daemon with process name: 414@algo-2\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:06 INFO util.SignalUtils: Registered signal handler for TERM\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:06 INFO util.SignalUtils: Registered signal handler for HUP\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:06 INFO util.SignalUtils: Registered signal handler for INT\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:07 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:07 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:07 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:07 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:07 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:07 INFO client.TransportClientFactory: Successfully created connection to /10.2.226.32:43569 after 113 ms (0 ms spent in bootstraps)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:08 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:08 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:08 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:08 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:08 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:08 INFO client.TransportClientFactory: Successfully created connection to /10.2.226.32:43569 after 2 ms (0 ms spent in bootstraps)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:08 INFO storage.DiskBlockManager: Created local directory at /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1638727061555_0001/blockmgr-1fa3cf9c-687c-4b0f-89b9-e0b31102ff3e\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:08 INFO memory.MemoryStore: MemoryStore started with capacity 6.3 GB\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:08 INFO scheduler.AppSchedulingInfo: checking for deactivate of application :application_1638727061555_0001\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:08 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.2.213.203:40106) with ID 2\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:08 INFO storage.BlockManagerMasterEndpoint: Registering block manager algo-2:46083 with 6.3 GB RAM, BlockManagerId(2, algo-2, 46083, None)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:09 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m21/12/05 17:58:09 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m21/12/05 17:58:09 INFO datasources.FileSourceStrategy: Output Data Schema: struct<sex: string, length: double, diameter: double, height: double, whole_weight: double ... 7 more fields>\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:09 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m21/12/05 17:58:09 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m21/12/05 17:58:09 INFO container.ContainerImpl: Container container_1638727061555_0001_01_000002 transitioned from LOCALIZING to SCHEDULED\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:09 INFO scheduler.ContainerScheduler: Starting container [container_1638727061555_0001_01_000002]\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:09 INFO container.ContainerImpl: Container container_1638727061555_0001_01_000002 transitioned from SCHEDULED to RUNNING\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:09 INFO monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1638727061555_0001_01_000002\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:09 INFO nodemanager.DefaultContainerExecutor: launchContainer: [bash, /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1638727061555_0001/container_1638727061555_0001_01_000002/default_container_executor.sh]\u001b[0m\n",
      "\u001b[34mHandling create event for file: /var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/prelaunch.out\u001b[0m\n",
      "\u001b[34mHandling create event for file: /var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/prelaunch.err\u001b[0m\n",
      "\u001b[34mHandling create event for file: /var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stdout\u001b[0m\n",
      "\u001b[34mHandling create event for file: /var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:09 INFO codegen.CodeGenerator: Code generated in 397.65764 ms\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:10 INFO codegen.CodeGenerator: Code generated in 58.176749 ms\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:10 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.5 KB, free 1028.6 MB)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:10 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.4 KB, free 1028.5 MB)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:10 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.2.226.32:34693 (size: 27.4 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:10 INFO spark.SparkContext: Created broadcast 0 from showString at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:10 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:10 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: ArrayBuffer((1 fileSplits,1))\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:10 INFO scheduler.DAGScheduler: Registering RDD 2 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 0\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:10 INFO scheduler.DAGScheduler: Got map stage job 0 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:10 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 0 (showString at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:10 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:10 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:10 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[2] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:10 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 23.9 KB, free 1028.5 MB)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:10 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 9.9 KB, free 1028.5 MB)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:10 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.2.226.32:34693 (size: 9.9 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:10 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:10 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[2] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:10 INFO cluster.YarnScheduler: Adding task set 0.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:10 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, algo-2, executor 2, partition 0, PROCESS_LOCAL, 8335 bytes)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:11 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on algo-2:46083 (size: 9.9 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:11 INFO monitor.ContainersMonitorImpl: container_1638727061555_0001_01_000002's ip = 10.2.226.32, and hostname = algo-1\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:11 INFO monitor.ContainersMonitorImpl: Skipping monitoring container container_1638727061555_0001_01_000002 since CPU usage is not yet available.\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:11 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on algo-2:46083 (size: 27.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:12 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.2.226.32:37984) with ID 1\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:08 INFO executor.CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@10.2.226.32:43569\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:08 INFO executor.CoarseGrainedExecutorBackend: Successfully registered with driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:08 INFO executor.Executor: Starting executor ID 2 on host algo-2\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:08 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46083.\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:08 INFO netty.NettyBlockTransferService: Server created on algo-2:46083\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:08 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:08 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(2, algo-2, 46083, None)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:08 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(2, algo-2, 46083, None)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:08 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(2, algo-2, 46083, None)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:10 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 0\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:10 INFO executor.Executor: Running task 0.0 in stage 0.0 (TID 0)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:10 INFO Configuration.deprecation: fs.s3a.server-side-encryption-key is deprecated. Instead, use fs.s3a.server-side-encryption.key\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:10 INFO executor.CoarseGrainedExecutorBackend: eagerFSInit: Eagerly initialized FileSystem at s3://does/not/exist in 2238 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:10 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 1\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:11 INFO client.TransportClientFactory: Successfully created connection to /10.2.226.32:34693 after 6 ms (0 ms spent in bootstraps)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:11 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 9.9 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:11 INFO broadcast.TorrentBroadcast: Reading broadcast variable 1 took 189 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:11 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 23.9 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:11 INFO codegen.CodeGenerator: Code generated in 299.052084 ms\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:12 INFO storage.BlockManagerMasterEndpoint: Registering block manager algo-1:40047 with 6.3 GB RAM, BlockManagerId(1, algo-1, 40047, None)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:12 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1794 ms on algo-2 (executor 2) (1/1)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:12 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m21/12/05 17:58:12 INFO scheduler.DAGScheduler: ShuffleMapStage 0 (showString at NativeMethodAccessorImpl.java:0) finished in 1.927 s\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:12 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:12 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:12 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:12 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:12 INFO adaptive.CoalesceShufflePartitions: advisoryTargetPostShuffleInputSize: 67108864, targetPostShuffleInputSize 16.\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:12 INFO codegen.CodeGenerator: Code generated in 37.31035 ms\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:12 INFO spark.SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:12 INFO scheduler.DAGScheduler: Got job 1 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:12 INFO scheduler.DAGScheduler: Final stage: ResultStage 2 (showString at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:12 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:12 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:12 INFO scheduler.DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:12 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 12.2 KB, free 1028.5 MB)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:12 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.3 KB, free 1028.5 MB)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:12 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.2.226.32:34693 (size: 5.3 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:12 INFO spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:12 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:12 INFO cluster.YarnScheduler: Adding task set 2.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:12 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 1, algo-2, executor 2, partition 0, NODE_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:12 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on algo-2:46083 (size: 5.3 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:12 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.2.213.203:40106\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:12 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 1) in 124 ms on algo-2 (executor 2) (1/1)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:13 INFO cluster.YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m21/12/05 17:58:13 INFO scheduler.DAGScheduler: ResultStage 2 (showString at NativeMethodAccessorImpl.java:0) finished in 0.161 s\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:13 INFO scheduler.DAGScheduler: Job 1 finished: showString at NativeMethodAccessorImpl.java:0, took 0.194498 s\u001b[0m\n",
      "\u001b[34m+---+------+--------+------+------------+--------------+--------------+------------+-----+\u001b[0m\n",
      "\u001b[34m|sex|length|diameter|height|whole_weight|shucked_weight|viscera_weight|shell_weight|rings|\u001b[0m\n",
      "\u001b[34m+---+------+--------+------+------------+--------------+--------------+------------+-----+\u001b[0m\n",
      "\u001b[34m|  1|     1|       1|     1|           1|             1|             1|           1|    1|\u001b[0m\n",
      "\u001b[34m+---+------+--------+------+------------+--------------+--------------+------------+-----+\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:13 INFO spark.ContextCleaner: Cleaned accumulator 43\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:13 INFO spark.ContextCleaner: Cleaned accumulator 28\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:13 INFO spark.ContextCleaner: Cleaned accumulator 25\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:13 INFO spark.ContextCleaner: Cleaned accumulator 79\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:13 INFO spark.ContextCleaner: Cleaned accumulator 26\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:13 INFO spark.ContextCleaner: Cleaned accumulator 40\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:13 INFO spark.ContextCleaner: Cleaned accumulator 63\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:13 INFO spark.ContextCleaner: Cleaned accumulator 56\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:13 INFO spark.ContextCleaner: Cleaned accumulator 48\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:13 INFO spark.ContextCleaner: Cleaned accumulator 38\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:13 INFO spark.ContextCleaner: Cleaned accumulator 74\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:13 INFO spark.ContextCleaner: Cleaned accumulator 58\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:13 INFO spark.ContextCleaner: Cleaned accumulator 37\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:13 INFO spark.ContextCleaner: Cleaned accumulator 30\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:13 INFO spark.ContextCleaner: Cleaned accumulator 68\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:13 INFO spark.ContextCleaner: Cleaned accumulator 55\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:13 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on algo-2:46083 in memory (size: 5.3 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:13 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on 10.2.226.32:34693 in memory (size: 5.3 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:10 INFO executor.CoarseGrainedExecutorBackend: Started daemon with process name: 964@algo-1\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:10 INFO util.SignalUtils: Registered signal handler for TERM\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:10 INFO util.SignalUtils: Registered signal handler for HUP\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:10 INFO util.SignalUtils: Registered signal handler for INT\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:11 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:11 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:11 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:11 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:11 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:11 INFO client.TransportClientFactory: Successfully created connection to /10.2.226.32:43569 after 93 ms (0 ms spent in bootstraps)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:11 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:11 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:11 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:11 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:11 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:11 INFO client.TransportClientFactory: Successfully created connection to /10.2.226.32:43569 after 1 ms (0 ms spent in bootstraps)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:11 INFO storage.DiskBlockManager: Created local directory at /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1638727061555_0001/blockmgr-04dc0827-dd22-4001-a5f0-3c7f7badc65c\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:12 INFO memory.MemoryStore: MemoryStore started with capacity 6.3 GB\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:13 INFO spark.ContextCleaner: Cleaned accumulator 44\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:13 INFO spark.ContextCleaner: Cleaned accumulator 65\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:13 INFO spark.ContextCleaner: Cleaned accumulator 60\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:13 INFO spark.ContextCleaner: Cleaned accumulator 57\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:13 INFO spark.ContextCleaner: Cleaned accumulator 35\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:13 INFO spark.ContextCleaner: Cleaned accumulator 69\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:13 INFO spark.ContextCleaner: Cleaned accumulator 61\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:13 INFO spark.ContextCleaner: Cleaned accumulator 41\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:13 INFO spark.ContextCleaner: Cleaned accumulator 47\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:13 INFO spark.ContextCleaner: Cleaned accumulator 70\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:13 INFO spark.ContextCleaner: Cleaned accumulator 59\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:13 INFO spark.ContextCleaner: Cleaned accumulator 78\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:13 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 10.2.226.32:34693 in memory (size: 9.9 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:13 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on algo-2:46083 in memory (size: 9.9 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:11 INFO datasources.FileScanRDD: TID: 0 - Reading current file: path: s3://sagemaker-us-east-1-193890026231/sagemaker/spark-preprocess-demo/2021-12-05-17-53-35/input/raw/abalone/abalone.csv, range: 0-196156, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:11 INFO codegen.CodeGenerator: Code generated in 22.027852 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:11 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 0\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:11 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.4 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:11 INFO broadcast.TorrentBroadcast: Reading broadcast variable 0 took 15 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:12 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 401.8 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:12 INFO executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 1783 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:12 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:12 INFO executor.Executor: Running task 0.0 in stage 2.0 (TID 1)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:12 INFO spark.MapOutputTrackerWorker: Updating epoch to 1 and clearing cache\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:12 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 2\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:12 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.3 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:12 INFO broadcast.TorrentBroadcast: Reading broadcast variable 2 took 10 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:12 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 12.2 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:12 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:12 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.2.226.32:43569)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:12 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:12 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:12 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:12 INFO codegen.CodeGenerator: Code generated in 22.803714 ms\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:13 INFO spark.ContextCleaner: Cleaned accumulator 66\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:13 INFO spark.ContextCleaner: Cleaned accumulator 42\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:13 INFO spark.ContextCleaner: Cleaned accumulator 39\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:13 INFO spark.ContextCleaner: Cleaned accumulator 34\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:13 INFO spark.ContextCleaner: Cleaned accumulator 76\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:13 INFO spark.ContextCleaner: Cleaned accumulator 75\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:13 INFO spark.ContextCleaner: Cleaned accumulator 71\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:13 INFO spark.ContextCleaner: Cleaned accumulator 64\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:13 INFO spark.ContextCleaner: Cleaned accumulator 77\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:13 INFO spark.ContextCleaner: Cleaned accumulator 24\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:13 INFO spark.ContextCleaner: Cleaned accumulator 45\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:13 INFO spark.ContextCleaner: Cleaned accumulator 62\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:13 INFO spark.ContextCleaner: Cleaned accumulator 67\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:13 INFO spark.ContextCleaner: Cleaned accumulator 72\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:13 INFO spark.ContextCleaner: Cleaned accumulator 46\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:13 INFO spark.ContextCleaner: Cleaned accumulator 36\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:13 INFO spark.ContextCleaner: Cleaned accumulator 73\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:13 INFO spark.ContextCleaner: Cleaned accumulator 33\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:13 INFO spark.ContextCleaner: Cleaned accumulator 32\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:13 INFO spark.ContextCleaner: Cleaned accumulator 31\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:13 INFO spark.ContextCleaner: Cleaned accumulator 27\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:13 INFO spark.ContextCleaner: Cleaned accumulator 29\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:13 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m21/12/05 17:58:13 INFO datasources.FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, sex#0)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:13 INFO datasources.FileSourceStrategy: Output Data Schema: struct<sex: string>\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:13 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m21/12/05 17:58:13 INFO codegen.CodeGenerator: Code generated in 21.807055 ms\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:13 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 303.5 KB, free 1028.2 MB)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:13 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 27.4 KB, free 1028.2 MB)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:13 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.2.226.32:34693 (size: 27.4 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:13 INFO spark.SparkContext: Created broadcast 3 from rdd at StringIndexer.scala:138\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:13 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:13 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: ArrayBuffer((1 fileSplits,1))\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:13 INFO spark.SparkContext: Starting job: countByValue at StringIndexer.scala:140\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:14 INFO scheduler.DAGScheduler: Registering RDD 14 (countByValue at StringIndexer.scala:140) as input to shuffle 1\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:14 INFO scheduler.DAGScheduler: Got job 2 (countByValue at StringIndexer.scala:140) with 16 output partitions\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:14 INFO scheduler.DAGScheduler: Final stage: ResultStage 4 (countByValue at StringIndexer.scala:140)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:14 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:14 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 3)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:14 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[14] at countByValue at StringIndexer.scala:140), which has no missing parents\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:14 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 20.1 KB, free 1028.2 MB)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:14 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 10.3 KB, free 1028.2 MB)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:14 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.2.226.32:34693 (size: 10.3 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:14 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:14 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[14] at countByValue at StringIndexer.scala:140) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:14 INFO cluster.YarnScheduler: Adding task set 3.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:14 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 2, algo-1, executor 1, partition 0, PROCESS_LOCAL, 8335 bytes)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:14 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on algo-1:40047 (size: 10.3 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:17 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on algo-1:40047 (size: 27.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:12 INFO executor.CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@10.2.226.32:43569\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:12 INFO executor.CoarseGrainedExecutorBackend: Successfully registered with driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:12 INFO executor.Executor: Starting executor ID 1 on host algo-1\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:12 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40047.\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:12 INFO netty.NettyBlockTransferService: Server created on algo-1:40047\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:12 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:12 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(1, algo-1, 40047, None)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:12 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(1, algo-1, 40047, None)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:12 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(1, algo-1, 40047, None)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:14 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 2\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:14 INFO executor.Executor: Running task 0.0 in stage 3.0 (TID 2)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:14 INFO spark.MapOutputTrackerWorker: Updating epoch to 1 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:14 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 4\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:14 INFO client.TransportClientFactory: Successfully created connection to /10.2.226.32:34693 after 27 ms (0 ms spent in bootstraps)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:14 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 10.3 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:14 INFO broadcast.TorrentBroadcast: Reading broadcast variable 4 took 195 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:14 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 20.1 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:15 INFO Configuration.deprecation: fs.s3a.server-side-encryption-key is deprecated. Instead, use fs.s3a.server-side-encryption.key\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:15 INFO executor.CoarseGrainedExecutorBackend: eagerFSInit: Eagerly initialized FileSystem at s3://does/not/exist in 2706 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:15 INFO codegen.CodeGenerator: Code generated in 336.151859 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:12 INFO executor.Executor: Finished task 0.0 in stage 2.0 (TID 1). 1796 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:17 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 6\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:17 INFO executor.Executor: Running task 0.0 in stage 4.0 (TID 6)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:17 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 8\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:17 INFO executor.Executor: Running task 2.0 in stage 4.0 (TID 8)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:17 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 9\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:17 INFO executor.Executor: Running task 3.0 in stage 4.0 (TID 9)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:17 INFO spark.MapOutputTrackerWorker: Updating epoch to 2 and clearing cache\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:17 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 10\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:17 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 5\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:17 INFO executor.Executor: Running task 4.0 in stage 4.0 (TID 10)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:17 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.2 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:17 INFO broadcast.TorrentBroadcast: Reading broadcast variable 5 took 13 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:17 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 3.6 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:17 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 1, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:17 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 1, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:17 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.2.226.32:43569)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:17 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 1, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:17 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 1, fetching them\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:17 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:17 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 2) in 3542 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:17 INFO cluster.YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m21/12/05 17:58:17 INFO scheduler.DAGScheduler: ShuffleMapStage 3 (countByValue at StringIndexer.scala:140) finished in 3.568 s\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:17 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:17 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:17 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 4)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:17 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:17 INFO scheduler.DAGScheduler: Submitting ResultStage 4 (ShuffledRDD[15] at countByValue at StringIndexer.scala:140), which has no missing parents\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:17 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 3.6 KB, free 1028.2 MB)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:17 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.2 KB, free 1028.2 MB)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:17 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.2.226.32:34693 (size: 2.2 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:17 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:17 INFO scheduler.DAGScheduler: Submitting 16 missing tasks from ResultStage 4 (ShuffledRDD[15] at countByValue at StringIndexer.scala:140) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:17 INFO cluster.YarnScheduler: Adding task set 4.0 with 16 tasks\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:17 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 4.0 (TID 3, algo-1, executor 1, partition 6, NODE_LOCAL, 7673 bytes)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:17 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 4.0 (TID 4, algo-1, executor 1, partition 9, NODE_LOCAL, 7673 bytes)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:17 INFO scheduler.TaskSetManager: Starting task 13.0 in stage 4.0 (TID 5, algo-1, executor 1, partition 13, NODE_LOCAL, 7673 bytes)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:17 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 6, algo-2, executor 2, partition 0, PROCESS_LOCAL, 7673 bytes)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:17 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 4.0 (TID 7, algo-1, executor 1, partition 1, PROCESS_LOCAL, 7673 bytes)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:17 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 4.0 (TID 8, algo-2, executor 2, partition 2, PROCESS_LOCAL, 7673 bytes)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:17 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 4.0 (TID 9, algo-2, executor 2, partition 3, PROCESS_LOCAL, 7673 bytes)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:17 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 4.0 (TID 10, algo-2, executor 2, partition 4, PROCESS_LOCAL, 7673 bytes)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:17 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on algo-2:46083 (size: 2.2 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:17 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on algo-1:40047 (size: 2.2 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:17 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.2.213.203:40106\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:17 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.2.226.32:37984\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:17 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 4.0 (TID 11, algo-1, executor 1, partition 5, PROCESS_LOCAL, 7673 bytes)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:17 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 4.0 (TID 12, algo-1, executor 1, partition 7, PROCESS_LOCAL, 7673 bytes)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:17 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 4.0 (TID 7) in 119 ms on algo-1 (executor 1) (1/16)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:17 INFO scheduler.TaskSetManager: Finished task 9.0 in stage 4.0 (TID 4) in 122 ms on algo-1 (executor 1) (2/16)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:17 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 4.0 (TID 13, algo-1, executor 1, partition 8, PROCESS_LOCAL, 7673 bytes)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:17 INFO scheduler.TaskSetManager: Starting task 10.0 in stage 4.0 (TID 14, algo-1, executor 1, partition 10, PROCESS_LOCAL, 7673 bytes)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:17 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 4.0 (TID 3) in 134 ms on algo-1 (executor 1) (3/16)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:17 INFO scheduler.TaskSetManager: Finished task 13.0 in stage 4.0 (TID 5) in 132 ms on algo-1 (executor 1) (4/16)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:17 INFO scheduler.TaskSetManager: Starting task 11.0 in stage 4.0 (TID 15, algo-1, executor 1, partition 11, PROCESS_LOCAL, 7673 bytes)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:17 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 4.0 (TID 11) in 19 ms on algo-1 (executor 1) (5/16)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:17 INFO scheduler.TaskSetManager: Starting task 12.0 in stage 4.0 (TID 16, algo-1, executor 1, partition 12, PROCESS_LOCAL, 7673 bytes)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:17 INFO scheduler.TaskSetManager: Finished task 10.0 in stage 4.0 (TID 14) in 26 ms on algo-1 (executor 1) (6/16)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:17 INFO scheduler.TaskSetManager: Starting task 14.0 in stage 4.0 (TID 17, algo-1, executor 1, partition 14, PROCESS_LOCAL, 7673 bytes)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:17 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 4.0 (TID 12) in 36 ms on algo-1 (executor 1) (7/16)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:17 INFO scheduler.TaskSetManager: Starting task 15.0 in stage 4.0 (TID 18, algo-1, executor 1, partition 15, PROCESS_LOCAL, 7673 bytes)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:17 INFO scheduler.TaskSetManager: Finished task 11.0 in stage 4.0 (TID 15) in 22 ms on algo-1 (executor 1) (8/16)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:17 INFO scheduler.TaskSetManager: Finished task 8.0 in stage 4.0 (TID 13) in 33 ms on algo-1 (executor 1) (9/16)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:17 INFO scheduler.TaskSetManager: Finished task 15.0 in stage 4.0 (TID 18) in 16 ms on algo-1 (executor 1) (10/16)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:17 INFO scheduler.TaskSetManager: Finished task 12.0 in stage 4.0 (TID 16) in 26 ms on algo-1 (executor 1) (11/16)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:17 INFO scheduler.TaskSetManager: Finished task 14.0 in stage 4.0 (TID 17) in 27 ms on algo-1 (executor 1) (12/16)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 4.0 (TID 10) in 391 ms on algo-2 (executor 2) (13/16)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 4.0 (TID 8) in 394 ms on algo-2 (executor 2) (14/16)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 4.0 (TID 9) in 395 ms on algo-2 (executor 2) (15/16)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 6) in 396 ms on algo-2 (executor 2) (16/16)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO cluster.YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO scheduler.DAGScheduler: ResultStage 4 (countByValue at StringIndexer.scala:140) finished in 0.406 s\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO scheduler.DAGScheduler: Job 2 finished: countByValue at StringIndexer.scala:140, took 4.318772 s\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO spark.ContextCleaner: Cleaned accumulator 120\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO spark.ContextCleaner: Cleaned shuffle 1\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 10.2.226.32:34693 in memory (size: 2.2 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on algo-2:46083 in memory (size: 2.2 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on algo-1:40047 in memory (size: 2.2 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO spark.ContextCleaner: Cleaned accumulator 84\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO spark.ContextCleaner: Cleaned accumulator 95\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO spark.ContextCleaner: Cleaned accumulator 112\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on algo-1:40047 in memory (size: 10.3 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 10.2.226.32:34693 in memory (size: 10.3 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO spark.ContextCleaner: Cleaned accumulator 114\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO spark.ContextCleaner: Cleaned accumulator 135\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO spark.ContextCleaner: Cleaned accumulator 98\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO spark.ContextCleaner: Cleaned accumulator 123\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO spark.ContextCleaner: Cleaned accumulator 121\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO spark.ContextCleaner: Cleaned accumulator 83\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO spark.ContextCleaner: Cleaned accumulator 80\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO spark.ContextCleaner: Cleaned accumulator 101\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO spark.ContextCleaner: Cleaned accumulator 125\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO spark.ContextCleaner: Cleaned accumulator 93\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO spark.ContextCleaner: Cleaned accumulator 90\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO spark.ContextCleaner: Cleaned accumulator 117\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO spark.ContextCleaner: Cleaned accumulator 134\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO spark.ContextCleaner: Cleaned accumulator 102\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO spark.ContextCleaner: Cleaned accumulator 113\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO spark.ContextCleaner: Cleaned accumulator 118\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO spark.ContextCleaner: Cleaned accumulator 97\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO spark.ContextCleaner: Cleaned accumulator 131\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO spark.ContextCleaner: Cleaned accumulator 124\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO spark.ContextCleaner: Cleaned accumulator 92\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO spark.ContextCleaner: Cleaned accumulator 109\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO spark.ContextCleaner: Cleaned accumulator 108\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO spark.ContextCleaner: Cleaned accumulator 86\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO spark.ContextCleaner: Cleaned accumulator 132\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO spark.ContextCleaner: Cleaned accumulator 111\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO spark.ContextCleaner: Cleaned accumulator 110\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO spark.ContextCleaner: Cleaned accumulator 85\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO spark.ContextCleaner: Cleaned accumulator 91\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO spark.ContextCleaner: Cleaned accumulator 107\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO spark.ContextCleaner: Cleaned accumulator 100\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO spark.ContextCleaner: Cleaned accumulator 127\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO spark.ContextCleaner: Cleaned accumulator 104\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO spark.ContextCleaner: Cleaned accumulator 99\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO spark.ContextCleaner: Cleaned accumulator 87\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO spark.ContextCleaner: Cleaned accumulator 119\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO spark.ContextCleaner: Cleaned accumulator 116\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO spark.ContextCleaner: Cleaned accumulator 103\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO spark.ContextCleaner: Cleaned accumulator 89\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO spark.ContextCleaner: Cleaned accumulator 96\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO spark.ContextCleaner: Cleaned accumulator 129\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO spark.ContextCleaner: Cleaned accumulator 106\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO spark.ContextCleaner: Cleaned accumulator 105\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO spark.ContextCleaner: Cleaned accumulator 115\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on algo-1:40047 in memory (size: 27.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 10.2.226.32:34693 in memory (size: 27.4 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO spark.ContextCleaner: Cleaned accumulator 130\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO spark.ContextCleaner: Cleaned accumulator 94\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO spark.ContextCleaner: Cleaned accumulator 81\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO spark.ContextCleaner: Cleaned accumulator 126\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO spark.ContextCleaner: Cleaned accumulator 128\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO spark.ContextCleaner: Cleaned accumulator 82\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO spark.ContextCleaner: Cleaned accumulator 122\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO spark.ContextCleaner: Cleaned accumulator 88\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO spark.ContextCleaner: Cleaned accumulator 133\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:16 INFO codegen.CodeGenerator: Code generated in 17.162863 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO datasources.FileScanRDD: TID: 2 - Reading current file: path: s3://sagemaker-us-east-1-193890026231/sagemaker/spark-preprocess-demo/2021-12-05-17-53-35/input/raw/abalone/abalone.csv, range: 0-196156, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO codegen.CodeGenerator: Code generated in 11.409749 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 3\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 27.4 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO broadcast.TorrentBroadcast: Reading broadcast variable 3 took 9 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 401.8 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO executor.Executor: Finished task 0.0 in stage 3.0 (TID 2). 2030 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 3\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO executor.Executor: Running task 6.0 in stage 4.0 (TID 3)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 4\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO executor.Executor: Running task 9.0 in stage 4.0 (TID 4)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 5\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 7\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO executor.Executor: Running task 13.0 in stage 4.0 (TID 5)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO executor.Executor: Running task 1.0 in stage 4.0 (TID 7)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO spark.MapOutputTrackerWorker: Updating epoch to 2 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 5\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 2.2 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO broadcast.TorrentBroadcast: Reading broadcast variable 5 took 18 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 3.6 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 1, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 1, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 1, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.2.226.32:43569)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 1, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO storage.ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO executor.Executor: Finished task 1.0 in stage 4.0 (TID 7). 1134 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO executor.Executor: Finished task 9.0 in stage 4.0 (TID 4). 1324 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO executor.Executor: Finished task 6.0 in stage 4.0 (TID 3). 1350 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO executor.Executor: Finished task 13.0 in stage 4.0 (TID 5). 1281 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 11\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 12\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO executor.Executor: Running task 5.0 in stage 4.0 (TID 11)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO executor.Executor: Running task 7.0 in stage 4.0 (TID 12)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 13\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO executor.Executor: Running task 8.0 in stage 4.0 (TID 13)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO executor.Executor: Finished task 5.0 in stage 4.0 (TID 11). 1134 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 14\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO executor.Executor: Running task 10.0 in stage 4.0 (TID 14)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 15\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO executor.Executor: Running task 11.0 in stage 4.0 (TID 15)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO datasources.FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, sex#0),UDF(sex#0)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO datasources.FileSourceStrategy: Output Data Schema: struct<sex: string, length: double, diameter: double, height: double, whole_weight: double ... 7 more fields>\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO codegen.CodeGenerator: Code generated in 96.954566 ms\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 303.5 KB, free 1028.2 MB)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 27.4 KB, free 1028.2 MB)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.2.226.32:34693 (size: 27.4 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO spark.SparkContext: Created broadcast 6 from javaToPython at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: ArrayBuffer((1 fileSplits,1))\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO Configuration.deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO io.HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:18 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:19 INFO spark.ContextCleaner: Cleaned accumulator 6\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:19 INFO spark.ContextCleaner: Cleaned accumulator 5\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:19 INFO spark.ContextCleaner: Cleaned accumulator 1\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:19 INFO spark.ContextCleaner: Cleaned accumulator 4\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:19 INFO spark.ContextCleaner: Cleaned accumulator 3\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:19 INFO spark.ContextCleaner: Cleaned accumulator 7\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:19 INFO spark.SparkContext: Starting job: runJob at SparkHadoopWriter.scala:78\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:19 INFO scheduler.DAGScheduler: Got job 3 (runJob at SparkHadoopWriter.scala:78) with 1 output partitions\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:19 INFO scheduler.DAGScheduler: Final stage: ResultStage 5 (runJob at SparkHadoopWriter.scala:78)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:19 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:19 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:19 INFO scheduler.DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[23] at saveAsTextFile at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:19 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 137.1 KB, free 1028.1 MB)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:19 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 51.8 KB, free 1028.0 MB)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:19 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.2.226.32:34693 (size: 51.8 KB, free: 1028.7 MB)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:19 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:19 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[23] at saveAsTextFile at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:19 INFO cluster.YarnScheduler: Adding task set 5.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:19 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 19, algo-2, executor 2, partition 0, PROCESS_LOCAL, 8346 bytes)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:19 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on algo-2:46083 (size: 51.8 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:17 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stdout] 2021-12-05T17:58:06.325+0000: [GC (Allocation Failure) [PSYoungGen: 57344K->4244K(66560K)] 57344K->4252K(218112K), 0.0053267 secs] [Times: user=0.01 sys=0.00, real=0.00 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stdout] 2021-12-05T17:58:06.509+0000: [GC (Allocation Failure) [PSYoungGen: 61588K->3508K(66560K)] 61596K->3524K(218112K), 0.0030483 secs] [Times: user=0.01 sys=0.00, real=0.01 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stdout] 2021-12-05T17:58:06.773+0000: [GC (Allocation Failure) [PSYoungGen: 60852K->4435K(66560K)] 60868K->4459K(218112K), 0.0031465 secs] [Times: user=0.01 sys=0.00, real=0.00 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stdout] 2021-12-05T17:58:06.971+0000: [GC (Allocation Failure) [PSYoungGen: 61779K->4899K(123904K)] 61803K->4931K(275456K), 0.0043087 secs] [Times: user=0.01 sys=0.00, real=0.00 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stdout] 2021-12-05T17:58:07.466+0000: [GC (Allocation Failure) [PSYoungGen: 119587K->6836K(123904K)] 119619K->6876K(275456K), 0.0072354 secs] [Times: user=0.02 sys=0.01, real=0.01 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stdout] 2021-12-05T17:58:07.769+0000: [GC (Allocation Failure) [PSYoungGen: 121524K->7164K(235520K)] 121564K->7391K(387072K), 0.0074046 secs] [Times: user=0.01 sys=0.01, real=0.01 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stdout] 2021-12-05T17:58:07.957+0000: [GC (Metadata GC Threshold) [PSYoungGen: 66614K->4120K(236544K)] 66842K->22842K(388096K), 0.0149695 secs] [Times: user=0.05 sys=0.01, real=0.02 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stdout] 2021-12-05T17:58:07.972+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 4120K->0K(236544K)] [ParOldGen: 18722K->22466K(106496K)] 22842K->22466K(343040K), [Metaspace: 20937K->20937K(1067008K)], 0.0289787 secs] [Times: user=0.07 sys=0.00, real=0.02 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stdout] 2021-12-05T17:58:08.687+0000: [GC (Allocation Failure) [PSYoungGen: 228352K->7840K(396288K)] 250818K->46699K(502784K), 0.0173260 secs] [Times: user=0.04 sys=0.02, real=0.02 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stdout] 2021-12-05T17:58:09.497+0000: [GC (Metadata GC Threshold) [PSYoungGen: 218611K->8699K(464384K)] 257469K->48921K(570880K), 0.0108787 secs] [Times: user=0.03 sys=0.01, real=0.01 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stdout] 2021-12-05T17:58:09.508+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 8699K->0K(464384K)] [ParOldGen: 40222K->47824K(172032K)] 48921K->47824K(636416K), [Metaspace: 34792K->34792K(1079296K)], 0.0530773 secs] [Times: user=0.16 sys=0.00, real=0.05 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stdout] 2021-12-05T17:58:12.493+0000: [GC (Allocation Failure) [PSYoungGen: 455680K->10238K(543232K)] 503504K->81753K(715264K), 0.0273611 secs] [Times: user=0.03 sys=0.03, real=0.02 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stdout] 2021-12-05T17:58:19.885+0000: [GC (Metadata GC Threshold) [PSYoungGen: 232505K->8393K(553472K)] 304020K->86984K(725504K), 0.0162736 secs] [Times: user=0.04 sys=0.01, real=0.01 secs] \u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stdout] 2021-12-05T17:58:19.901+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 8393K->0K(553472K)] [ParOldGen: 78591K->73930K(239104K)] 86984K->73930K(792576K), [Metaspace: 58189K->581empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:17 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:17 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:17 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:17 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:17 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:17 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:17 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:18 INFO executor.Executor: Finished task 4.0 in stage 4.0 (TID 10). 1134 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:18 INFO executor.Executor: Finished task 2.0 in stage 4.0 (TID 8). 1134 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:18 INFO executor.Executor: Finished task 3.0 in stage 4.0 (TID 9). 1134 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:18 INFO executor.Executor: Finished task 0.0 in stage 4.0 (TID 6). 1177 bytes result sent to driver\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:19 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 19\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:19 INFO executor.Executor: Running task 0.0 in stage 5.0 (TID 19)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:19 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 7\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:19 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 51.8 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:19 INFO broadcast.TorrentBroadcast: Reading broadcast variable 7 took 7 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:19 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 137.1 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:19 INFO codegen.CodeGenerator: Code generated in 90.094089 ms\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:58:20 INFO codegen.CodeGenerator: Code generated in 169.252491 ms\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:20 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on algo-2:46083 (size: 27.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m12-05 17:58 sagemaker-spark-event-logs-publisher INFO     Got spark event logs file: application_1638727061555_0001.inprogress\u001b[0m\n",
      "\u001b[34m12-05 17:58 root         INFO     copying /tmp/spark-events/application_1638727061555_0001.inprogress to /opt/ml/processing/spark-events/application_1638727061555_0001\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:22 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 19) in 2876 ms on algo-2 (executor 2) (1/1)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:22 INFO cluster.YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m21/12/05 17:58:22 INFO python.PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 39195\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:22 INFO scheduler.DAGScheduler: ResultStage 5 (runJob at SparkHadoopWriter.scala:78) finished in 2.908 s\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:22 INFO scheduler.DAGScheduler: Job 3 finished: runJob at SparkHadoopWriter.scala:78, took 2.914163 s\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:22 INFO io.SparkHadoopWriter: Job job_20211205175818_0023 committed.\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:23 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m21/12/05 17:58:23 INFO datasources.FileSourceStrategy: Post-Scan Filters: AtLeastNNulls(n, sex#0),UDF(sex#0)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:23 INFO datasources.FileSourceStrategy: Output Data Schema: struct<sex: string, length: double, diameter: double, height: double, whole_weight: double ... 7 more fields>\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:23 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m21/12/05 17:58:23 INFO codegen.CodeGenerator: Code generated in 173.670276 ms\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:23 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 303.5 KB, free 1027.7 MB)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:23 INFO spark.ContextCleaner: Cleaned accumulator 153\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:23 INFO spark.ContextCleaner: Cleaned accumulator 164\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:23 INFO spark.ContextCleaner: Cleaned accumulator 149\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:23 INFO spark.ContextCleaner: Cleaned accumulator 161\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:23 INFO spark.ContextCleaner: Cleaned accumulator 148\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:23 INFO spark.ContextCleaner: Cleaned accumulator 170\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:23 INFO spark.ContextCleaner: Cleaned accumulator 160\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:23 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on 10.2.226.32:34693 in memory (size: 51.8 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:23 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on algo-2:46083 in memory (size: 51.8 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:23 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 27.4 KB, free 1027.9 MB)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:23 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.2.226.32:34693 (size: 27.4 KB, free: 1028.8 MB)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:23 INFO spark.SparkContext: Created broadcast 8 from javaToPython at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:23 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 1, prefetch: false\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:23 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: ArrayBuffer((1 fileSplits,1))\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:23 INFO spark.ContextCleaner: Cleaned accumulator 150\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:23 INFO spark.ContextCleaner: Cleaned accumulator 156\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:23 INFO spark.ContextCleaner: Cleaned accumulator 169\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:23 INFO spark.ContextCleaner: Cleaned accumulator 168\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:23 INFO spark.ContextCleaner: Cleaned accumulator 151\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:23 INFO spark.ContextCleaner: Cleaned accumulator 162\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:23 INFO spark.ContextCleaner: Cleaned accumulator 157\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:23 INFO spark.ContextCleaner: Cleaned accumulator 165\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:23 INFO spark.ContextCleaner: Cleaned accumulator 152\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:23 INFO spark.ContextCleaner: Cleaned accumulator 167\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:23 INFO spark.ContextCleaner: Cleaned accumulator 155\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:23 INFO spark.ContextCleaner: Cleaned accumulator 163\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:23 INFO spark.ContextCleaner: Cleaned accumulator 166\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:23 INFO spark.ContextCleaner: Cleaned accumulator 146\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:23 INFO spark.ContextCleaner: Cleaned accumulator 147\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:23 INFO spark.ContextCleaner: Cleaned accumulator 159\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:23 INFO spark.ContextCleaner: Cleaned accumulator 154\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:23 INFO spark.ContextCleaner: Cleaned accumulator 158\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:23 INFO io.HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:23 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:23 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:23 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:23 INFO spark.SparkContext: Starting job: runJob at SparkHadoopWriter.scala:78\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:23 INFO scheduler.DAGScheduler: Got job 4 (runJob at SparkHadoopWriter.scala:78) with 1 output partitions\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:23 INFO scheduler.DAGScheduler: Final stage: ResultStage 6 (runJob at SparkHadoopWriter.scala:78)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:23 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:23 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:23 INFO scheduler.DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[31] at saveAsTextFile at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:23 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 137.1 KB, free 1027.7 MB)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:24 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 51.8 KB, free 1027.7 MB)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:24 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.2.226.32:34693 (size: 51.8 KB, free: 1028.7 MB)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:24 INFO spark.SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:24 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[31] at saveAsTextFile at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:24 INFO cluster.YarnScheduler: Adding task set 6.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:24 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 20, algo-1, executor 1, partition 0, PROCESS_LOCAL, 8346 bytes)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:24 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on algo-1:40047 (size: 51.8 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO executor.Executor: Finished task 7.0 in stage 4.0 (TID 12). 1134 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO executor.Executor: Finished task 10.0 in stage 4.0 (TID 14). 1134 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO executor.Executor: Finished task 11.0 in stage 4.0 (TID 15). 1134 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO executor.Executor: Finished task 8.0 in stage 4.0 (TID 13). 1134 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 16\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO executor.Executor: Running task 12.0 in stage 4.0 (TID 16)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 17\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 18\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO executor.Executor: Running task 15.0 in stage 4.0 (TID 18)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO executor.Executor: Running task 14.0 in stage 4.0 (TID 17)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO storage.ShuffleBlockFetcherIterator: Getting 0 non-empty blocks including 0 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO executor.Executor: Finished task 15.0 in stage 4.0 (TID 18). 1134 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO executor.Executor: Finished task 12.0 in stage 4.0 (TID 16). 1134 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:17 INFO executor.Executor: Finished task 14.0 in stage 4.0 (TID 17). 1134 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:24 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on algo-1:40047 (size: 27.4 KB, free: 6.3 GB)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:26 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 20) in 2661 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:26 INFO cluster.YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m21/12/05 17:58:26 INFO scheduler.DAGScheduler: ResultStage 6 (runJob at SparkHadoopWriter.scala:78) finished in 3.036 s\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:26 INFO scheduler.DAGScheduler: Job 4 finished: runJob at SparkHadoopWriter.scala:78, took 3.039140 s\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:27 INFO io.SparkHadoopWriter: Job job_20211205175823_0031 committed.\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:27 INFO spark.SparkContext: Invoking stop() from shutdown hook\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:27 INFO server.AbstractConnector: Stopped Spark@f70a399{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:27 INFO ui.SparkUI: Stopped Spark web UI at http://10.2.226.32:4040\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:27 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:27 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:27 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:27 INFO cluster.SchedulerExtensionServices: Stopping SchedulerExtensionServices\u001b[0m\n",
      "\u001b[34m(serviceOption=None,\n",
      " services=List(),\n",
      " started=false)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:27 INFO cluster.YarnClientSchedulerBackend: Stopped\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:24 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 20\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:24 INFO executor.Executor: Running task 0.0 in stage 6.0 (TID 20)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:24 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 9\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:24 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 51.8 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:24 INFO broadcast.TorrentBroadcast: Reading broadcast variable 9 took 13 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:24 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 137.1 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:24 INFO codegen.CodeGenerator: Code generated in 120.831749 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:24 INFO codegen.CodeGenerator: Code generated in 61.280775 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:24 INFO codegen.CodeGenerator: Code generated in 16.621975 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:24 INFO datasources.FileScanRDD: TID: 20 - Reading current file: path: s3://sagemaker-us-east-1-193890026231/sagemaker/spark-preprocess-demo/2021-12-05-17-53-35/input/raw/abalone/abalone.csv, range: 0-196156, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:24 INFO io.HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:24 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:24 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:24 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:24 INFO codegen.CodeGenerator: Code generated in 14.575898 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:24 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 8\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:24 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 27.4 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:24 INFO broadcast.TorrentBroadcast: Reading broadcast variable 8 took 11 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:24 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 401.8 KB, free 6.3 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stderr] 21/12/05 17:58:25 INFO python.PythonRunner: Times: total = 1102, boot = 374, init = 504, finish = 224\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:27 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:27 INFO memory.MemoryStore: MemoryStore cleared\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:27 INFO storage.BlockManager: BlockManager stopped\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:27 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:27 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:27 INFO spark.SparkContext: Successfully stopped SparkContext\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:27 INFO util.ShutdownHookManager: Shutdown hook called\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:27 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-df93933a-95b4-4d38-b5b7-5c5b96b6eae0\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:27 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-3caa1682-3b63-4ce7-8308-01767f4d1ed1\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:27 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-3caa1682-3b63-4ce7-8308-01767f4d1ed1/pyspark-96f3e451-e4c6-4761-bb42-a9eaa13bac25\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:27 INFO attempt.RMAppAttemptImpl: Updating application attempt appattempt_1638727061555_0001_000001 with final state: FINISHING, and exit status: -1000\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:27 INFO attempt.RMAppAttemptImpl: appattempt_1638727061555_0001_000001 State change from RUNNING to FINAL_SAVING on event = UNREGISTERED\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:27 INFO rmapp.RMAppImpl: Updating application application_1638727061555_0001 with final state: FINISHING\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:27 INFO recovery.RMStateStore: Updating info for app: application_1638727061555_0001\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:27 INFO rmapp.RMAppImpl: application_1638727061555_0001 State change from RUNNING to FINAL_SAVING on event = ATTEMPT_UNREGISTERED\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:27 INFO attempt.RMAppAttemptImpl: appattempt_1638727061555_0001_000001 State change from FINAL_SAVING to FINISHING on event = ATTEMPT_UPDATE_SAVED\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:27 INFO rmapp.RMAppImpl: application_1638727061555_0001 State change from FINAL_SAVING to FINISHING on event = APP_UPDATE_SAVED\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:27 INFO rmcontainer.RMContainerImpl: container_1638727061555_0001_01_000003 Container Transitioned from RUNNING to COMPLETED\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:27 INFO resourcemanager.RMAuditLogger: USER=root#011OPERATION=AM Released Container#011TARGET=SchedulerApp#011RESULT=SUCCESS#011APPID=application_1638727061555_0001#011CONTAINERID=container_1638727061555_0001_01_000003#011RESOURCE=<memory:13638, vCores:1>#011QUEUENAME=default\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:27 INFO launcher.ContainerLaunch: Container container_1638727061555_0001_01_000002 succeeded \u001b[0m\n",
      "\u001b[34m21/12/05 17:58:27 INFO container.ContainerImpl: Container container_1638727061555_0001_01_000002 transitioned from RUNNING to EXITED_WITH_SUCCESS\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:27 INFO launcher.ContainerLaunch: Cleaning up container container_1638727061555_0001_01_000002\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:27 INFO nodemanager.NMAuditLogger: USER=root#011OPERATION=Container Finished - Succeeded#011TARGET=ContainerImpl#011RESULT=SUCCESS#011APPID=application_1638727061555_0001#011CONTAINERID=container_1638727061555_0001_01_000002\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:27 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1638727061555_0001/container_1638727061555_0001_01_000002\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:27 INFO container.ContainerImpl: Container container_1638727061555_0001_01_000002 transitioned from EXITED_WITH_SUCCESS to DONE\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:27 INFO application.ApplicationImpl: Removing container_1638727061555_0001_01_000002 from application application_1638727061555_0001\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:27 INFO monitor.ContainersMonitorImpl: Stopping resource-monitoring for container_1638727061555_0001_01_000002\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:27 INFO containermanager.AuxServices: Got event CONTAINER_STOP for appId application_1638727061555_0001\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:27 INFO rmcontainer.RMContainerImpl: container_1638727061555_0001_01_000002 Container Transitioned from RUNNING to COMPLETED\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:27 INFO resourcemanager.RMAuditLogger: USER=root#011OPERATION=AM Released Container#011TARGET=SchedulerApp#011RESULT=SUCCESS#011APPID=application_1638727061555_0001#011CONTAINERID=container_1638727061555_0001_01_000002#011RESOURCE=<memory:13638, vCores:1>#011QUEUENAME=default\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:27 INFO resourcemanager.ApplicationMasterService: application_1638727061555_0001 unregistered successfully. \u001b[0m\n",
      "\u001b[35m21/12/05 17:58:27 INFO launcher.ContainerLaunch: Container container_1638727061555_0001_01_000003 succeeded \u001b[0m\n",
      "\u001b[35m21/12/05 17:58:27 INFO container.ContainerImpl: Container container_1638727061555_0001_01_000003 transitioned from RUNNING to EXITED_WITH_SUCCESS\u001b[0m\n",
      "\u001b[35m21/12/05 17:58:27 INFO launcher.ContainerLaunch: Cleaning up container container_1638727061555_0001_01_000003\u001b[0m\n",
      "\u001b[35m21/12/05 17:58:27 INFO nodemanager.NMAuditLogger: USER=root#011OPERATION=Container Finished - Succeeded#011TARGET=ContainerImpl#011RESULT=SUCCESS#011APPID=application_1638727061555_0001#011CONTAINERID=container_1638727061555_0001_01_000003\u001b[0m\n",
      "\u001b[35m21/12/05 17:58:27 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1638727061555_0001/container_1638727061555_0001_01_000003\u001b[0m\n",
      "\u001b[35m21/12/05 17:58:27 INFO container.ContainerImpl: Container container_1638727061555_0001_01_000003 transitioned from EXITED_WITH_SUCCESS to DONE\u001b[0m\n",
      "\u001b[35m21/12/05 17:58:27 INFO application.ApplicationImpl: Removing container_1638727061555_0001_01_000003 from application application_1638727061555_0001\u001b[0m\n",
      "\u001b[35m21/12/05 17:58:27 INFO monitor.ContainersMonitorImpl: Stopping resource-monitoring for container_1638727061555_0001_01_000003\u001b[0m\n",
      "\u001b[35m21/12/05 17:58:27 INFO containermanager.AuxServices: Got event CONTAINER_STOP for appId application_1638727061555_0001\u001b[0m\n",
      "\u001b[35m21/12/05 17:58:27 INFO launcher.ContainerLaunch: Container container_1638727061555_0001_01_000001 succeeded \u001b[0m\n",
      "\u001b[35m21/12/05 17:58:27 INFO container.ContainerImpl: Container container_1638727061555_0001_01_000001 transitioned from RUNNING to EXITED_WITH_SUCCESS\u001b[0m\n",
      "\u001b[35m21/12/05 17:58:27 INFO launcher.ContainerLaunch: Cleaning up container container_1638727061555_0001_01_000001\u001b[0m\n",
      "\u001b[35m21/12/05 17:58:27 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1638727061555_0001/container_1638727061555_0001_01_000001\u001b[0m\n",
      "\u001b[35m21/12/05 17:58:27 INFO nodemanager.NMAuditLogger: USER=root#011OPERATION=Container Finished - Succeeded#011TARGET=ContainerImpl#011RESULT=SUCCESS#011APPID=application_1638727061555_0001#011CONTAINERID=container_1638727061555_0001_01_000001\u001b[0m\n",
      "\u001b[35m21/12/05 17:58:27 INFO container.ContainerImpl: Container container_1638727061555_0001_01_000001 transitioned from EXITED_WITH_SUCCESS to DONE\u001b[0m\n",
      "\u001b[35m21/12/05 17:58:27 INFO application.ApplicationImpl: Removing container_1638727061555_0001_01_000001 from application application_1638727061555_0001\u001b[0m\n",
      "\u001b[35m21/12/05 17:58:27 INFO monitor.ContainersMonitorImpl: Stopping resource-monitoring for container_1638727061555_0001_01_000001\u001b[0m\n",
      "\u001b[35m21/12/05 17:58:27 INFO containermanager.AuxServices: Got event CONTAINER_STOP for appId application_1638727061555_0001\u001b[0m\n",
      "\u001b[34m12-05 17:58 smspark-submit INFO     spark submit was successful. primary node exiting.\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:27 INFO rmcontainer.RMContainerImpl: container_1638727061555_0001_01_000001 Container Transitioned from RUNNING to COMPLETED\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:27 INFO resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1638727061555_0001_000001\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:27 INFO resourcemanager.RMAuditLogger: USER=root#011OPERATION=AM Released Container#011TARGET=SchedulerApp#011RESULT=SUCCESS#011APPID=application_1638727061555_0001#011CONTAINERID=container_1638727061555_0001_01_000001#011RESOURCE=<memory:896, vCores:1>#011QUEUENAME=default\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:27 INFO security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1638727061555_0001_000001\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:27 INFO attempt.RMAppAttemptImpl: appattempt_1638727061555_0001_000001 State change from FINISHING to FINISHED on event = CONTAINER_FINISHED\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:27 INFO rmapp.RMAppImpl: application_1638727061555_0001 State change from FINISHING to FINISHED on event = ATTEMPT_FINISHED\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:27 INFO capacity.CapacityScheduler: Application Attempt appattempt_1638727061555_0001_000001 is done. finalState=FINISHED\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:27 INFO scheduler.AppSchedulingInfo: Application application_1638727061555_0001 requests cleared\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:27 INFO amlauncher.AMLauncher: Cleaning master appattempt_1638727061555_0001_000001\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:27 INFO resourcemanager.RMAuditLogger: USER=root#011OPERATION=Application Finished - Succeeded#011TARGET=RMAppManager#011RESULT=SUCCESS#011APPID=application_1638727061555_0001\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:27 INFO capacity.LeafQueue: Application removed - appId: application_1638727061555_0001 user: root queue: default #user-pending-applications: 0 #user-active-applications: 0 #queue-pending-applications: 0 #queue-active-applications: 0\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:27 INFO capacity.ParentQueue: Application removed - appId: application_1638727061555_0001 user: root leaf-queue of parent: root #applications: 0\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:27 INFO resourcemanager.RMAppManager$ApplicationSummary: appId=application_1638727061555_0001,name=PySparkApp,user=root,queue=default,state=FINISHED,trackingUrl=http://algo-1:8088/proxy/application_1638727061555_0001/,appMasterHost=10.2.213.203,submitTime=1638727075068,startTime=1638727075111,launchTime=1638727075828,finishTime=1638727107379,finalStatus=SUCCEEDED,memorySeconds=631061,vcoreSeconds=75,preemptedMemorySeconds=0,preemptedVcoreSeconds=0,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\\, vCores:0>,applicationType=SPARK,resourceSeconds=631061 MB-seconds\\, 75 vcore-seconds,preemptedResourceSeconds=0 MB-seconds\\, 0 vcore-seconds,applicationTags=,applicationNodeLabel=\u001b[0m\n",
      "\u001b[34m[/var/log/yar[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stdout] 2021-12-05T17:58:10.196+0000: [GC (Allocation Failure) [PSYoungGen: 57344K->4527K(66560K)] 57344K->4527K(218112K), 0.0150864 secs] [Times: user=0.01 sys=0.00, real=0.02 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stdout] 2021-12-05T17:58:10.540+0000: [GC (Allocation Failure) [PSYoungGen: 61871K->4657K(123904K)] 61871K->4665K(275456K), 0.0127267 secs] [Times: user=0.02 sys=0.00, real=0.01 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stdout] 2021-12-05T17:58:11.091+0000: [GC (Allocation Failure) [PSYoungGen: 119345K->6948K(123904K)] 119353K->6964K(275456K), 0.0084785 secs] [Times: user=0.01 sys=0.00, real=0.01 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stdout] 2021-12-05T17:58:11.464+0000: [GC (Allocation Failure) [PSYoungGen: 121636K->6799K(238592K)] 121652K->6823K(390144K), 0.0078482 secs] [Times: user=0.01 sys=0.01, real=0.01 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stdout] 2021-12-05T17:58:11.770+0000: [GC (Metadata GC Threshold) [PSYoungGen: 127348K->7851K(238592K)] 127372K->24267K(390144K), 0.0219711 secs] [Times: user=0.06 sys=0.02, real=0.02 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stdout] 2021-12-05T17:58:11.792+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 7851K->0K(238592K)] [ParOldGen: 16416K->23973K(106496K)] 24267K->23973K(345088K), [Metaspace: 20980K->20980K(1067008K)], 0.0269250 secs] [Times: user=0.05 sys=0.01, real=0.03 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stdout] 2021-12-05T17:58:12.483+0000: [GC (Allocation Failure) [PSYoungGen: 229376K->8081K(409088K)] 253349K->48447K(515584K), 0.0170166 secs] [Times: user=0.05 sys=0.01, real=0.02 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stdout] 2021-12-05T17:58:13.569+0000: [GC (Metadata GC Threshold) [PSYoungGen: 194522K->9708K(466432K)] 234888K->50297K(572928K), 0.0128010 secs] [Times: user=0.01 sys=0.01, real=0.01 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stdout] 2021-12-05T17:58:13.582+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 9708K->0K(466432K)] [ParOldGen: 40589K->47645K(172544K)] 50297K->47645K(638976K), [Metaspace: 34928K->34928K(1079296K)], 0.0841570 secs] [Times: user=0.14 sys=0.00, real=0.08 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stdout] 2021-12-05T17:58:15.755+0000: [GC (Allocation Failure) [PSYoungGen: 456704K->11246K(671232K)] 504349K->82784K(843776K), 0.0240939 secs] [Times: user=0.04 sys=0.02, real=0.03 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stdout] 2021-12-05T17:58:16.637+0000: [GC (Metadata GC Threshold) [PSYoungGen: 235459K->14134K(686080K)] 306997K->85673K(858624K), 0.0143326 secs] [Times: user=0.03 sys=0.00, real=0.01 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stdout] 2021-12-05T17:58:16.652+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 14134K->0K(686080K)] [ParOldGen: 71538K->72812K(242688K)] 85673K->72812K(928768K), [Metaspace: 58363K->58363K(1099776K)], 0.1006099 secs] [Times: user=0.24 sys=0.00, real=0.10 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stdout] Heap\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stdout]  PSYoungGen      total 686080K, used 591299K [0x00000006bdb00000, 0x00000006f3200000, 0x00000007c0000000)\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:28 INFO application.ApplicationImpl: Application application_1638727061555_0001 transitioned from RUNNING to APPLICATION_RESOURCES_CLEANINGUP\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:28 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1638727061555_0001\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:28 INFO containermanager.AuxServices: Got event APPLICATION_STOP for appId application_1638727061555_0001\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:28 INFO application.ApplicationImpl: Application application_1638727061555_0001 transitioned from APPLICATION_RESOURCES_CLEANINGUP to FINISHED\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:28 INFO loghandler.NonAggregatingLogHandler: Scheduling Log Deletion for application: application_1638727061555_0001, with delay of 10800 seconds\u001b[0m\n",
      "\u001b[35m21/12/05 17:58:27 INFO ipc.Server: Auth successful for appattempt_1638727061555_0001_000001 (auth:SIMPLE)\u001b[0m\n",
      "\u001b[35m21/12/05 17:58:27 INFO containermanager.ContainerManagerImpl: Stopping container with container Id: container_1638727061555_0001_01_000001\u001b[0m\n",
      "\u001b[35m21/12/05 17:58:27 INFO nodemanager.NMAuditLogger: USER=root#011IP=10.2.226.32#011OPERATION=Stop Container Request#011TARGET=ContainerManageImpl#011RESULT=SUCCESS#011APPID=application_1638727061555_0001#011CONTAINERID=container_1638727061555_0001_01_000001\u001b[0m\n",
      "\u001b[35m21/12/05 17:58:28 INFO nodemanager.NodeStatusUpdaterImpl: Removed completed containers from NM context: [container_1638727061555_0001_01_000001]\u001b[0m\n",
      "\u001b[35m21/12/05 17:58:28 INFO application.ApplicationImpl: Application application_1638727061555_0001 transitioned from RUNNING to APPLICATION_RESOURCES_CLEANINGUP\u001b[0m\n",
      "\u001b[35m21/12/05 17:58:28 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1638727061555_0001\u001b[0m\n",
      "\u001b[35m21/12/05 17:58:28 INFO containermanager.AuxServices: Got event APPLICATION_STOP for appId application_1638727061555_0001\u001b[0m\n",
      "\u001b[35m21/12/05 17:58:28 INFO application.ApplicationImpl: Application application_1638727061555_0001 transitioned from APPLICATION_RESOURCES_CLEANINGUP to FINISHED\u001b[0m\n",
      "\u001b[35m21/12/05 17:58:28 INFO loghandler.NonAggregatingLogHandler: Scheduling Log Deletion for application: application_1638727061555_0001, with delay of 10800 seconds\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:31 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741825_1001 file /opt/amazon/hadoop/hdfs/datanode/current/BP-876741428-10.2.226.32-1638727053679/current/finalized/subdir0/subdir0/blk_1073741825 for deletion\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:31 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741826_1002 file /opt/amazon/hadoop/hdfs/datanode/current/BP-876741428-10.2.226.32-1638727053679/current/finalized/subdir0/subdir0/blk_1073741826 for deletion\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:31 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741827_1003 file /opt/amazon/hadoop/hdfs/datanode/current/BP-876741428-10.2.226.32-1638727053679/current/finalized/subdir0/subdir0/blk_1073741827 for deletion\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:31 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741828_1004 file /opt/amazon/hadoop/hdfs/datanode/current/BP-876741428-10.2.226.32-1638727053679/current/finalized/subdir0/subdir0/blk_1073741828 for deletion\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:31 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741829_1005 file /opt/amazon/hadoop/hdfs/datanode/current/BP-876741428-10.2.226.32-1638727053679/current/finalized/subdir0/subdir0/blk_1073741829 for deletion\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:31 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741830_1006 file /opt/amazon/hadoop/hdfs/datanode/current/BP-876741428-10.2.226.32-1638727053679/current/finalized/subdir0/subdir0/blk_1073741830 for deletion\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:31 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741831_1007 file /opt/amazon/hadoop/hdfs/datanode/current/BP-876741428-10.2.226.32-1638727053679/current/finalized/subdir0/subdir0/blk_1073741831 for deletion\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:31 INFO impl.FsDatasetAsyncDiskService: Deleted BP-876741428-10.2.226.32-1638727053679 blk_1073741825_1001 file /opt/amazon/hadoop/hdfs/datanode/current/BP-876741428-10.2.226.32-1638727053679/current/finalized/subdir0/subdir0/blk_1073741825\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:31 INFO impl.FsDatasetAsyncDiskService: Deleted BP-876741428-10.2.226.32-1638727053679 blk_1073741826_1002 file /opt/amazon/hadoop/hdfs/datanode/current/BP-876741428-10.2.226.32-1638727053679/current/finalized/subdir0/subdir0/blk_1073741826\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:31 INFO impl.FsDatasetAsyncDiskService: Deleted BP-876741428-10.2.226.32-1638727053679 blk_1073741827_1003 file /opt/amazon/hadoop/hdfs/datanode/current/BP-876741428-10.2.226.32-1638727053679/current/finalized/subdir0/subdir0/blk_1073741827\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:31 INFO impl.FsDatasetAsyncDiskService: Deleted BP-876741428-10.2.226.32-1638727053679 blk_1073741828_1004 file /opt/amazon/hadoop/hdfs/datanode/current/BP-876741428-10.2.226.32-1638727053679/current/finalized/subdir0/subdir0/blk_1073741828\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:31 INFO impl.FsDatasetAsyncDiskService: Deleted BP-876741428-10.2.226.32-1638727053679 blk_1073741829_1005 file /opt/amazon/hadoop/hdfs/datanode/current/BP-876741428-10.2.226.32-1638727053679/current/finalized/subdir0/subdir0/blk_1073741829\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:31 INFO impl.FsDatasetAsyncDiskService: Deleted BP-876741428-10.2.226.32-1638727053679 blk_1073741830_1006 file /opt/amazon/hadoop/hdfs/datanode/current/BP-876741428-10.2.226.32-1638727053679/current/finalized/subdir0/subdir0/blk_1073741830\u001b[0m\n",
      "\u001b[34m21/12/05 17:58:31 INFO impl.FsDatasetAsyncDiskService: Deleted BP-876741428-10.2.226.32-1638727053679 blk_1073741831_1007 file /opt/amazon/hadoop/hdfs/datanode/current/BP-876741428-10.2.226.32-1638727053679/current/finalized/subdir0/subdir0/blk_1073741831\u001b[0m\n",
      "\u001b[35m21/12/05 17:58:34 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741825_1001 file /opt/amazon/hadoop/hdfs/datanode/current/BP-876741428-10.2.226.32-1638727053679/current/finalized/subdir0/subdir0/blk_1073741825 for deletion\u001b[0m\n",
      "\u001b[35m21/12/05 17:58:34 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741826_1002 file /opt/amazon/hadoop/hdfs/datanode/current/BP-876741428-10.2.226.32-1638727053679/current/finalized/subdir0/subdir0/blk_1073741826 for deletion\u001b[0m\n",
      "\u001b[35m21/12/05 17:58:34 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741827_1003 file /opt/amazon/hadoop/hdfs/datanode/current/BP-876741428-10.2.226.32-1638727053679/current/finalized/subdir0/subdir0/blk_1073741827 for deletion\u001b[0m\n",
      "\u001b[35m21/12/05 17:58:34 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741828_1004 file /opt/amazon/hadoop/hdfs/datanode/current/BP-876741428-10.2.226.32-1638727053679/current/finalized/subdir0/subdir0/blk_1073741828 for deletion\u001b[0m\n",
      "\u001b[35m21/12/05 17:58:34 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741829_1005 file /opt/amazon/hadoop/hdfs/datanode/current/BP-876741428-10.2.226.32-1638727053679/current/finalized/subdir0/subdir0/blk_1073741829 for deletion\u001b[0m\n",
      "\u001b[35m21/12/05 17:58:34 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741830_1006 file /opt/amazon/hadoop/hdfs/datanode/current/BP-876741428-10.2.226.32-1638727053679/current/finalized/subdir0/subdir0/blk_1073741830 for deletion\u001b[0m\n",
      "\u001b[35m21/12/05 17:58:34 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741831_1007 file /opt/amazon/hadoop/hdfs/datanode/current/BP-876741428-10.2.226.32-1638727053679/current/finalized/subdir0/subdir0/blk_1073741831 for deletion\u001b[0m\n",
      "\u001b[35m21/12/05 17:58:34 INFO impl.FsDatasetAsyncDiskService: Deleted BP-876741428-10.2.226.32-1638727053679 blk_1073741825_1001 file /opt/amazon/hadoop/hdfs/datanode/current/BP-876741428-10.2.226.32-1638727053679/current/finalized/subdir0/subdir0/blk_1073741825\u001b[0m\n",
      "\u001b[35m21/12/05 17:58:34 INFO impl.FsDatasetAsyncDiskService: Deleted BP-876741428-10.2.226.32-1638727053679 blk_1073741826_1002 file /opt/amazon/hadoop/hdfs/datanode/current/BP-876741428-10.2.226.32-1638727053679/current/finalized/subdir0/subdir0/blk_1073741826\u001b[0m\n",
      "\u001b[35m21/12/05 17:58:34 INFO impl.FsDatasetAsyncDiskService: Deleted BP-876741428-10.2.226.32-1638727053679 blk_1073741827_1003 file /opt/amazon/hadoop/hdfs/datanode/current/BP-876741428-10.2.226.32-1638727053679/current/finalized/subdir0/subdir0/blk_1073741827\u001b[0m\n",
      "\u001b[35m21/12/05 17:58:34 INFO impl.FsDatasetAsyncDiskService: Deleted BP-876741428-10.2.226.32-1638727053679 blk_1073741828_1004 file /opt/amazon/hadoop/hdfs/datanode/current/BP-876741428-10.2.226.32-1638727053679/current/finalized/subdir0/subdir0/blk_1073741828\u001b[0m\n",
      "\u001b[35m21/12/05 17:58:34 INFO impl.FsDatasetAsyncDiskService: Deleted BP-876741428-10.2.226.32-1638727053679 blk_1073741829_1005 file /opt/amazon/hadoop/hdfs/datanode/current/BP-876741428-10.2.226.32-1638727053679/current/finalized/subdir0/subdir0/blk_1073741829\u001b[0m\n",
      "\u001b[35m21/12/05 17:58:34 INFO impl.FsDatasetAsyncDiskService: Deleted BP-876741428-10.2.226.32-1638727053679 blk_1073741830_1006 file /opt/amazon/hadoop/hdfs/datanode/current/BP-876741428-10.2.226.32-1638727053679/current/finalized/subdir0/subdir0/blk_1073741830\u001b[0m\n",
      "\u001b[35m21/12/05 17:58:34 INFO impl.FsDatasetAsyncDiskService: Deleted BP-876741428-10.2.226.32-1638727053679 blk_1073741831_1007 file /opt/amazon/hadoop/hdfs/datanode/current/BP-876741428-10.2.226.32-1638727053679/current/finalized/subdir0/subdir0/blk_1073741831\u001b[0m\n",
      "\u001b[34m12-05 17:58 sagemaker-spark-event-logs-publisher INFO     Got spark event logs file: application_1638727061555_0001\u001b[0m\n",
      "\u001b[34m12-05 17:58 root         INFO     copying /tmp/spark-events/application_1638727061555_0001 to /opt/ml/processing/spark-events/application_1638727061555_0001\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000002/stdout]   eden space 663040K, 89% used [0x00000006bdb00000,0x00000\u001b[0m\n",
      "\u001b[35m21/12/05 17:58:42 INFO retry.RetryInvocationHandler: java.io.EOFException: End of File Exception between local host is: \"algo-2/10.2.213.203\"; destination host is: \"algo-1\":8031; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException, while invoking ResourceTrackerPBClientImpl.nodeHeartbeat over null. Retrying after sleeping for 30000ms.\u001b[0m\n",
      "\u001b[35m21/12/05 17:58:43 WARN datanode.DataNode: IOException in offerService\u001b[0m\n",
      "\u001b[35mjava.io.EOFException: End of File Exception between local host is: \"algo-2/10.2.213.203\"; destination host is: \"algo-1\":8020; : java.io.EOFException; For more details see:  http://wiki.apache.org/hadoop/EOFException\u001b[0m\n",
      "\u001b[35m#011at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\u001b[0m\n",
      "\u001b[35m#011at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\u001b[0m\n",
      "\u001b[35m#011at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\u001b[0m\n",
      "\u001b[35m#011at java.lang.reflect.Constructor.newInstance(Constructor.java:423)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:824)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:788)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1544)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client.call(Client.java:1486)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client.call(Client.java:1385)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)\u001b[0m\n",
      "\u001b[35m#011at com.sun.proxy.$Proxy15.sendHeartbeat(Unknown Source)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:166)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:516)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:646)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:846)\u001b[0m\n",
      "\u001b[35m#011at java.lang.Thread.run(Thread.java:748)\u001b[0m\n",
      "\u001b[35mCaused by: java.io.EOFException\u001b[0m\n",
      "\u001b[35m#011at java.io.DataInputStream.readInt(DataInputStream.java:392)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1845)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1184)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1080)\u001b[0m\n",
      "\u001b[35m12-05 17:58 urllib3.connectionpool WARNING  Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fb50d28ecd0>: Failed to establish a new connection: [Errno 111] Connection refused')': /\u001b[0m\n",
      "\u001b[35m21/12/05 17:58:47 INFO ipc.Client: Retrying connect to server: algo-1/10.2.226.32:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m12-05 17:58 urllib3.connectionpool WARNING  Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fb50d28ebd0>: Failed to establish a new connection: [Errno 111] Connection refused')': /\u001b[0m\n",
      "\u001b[35m21/12/05 17:58:48 INFO ipc.Client: Retrying connect to server: algo-1/10.2.226.32:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m21/12/05 17:58:49 INFO ipc.Client: Retrying connect to server: algo-1/10.2.226.32:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m12-05 17:58 urllib3.connectionpool WARNING  Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fb50d1a90d0>: Failed to establish a new connection: [Errno 111] Connection refused')': /\u001b[0m\n",
      "\u001b[35m21/12/05 17:58:50 INFO ipc.Client: Retrying connect to server: algo-1/10.2.226.32:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m21/12/05 17:58:51 INFO ipc.Client: Retrying connect to server: algo-1/10.2.226.32:8020. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m21/12/05 17:58:52 INFO ipc.Client: Retrying connect to server: algo-1/10.2.226.32:8020. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m21/12/05 17:58:53 INFO ipc.Client: Retrying connect to server: algo-1/10.2.226.32:8020. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m12-05 17:58 urllib3.connectionpool WARNING  Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fb50d1a9550>: Failed to establish a new connection: [Errno 111] Connection refused')': /\u001b[0m\n",
      "\u001b[35m21/12/05 17:58:54 INFO ipc.Client: Retrying connect to server: algo-1/10.2.226.32:8020. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m21/12/05 17:58:55 INFO ipc.Client: Retrying connect to server: algo-1/10.2.226.32:8020. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m21/12/05 17:58:56 INFO ipc.Client: Retrying connect to server: algo-1/10.2.226.32:8020. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m21/12/05 17:58:56 WARN datanode.DataNode: IOException in offerService\u001b[0m\n",
      "\u001b[35mjava.net.ConnectException: Call From algo-2/10.2.213.203 to algo-1:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\u001b[0m\n",
      "\u001b[35m#011at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\u001b[0m\n",
      "\u001b[35m#011at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\u001b[0m\n",
      "\u001b[35m#011at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\u001b[0m\n",
      "\u001b[35m#011at java.lang.reflect.Constructor.newInstance(Constructor.java:423)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:824)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:754)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1544)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client.call(Client.java:1486)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client.call(Client.java:1385)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)\u001b[0m\n",
      "\u001b[35m#011at com.sun.proxy.$Proxy15.sendHeartbeat(Unknown Source)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.hdfs.protocolPB.DatanodeProtocolClientSideTranslatorPB.sendHeartbeat(DatanodeProtocolClientSideTranslatorPB.java:166)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.sendHeartBeat(BPServiceActor.java:516)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.offerService(BPServiceActor.java:646)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:846)\u001b[0m\n",
      "\u001b[35m#011at java.lang.Thread.run(Thread.java:748)\u001b[0m\n",
      "\u001b[35mCaused by: java.net.ConnectException: Connection refused\u001b[0m\n",
      "\u001b[35m#011at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\u001b[0m\n",
      "\u001b[35m#011at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:716)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:531)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:701)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:805)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client$Connection.access$3700(Client.java:423)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client.getConnection(Client.java:1601)\u001b[0m\n",
      "\u001b[35m#011at org.apache.hadoop.ipc.Client.call(Client.java:1432)\u001b[0m\n",
      "\u001b[35m#011... 9 more\u001b[0m\n",
      "\n",
      "\u001b[35m21/12/05 17:58:58 INFO ipc.Client: Retrying connect to server: algo-1/10.2.226.32:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m21/12/05 17:58:59 INFO ipc.Client: Retrying connect to server: algo-1/10.2.226.32:8020. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m21/12/05 17:59:00 INFO ipc.Client: Retrying connect to server: algo-1/10.2.226.32:8020. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m21/12/05 17:59:01 INFO ipc.Client: Retrying connect to server: algo-1/10.2.226.32:8020. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[35m12-05 17:59 urllib3.connectionpool WARNING  Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fb50d1a9990>: Failed to establish a new connection: [Errno 111] Connection refused')': /\u001b[0m\n",
      "\u001b[35m12-05 17:59 smspark-submit INFO     primary is down, worker now exiting\u001b[0m\n",
      "\u001b[35m[/var/log/yarn/userlogs/application_1638727061555_0001/container_1638727061555_0001_01_000003/stderr] 21/12/05 17:5\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.spark.processing import PySparkProcessor\n",
    "\n",
    "# Upload the raw input dataset to a unique S3 location\n",
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "prefix = \"sagemaker/spark-preprocess-demo/{}\".format(timestamp_prefix)\n",
    "input_prefix_abalone = \"{}/input/raw/abalone\".format(prefix)\n",
    "input_preprocessed_prefix_abalone = \"{}/input/preprocessed/abalone\".format(prefix)\n",
    "\n",
    "sagemaker_session.upload_data(\n",
    "    path=\"./data/abalone.csv\", bucket=bucket, key_prefix=input_prefix_abalone\n",
    ")\n",
    "\n",
    "# Run the processing job\n",
    "spark_processor = PySparkProcessor(\n",
    "    base_job_name=\"sm-spark\",\n",
    "    framework_version=\"2.4\",\n",
    "    role=role,\n",
    "    instance_count=2,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    max_runtime_in_seconds=1200,\n",
    ")\n",
    "\n",
    "spark_processor.run(\n",
    "    submit_app=\"./code/preprocess.py\",\n",
    "    arguments=[\n",
    "        \"--s3_input_bucket\", bucket,\n",
    "        \"--s3_input_key_prefix\", input_prefix_abalone,\n",
    "        \"--s3_output_bucket\", bucket,\n",
    "        \"--s3_output_key_prefix\", input_preprocessed_prefix_abalone,\n",
    "    ],\n",
    "    spark_event_logs_s3_uri=\"s3://{}/{}/spark_event_logs\".format(bucket, prefix),\n",
    "    logs=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d45a7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 rows from s3://sagemaker-us-east-1-193890026231/sagemaker/spark-preprocess-demo/2021-12-05-17-53-35/input/preprocessed/abalone/train/\n",
      "5.0,0.0,0.0,1.0,0.275,0.195,0.07,0.08,0.031,0.0215,0.025\n",
      "6.0,0.0,0.0,1.0,0.29,0.21,0.075,0.275,0.113,0.0675,0.035\n",
      "5.0,0.0,0.0,1.0,0.29,0.225,0.075,0.14,0.0515,0.0235,0.04\n",
      "7.0,0.0,0.0,1.0,0.305,0.225,0.07,0.1485,0.0585,0.0335,0.045\n",
      "7.0,0.0,0.0,1.0,0.325,0.26,0.09,0.1915,0.085,0.036,0.062\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 5 rows from s3://{}/{}/train/\".format(bucket, input_preprocessed_prefix_abalone))\n",
    "!aws s3 cp --quiet s3://$bucket/$input_preprocessed_prefix_abalone/train/part-00000 - | head -n5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2eb0820f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pulling spark history server image...\n",
      "docker command: docker pull 173754725891.dkr.ecr.us-east-1.amazonaws.com/sagemaker-spark-processing:2.4-cpu\n",
      "image pulled: 173754725891.dkr.ecr.us-east-1.amazonaws.com/sagemaker-spark-processing:2.4-cpu\n",
      "History server terminated\n",
      "Starting history server...\n",
      "History server is up on https://rpbarros-br-flight-2021.notebook.us-east-1.sagemaker.aws/proxy/15050\n"
     ]
    }
   ],
   "source": [
    "spark_processor.start_history_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57773abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "History server is running, terminating history server\n",
      "History server terminated\n"
     ]
    }
   ],
   "source": [
    "spark_processor.terminate_history_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2199493",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
